% -*- TeX-master: "../Thesis.tex" -*-


\chapter{Theoretical Background}

\epigraphhead[75]{
  \epigraph{\itshape Begin at the beginning, the King said
    gravely, ``and go on till you come to the end: then stop.''}
  {---Lewis Carroll\\ \textit{Alice in Wonderland}}
}


\lettrine{A}{neural} network, more properly referred to as an \gls{ann} are
computing systems vaguely inspired by the biological neural networks that
constitute animal brains. Dr
\fhref{https://en.wikipedia.org/wiki/Robert_Hecht-Nielsen}{Robert
  Hecht-Nielsen} (inventor of one of the first neurocomputers) defines a neural
network as:

\begin{quoteBox}
  \itshape
  \ldots a computing system made up of a number of simple, highly interconnected
  processing elements, which process information by their dynamic state response
  to external inputs.
  \tcblower
  \hfill \upshape
  ---\href{https://en.wikipedia.org/wiki/Robert_Hecht-Nielsen}
  {Robert Hecht-Nielsen}
\end{quoteBox}

\subsubsection{Mathematical perspective}
Although the analogy made above of an \gls{ann} with a biological brain, there
is no need for this, we can just think of a neural network as a mathematical
optimization problem. We can think of the whole network to be a function that
takes some inputs to some outputs, and this function dependent on
parameters. The idea is to adjust this parameters to get a function that works
well with some known dataset, and we will trust that it will generalize
well. If the network is big enough and we carefully adjust the parameters, we
will be able to learn and calculate very complex functions.

\subsubsection{Tipos de redes neuronales}
Durante los últimos años, se han producido avances significativos en el ámbito
de la inteligencia artificial y han aparecido múltiples modelos nuevos de redes
neuronales. En una clasificación muy general de modelos neuronales podemos
encontrar diferentes.

\begin{itemize}
  \item \textbf{Fully connected layered neural networks}. Estas redes son el
  tipo de modelo neuronales maś sencillo que existe y sobre el que se
  construyen modelos más complejos. Una descripción más detallada sobre este
  tipo de redes puede ser encontrado en la \vref{sec:fully-conn-layer}.
  \item \textbf{\acrlong*{cnn}}. Este tipo de redes neuronales son
  especialmente útiles en el procesado de imágenes, ya que permiten preservar
  la información espacial de relación entre píxeles. Serán estudiadas en la
  \vref{sec:conv-neur-netw}.
  \item \textbf{\acrlong*{rnn}}. Modelos neuronales ideales para el análisis
  de series temporales (como puede ser texto o audio). Serán estudiadas en la
  \vref{sec:recurr-neur-netw}.
  \item \textbf{Transformers}. Los modelos basados en transformadores forman
  parte del \acrlong{sota} actual\footnote{En \today, fecha actual del
    documento.}. Están siendo usados de manera extensiva en \gls{nlp} y
  estudiando sus posibles aplicaciones en visión por computadores y procesado
  de imágen. Serán descritos en la \vref{sec:transformers}.
\end{itemize}


\section{Fully Connected Layered Neural networks}
\label{sec:fully-conn-layer}

Las fully connected layered neural networks son el tipo de modelo neuronal más
sencillo que existe y que consitutye la base para estructuras neuronales más
complejas.

\subsection{Topology}
\label{sec:topology}

Este tipo de redes neuronales se caracteriza por una estructura de neuronales
organizada en diferentes capas, de manera que las neuronas entre capas
adyacentes están conectadas entre ellas por arcos. En la
\vref{fig:fully-conn-network} se muestra un ejemplo de este tipo de
arquitectura.

\begin{figure}[ht]
  \centering
  \input{Figures/Tikz/Fully Connected Neural Network.tex}
  \caption[Fully Connected Neural Network topology]{Ejemplo de fully connected
    layered neural network. Consta de 4 capas diferenciadas de neuronas y se
    muestran las conexiones entre ellas de manera gráfica con flechas.}
  \label{fig:fully-conn-network}
\end{figure}

A continuación discutiremos las partes principales constituyentes de este tipo
de redes neuronales: layers, neurons and connections.

\subsubsection{Layers}
The layers are just a collection of neurons, we will distinguish between three
types depending on its position in the network.

\begin{itemize}
  \item Input layer: patterns are presented to the network via this layer.
  \item Hidden layers: all the inner layers.
  \item Output layer: is the last layer, where the answer is obtained.
\end{itemize}

We will denote with \(L\) the number of layers and with \(n_l\) the size of the
\(l\)-th layer.

To gain some intuition on how this work, let's think about the handwritten
recognition problem. Suppose we have a set of images with handwritten digits in
it and that we will like to implement an ANN that is capable of recognizing
that digits. In this case, if the digits images are of \(28 \times 28\) pixels,
the input layer will consists of 784 (\(28 \times 28\)) neurons and each neuron
will hold the grayscale value of a pixel. For the output layer we will need 10
neurons (one for each number between 0 and 9), and we will like that when we
feed our network with an image holding a handwritten number 3, then the ouput
is one 1 in the position corresponding to the number 3 and the rest of zeros.
\todo{quizá borrar este párrafo que es innecesario}.

\subsubsection{Neurons}
Neurons are the core component of any neural network. Basically there are three
subparts that form a neuron.

\begin{itemize}
  \item Value: each neuron holds a value, it will be denoted by \(x_i^l \in \R\)
  for the \(i\)-th neuron in the \(l\)-th layer. Of course, it should be satisfied
  \(1 \leq i \leq n_l\). We will use use the notation \(\mathbf{x}^l\) for the
  vector of all the values in the \(l\)-th level. When we speak of the input
  vector, we may ommit the superindex, i.e.\ we will use \(\mathbf{x}\) to denote
  \(\mathbf{x}^0\). Similarly, for the output layer, we will use
  \(\mathbf{\hat{y}}\) to refer to \(\mathbf{x}^L\).
  \item Bias: also each neuron has a bias, denoted as \(b_i^l\) for the
  \(i\)-th neuron in the \(l\)-th layer. Is then true that
  \(1 \leq i \leq n_l\). The vector of all biases in the \(l\)-th layer will be
  denoted by \(\mathbf{b}^l\).
  \item Activation function: all neurons have an activation function
  \(f_i^l \in \mathcal{C}^1(\R, \R)\) for the \(i\)-th neuron in the \(l\)-th
  layer\footnote{Usually all the activation functions are neuron-independent
    (i.e.\ \(f_i^l\) does not really depend on \(i\) or \(l\)).}. Of course, it is
  needed \(1 \leq i \leq n_l\). The regularity assummed for this functions is
  important, since we will be optimizing in the future by taking derivatives.
\end{itemize}

\subsubsection{Connections}
As we discussed in the topology section (\ref{sec:topology}), all neurons
between adjacent layers are required to be connected, this are the connections,
that should have associated a \emph{weight}. For the connection between the
\(i\)-th neuron in the \(l\)-th layer and the \(j\)-th neuron in the
\(l + 1\)-th layer, we will denote this weight by \(w_{ij}^l \in \R\). The set of
all this weights is as follows,
\begin{equation}
   \{w_{ij}^l \mid 1 \leq i \leq n_{l}, \, 1 \leq j \leq n_{l+1}, \, 1 \leq l <
   L \} \subset \R.
 \end{equation}

The \emph{matrix} of all weights in the \(l\)-th layer will be denoted by
\(\mathbf{W}^l\). This is, \((\mathbf{W}^l)_{ij} = w_{ij}^l\).

\subsection{Model: feed forward}
\label{subsec:forward}

From now on let's suppose we are working with a fully-connected neural network,
with different layers and we will be using the same notation used before. The
values of the neurons can be computed with
\begin{equation}
  x_i^l = f_i^l \left( \sum_{k=1}^{n_{l-1}} w_{ik}^{l-1} x_{k}^{l-1} + b_i^l \right).
\end{equation}

This formula is sometimes referred to as the feed-forward formula or forward
propagation. It's important to note that it's a recursive formula, once the
values the neurons in the the input layer are known, we can iterate computing
the values of the neurons in the the next adjacent layer, until we reach the
output layer. In this network we can think of information travelling in one
direction, forward, from the input layer, through the hidden layers to the
output neurons.

\subsection{Trainning: backpropagation}
\label{sec:backward}

In order to being able to train our neural network, is mandatory to define an
error function (also known as loss function) that quantifies how good or bad
the neural network is performing when feeded with a particular dataset. We will
you the below notation.
\begin{itemize}
  \item \textbf{Dataset}: will be denoted by and consists in input-output pairs
  \((\mathbf{x}, \mathbf{y})\), where \(\mathbf{x}\) represents the input and
  \(\mathbf{y}\) the \emph{desired} output. We shall denote the size (cardinal)
  of the dataset by \(N\). Of course, in terms of our ANN \(x\) corresponds to
  the values of the first (or input) layer and \(\mathbf{y}\) to the output (or
  last) layer.
  \item \textbf{Parameters}: the parameters of our ANN are both the connection
  weights \(w_{ij}^l\) and the biases \(b_i^l\), we will denote the set of all
  parameters by \(\bm{\theta}\). Keep in mind that \(\bm{\theta}\) is just a
  set of real vectors of \(\R^D\) where \(D\) denotes the number of weigths and
  biases.
\end{itemize}

The error function quantifies how different is the desired output
\(\mathbf{y}\) and the calculated (\emph{predicted}) output
\(\mathbf{\hat{y}}\) of the neural network on input \(\mathbf{x}\) for a set of
input-output pairs \((\mathbf{x}, \mathbf{y}) \in \Omega\) and a particular
value of the parameters \(\bm{\theta}\). We will denote the error funcion by
\(E(\Omega, \theta)\) and we will assume that is continuosly differentiable
(i.e.\ \(\mathcal{C}^1\)).


\section{Convolutional Neural Networks}
\label{sec:conv-neur-netw}

\glspl{cnn} son una clase de \glspl{dnn} que son ampliamente utilizadas en
computer vision. Son comúnmente aplicadas en el análisis de imágenes, ya que
tienen la característica principal de tener la capacidad de preservar la
relación espacial entre píxeles. Así mismo, son versiones regularizadas de las
fully connected networks (explicadas en la \vref{sec:fully-conn-layer}), esto
es, es menos probable overfitear en los datos.

\fhref{http://yann.lecun.com/index.html}{Yann LeCun}, un reconocido científico
conocido como el padre de las \glspl{cnn}, las define de la siguiente manera:
\begin{quoteBox}
  \itshape
  Convolutional Neural Networks are are a special kind of multi-layer neural
  networks. Like almost every other neural networks they are trained with a
  version of the back-propagation algorithm. Where they differ is in the
  architecture.
  \tcblower
  \hfill \upshape
  ---\href{http://yann.lecun.com/index.html}{Yann LeCun}
\end{quoteBox}

Por tanto, estudiaremos la arquitectura o topología de este tipo de redes a
continuación en la \vref{sec:topology-1}.

\subsection{Topology} \label{sec:topology-1}

De manera análoga al caso de las fully connected networks, una \gls{cnn}
consiste en una capa de entrada, capas ocultas y una capa de salida. En una
\gls{cnn} las capas ocultas incluyen capas que calculan convoluciones. Estas
capas convolucionales están seguidas por otras diferentes, entre las que
destacan los pooling layers, fully connected layers and normalization
layers. En la \vref{fig:lenet-5} se puede observar un ejemplo de este
tipo de arquitecturas.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{Images/Architecture LeNet-5.png}
  \caption[Ejemplo de topología de una \acrshort{cnn} (LeNet-5)]{Ejemplo de la
    topología de una \gls{cnn}. Concretamente es
    \href{http://yann.lecun.com/exdb/lenet}{LeNet-5} (ver
    \url{http://yann.lecun.com/exdb/lenet}), una red convolucional creada por
    Yann LeCun.}
  \label{fig:lenet-5}
\end{figure}

\subsubsection{Convolutional layers}
El nombre proviene del hecho de que estas capas utilizan la operación
matemática de la
\fhref{https://en.wikipedia.org/wiki/Convolution}{convolución}. Esta operación
se define (en su versión discreta) de la siguiente manera: dado un par de
funciones \(f, g\) definidas en el conjunto de los enteros \(\Z\), la
convolución discreta entre \(f\) y \(g\) viene dada por,
\begin{equation} \label{eq:math-conv}
  (f \star g)[n] = \sum_{m=-\infty}^{\infty} f[m]g[n - m],
\end{equation}
donde \(\star\) es el operador de convolución.

Esta es la definición matemática de la convolución discreta. Ahora bien, el uso
de la convolución en el tratamiento de imágenes no funciona exactamente como se
ha definido en \vref{eq:math-conv}. En la práctica, las imágenes de
entrada\footnote{En niveles más profundos de una \gls{cnn} dejaremos de
  entender las capas como imágenes y las llamares como mapas de
  características} son tensores tridimensionales para las imágenes
\acrshort{rgb}. En las capas convolucionales se hace una convolución
bidimensional con un filtro (también llamado kernel) tridimensional sobre cada
canal de la imagen de entrada y después se apilan todas estos mapas de
características en el tensor de salida, donde el número de canales de salida
coincide con el número de filtros en esa capa.

Podemos expresar matemáticamente la operación realizada en las capas
convolucionales de la siguiente manera. Sea \(\mathbf{X}\) el
mapa de características de entrada, \(\mathbf{Y}\) el mapa de características
de salida y el filtro \(\mathbf{F}\). La convolución se define entonces de la
siguiente manera,
\begin{equation} \label{eq:convolution}
  \mathbf{Y}_{i, j, k} =
  \sum_{l, m, n} \mathbf{X}_{l, j + m, k + n}\mathbf{F}_{i, l, m, n},
\end{equation}
donde la suma se realiza para todos los índices \(l, m, n\) válidos (esto
dependerá del \emph{padding}\footnote{Hay diferentes técnicas para definir el
  padding de una imagen. El más típico es el conocido como zero-padding, que
  consiste en añadir alrededor de la imagen vectores de 0 hasta completarla
  para poder realizar las convoluciones deseadas.} de la imagen de entrada).

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Convolution.png}
  \caption[Example of a convolution]{Example of a convolution with a kernel
    (filter) of dimensions \((3, 3, 1)\) and an input feature map (tensor) of
    size \((8, 8, 1)\).}
  \label{fig:convolution}
\end{figure}

En la \vref{fig:convolution} se muestra gráficamente un ejemplo de
convolución. Como se puede ver para cada uno de los elementos del tensor de
salida se realiza el cálculo de la \vref{eq:convolution} con el filtro
mostrado.

\subsubsection{Pooling Layers}
Otro tipo de capas que se usa en este tipo de redes son las de pooling. Las
capas de pooling reducen la dimensión de la red combinando la salida de las
neuronas de una capa en una única neurona en la capa siguiente. Este tipo de
capas añaden además no-linealidades al modelo. Así mismo hacen a las \gls{cnn}
menos sensibles a pequeños cambios locales en la localización espacial.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Max pooling.png}
  \caption[Pooling layer example: max pooling]{Pooling layer example:
    \(2 \times 2\) max pooling. El tensor de entrada se divide en bloques
    \(2 \times 2\) y el max pool devuelve un tensor de salida con el valor
    máximo de cada bloque.}
  \label{fig:max-pool}
\end{figure}

Hay diferentes tipos de pooling. Los más conocidos son \emph{max} pooling y
\emph{average} pooling. El primero usa el valor máximo de cada cluster de
neuronas en el nivel anterior. Average pooling, sin embargo, usa el valor
medio. En la \vref{fig:max-pool} podemos ver una representación de un
\(2 \times 2\) max-pool. En este caso concreto, la operación matemática a
realizar viene dada por la siguiente expresión,
\begin{equation}
  f_{X,Y}(S) = \max _{a,b=0}^{1}S_{2X+a,2Y+b},
\end{equation}
where \(S\) is every depth slice in the input. This discharges 75\% of the
activations.

\subsection{Activation functions}
Analogamente a las fully connected neural networks, en este caso también es
común el uso de funciones de activación con el objetivo de introducir
no-linealidades en el modelo sin afectar los campos receptivos de las
convoluciones. Típicamente se añaden justo después de realizar la convolución.

Las principales funciones que se usan son las siguientes:
\begin{itemize}
  \item \textbf{\gls*{relu}}. Es la función \(f(x) = \max \{0, x\}\), que es la
  más usada. Elimina los valores negativos del mapa de características y los
  pone a cero.
  \item \textbf{Tangente hiperbolica}. \(f(x) = \tanh x\)
  \item \textbf{Sigmoid function}. \(\sigma(x) = {(1 + e^{-x})}^{-1}\).
  \item \(f(x) = |\tanh x|\)
\end{itemize}


\section{Recurrent Neural Networks}
\label{sec:recurr-neur-netw}

\glspl{rnn} son una clase de \glspl{ann} que son ampliamente utilizadas con
secuencias temporales (como por ejemplo en el ámbito de
\gls{nlp}). Analogamente a las \glspl{cnn} que son adecuadas para el tratamiento
de imágenes, las \glspl{rnn} son un tipo de red neuronal especializada para
procesar secuencias de valores del tipo \(\mathbf{x}^1, \ldots
\mathbf{x}^{\tau}\) con \(\mathbf{x}^t \in \R^n\).

\subsection{Topology}

Viendo la \vref{fig:rnn}, podemos ver la arquitectura de una \gls{rnn}. Los
valores de entrada \(\mathbf{x}^1, \ldots \mathbf{x}^\tau\) son alimentados a
la red de manera ordenada. Por ello, para obtener el vector de salida
\(\mathbf{y}^t\), es necesario el hidden state \(\mathbf{h}^{t - 1}\) y la
entrada \(\mathbf{y}^{t}\). Prior inputs
\(\mathbf{x}^1, \ldots \mathbf{x}^{t - 1}\) are represented by the hidden state
\(\mathbf{h}^{t - 1}\), therefore the output \(\mathbf{y}^t\), depends exactly
of all the inputs until time \(t\).

\begin{figure}[ht]
  \begin{subfigure}[b]{.25\textwidth}
    \centering
    \includegraphics[height=4cm]{Images/RNN.png}
    \caption{Basic \gls{rnn}.}
    \label{fig:rnn-a}
  \end{subfigure}\hfill
  \begin{subfigure}[b]{.75\textwidth}
    \centering
    \includegraphics[height=4cm]{Images/Unfolded RNN.png}
    \caption{Unfolded basic \gls{rnn}.}
    \label{fig:rnn-b}
  \end{subfigure}
  \caption[Basic topology of \acrfull{rnn}]{Basic topology of
    \acrfull{rnn}. Two versions of the same network are shown, one is the
    compact version and the other is the same network but unfolded in time.}
  \label{fig:rnn}
  \source{TODO. Citar es de stanford también}
\end{figure}
\footnotetext{TODO. Citar}

\subsubsection{Different types of \gls*{rnn}}
Las feedforward networks envían una entrada a una salida\footnote{La entrada y
  la salida puede ser vectorial. No significa, por tanto, que son solo un
  número, significa que solo son \emph{un} vector multidimensional de
  \(\R^n\)}, pero en el caso de las \glspl{rnn} no es así. Existen diferentes
posibilidades, en las que la longitud de la entrada y de la salida pueden
variar. Todos los tipos diferentes de architecturas que pueden existir están
recogidos en la \vref{fig:rnn-types}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{Images/RNN types.png}
  \caption[Types of architectures for \gls{rnn}]{Types of architectures for
    \gls{rnn}. All posibilities are considered (i.e.\ all combinations are
    presented).}
  \label{fig:rnn-types}
  \source{TODO. Es de Stanford curso}
\end{figure}
\footnotetext{TODO. de stanford.}

Estas diferentes posibilidades tienen luego su aplicación en diversos ámbitos
como pueden ser: image captioning (``one to many''), action prediction (``many
to one``), video captioning (``many to many'' first option), and video
classification on frame label (``many to many'' second option). El caso extremo
de ``one to one'' se corresponde precisamente con una fully connected layered
neural network normal como la estudiada en la \vref{sec:fully-conn-layer}.

\subsection{Model: feed forward}

Podemos calcular el valor de las neuronas con una fórmula recurrente, en la que
intervienen los hidden states \(\mathbf{h}^t\) y los valores de los vectores de
entrada \(\mathbf{x}^t\). En el caso más sencillo de una \gls{rnn}
\emph{simple}, este cálculo consiste en,
\begin{equation} \label{eq:rnn-hidden}
  \mathbf{h}^t = f_{W} (\mathbf{h}^{t - 1}, \mathbf{x}^t),
\end{equation}
y a continuación se puede calcular el vector de salida \(\mathbf{y}^t\) usando
la siguiente expresión,
\begin{equation} \label{eq:rnn-out}
  \mathbf{y}^t = W_{hy}\mathbf{h}^t,
\end{equation}
donde \(W_{hy}\) es una matriz de parámetros (entrenables).

\subsubsection{Variant \gls*{rnn} architectures}
El modelo definido anteriormente es la versión más simple para una
\gls{rnn}. El cálculo de los valores de salida con las
\vref{eq:rnn-hidden,eq:rnn-out} puede traer problemas con los gradientes y, por
tanto, limitar la capacidad de la información de viajar en el
tiempo\footnotetext{This problems are know as vanishing/exploding gradient. The
  problem of \emph{exploding} gradients can be solve by gradient clipping
  (scaling it if the norm is too big). However for the \emph{vanishing}
  gradient problem, it is neccesary to change the \gls{rnn}
  architecture.}. Introducimos dos variantes nuevas para solucionar estos
problemas:
\begin{itemize}
  \item \textbf{\gls*{lstm}}. Una unidad de \gls{lstm} está compuesta de
  diferentes gates que definen su comportamiento. Su nombre viene dado por la
  función que realizan: \emph{input} gate (\(i\)), \emph{output} gate (\(o\))
  and \emph{forget} gate (\(f\)). En la \vref{fig:lstm} se muestra una
  representación gráfica de este tipo de elementos.
  \begin{figure}[ht]
    \centering
    \includesvg[width=.6\textwidth]{LSTM.svg}
    \caption[\acrlong*{lstm}]{\acrfull{lstm} representation. Input (\(i\)),
      output (\(o\)) and forget (\(f\)) gates are shown in the image.}
    \label{fig:lstm}
    \source{\citetitle{enwiki:1005032489}}
  \end{figure}
  \footnotetext{\cite{enwiki:1005032489}}

  Su funcionamiento viene descrito por el siguiente conjunto de ecuaciones,
  \begin{equation}
    \begin{aligned}
      \begin{pmatrix}
        i \\
        f \\
        o \\
        g
      \end{pmatrix} &=
      \begin{pmatrix}
        \sigma \\
        \sigma \\
        \sigma \\
        \tanh
      \end{pmatrix}
      W
      \begin{pmatrix}
        h^{t-1} \\
        x^t
      \end{pmatrix}, \\
      c_t &= f \odot c_{t-1} + i \odot g, \\
      h_t &= o \odot \tanh c_{t}. \\
    \end{aligned}
  \end{equation}
  \item \textbf{\gls*{gru}}. Son otro tipo de gating mechanism introducido por
  \autocite{cho14:learn_rnn} TODO. me he quedado aquí
  \todo{estaba mirando \url{https://en.wikipedia.org/wiki/Gated_recurrent_unit}}
\end{itemize}



\section{Transformers}
\label{sec:transformers}


\section{Deep Neural Network Architectures}
