% -*- TeX-master: "../Thesis.tex" -*-


\chapter{Theoretical Background}\label{cha:theory}

\epigraphhead [75]{
  \epigraph{\itshape
    Wwithout theory, there is nothing to revise. \\
    Without theory, experience has no meaning. \\
    Without theory, one has no questions to ask. \\
    Hence, without theory, there is no learning.}
  {---\scshape William Edwards}}

\lettrine{A}{neural} network, more properly referred to as an \gls{ann} are
computing systems vaguely inspired by the biological neural networks that
constitute animal brains. Dr.\ Robert Hecht-Nielsen (inventor of one of the
first neurocomputers), in an article by
\myCite{caudill87:neural_networ_primer_part_i}, defines a neural network as:

\begin{quoteBox}
  \ldots a computing system made up of a number of simple, highly
  interconnected processing elements, which process information by their
  dynamic state response to external inputs.
  \tcblower\quotecite{caudill87:neural_networ_primer_part_i}
\end{quoteBox}

\subsubsection{Mathematical perspective}
Although the analogy made above of an \gls{ann} with a biological brain, there
is no need for this, we can just think of a neural network as a mathematical
optimization problem. We can think of the whole network to be a function that
takes some inputs to some outputs, and this function dependent on
parameters. The idea is to adjust this parameters to get a function that works
well with some known dataset, and we will trust that it will generalize
well. If the network is big enough and we carefully adjust the parameters, we
will be able to learn and calculate very complex functions.


\section{Tensors}\index{Tensor}

In our context, it will be useful to use the mathematical tool of
tensors. There are different approaches to define this mathematical objetcs,
among then, defining them as multi-dimensional arrays of real numbers, as
follows: a \emph{tensor} is an element
\(\mathbf{T} \in \R^{n_1 \times \cdots \times n_r}\), with
\(n_1, \ldots, n_r \in \N\). The number \(r\) is called the \emph{rank} of the
tensor.

Similarly to real vectors from \(\R^n\), each of the elements of a tensor
\(\mathbf{T}\) can be referred to using a multi-index
\(i = (i_1, \ldots, i_r) \in \N^r\). The notation \(\mathbf{T}_i\), or
\(\mathbf{T}_{i_1, \ldots, i_r}\), refers to the element indexed by \(i\) of
the tensor \(\mathbf{T}\).

\begin{exampleBox}
  Examples of tensors include scalars (view as tensors of rank \(0\)), vectors
  (rank \(1\) tensors) and matrices (rank \(2\)).
\end{exampleBox}

Most popular \gls{dl} frameworks such as PyTorch (used in this thesis), are
just programming libraries that provide efficient tensor data strctures and and
fast tensor computation (with the ability to compute in \gls{gpu}). These
tensors store the data used to train neuronal models. In the case of \gls{cv},
the following tensors appear:
\begin{itemize}
  \item \textbf{\acs*{rgb} Image}. An \acs{rgb} image can be interpreted as a
  tensor of rank 3, \(\mathbf{I} \in \R^{C \times H \times W}\), where \(C\)
  corresponded to the number of channels (i.e., in this case, \(C = 3\)), \(H\)
  corresponds to the height of the image and \(W\) to its
  width.\footnote{Grayscale images also fall into this category with just 1
    channel (i.e., using the above notation \(C = 1\)).}
  \item \textbf{Batch of \acs*{rgb} Images}. A batch of \acs{rgb} images is a
  set of \acs{rgb} images, therefore, can be interpreted as a tensor of rank 4,
  \(\mathbf{I} \in \R^{B \times C \times H \times W}\), where \(B\) corresponds
  to the batch size, and \((C, H, W)\) have the same meaning as in the case of
  an \acs{rgb} image.
\end{itemize}

\subsection{Tensor Operations}\index{Tensor!operations}

Tensors or rank 1 or 2 correspond to real vectors and real matrices, therefore,
all basic operations from linear algebra apply (e.g., vector sum, matrix-vector
multiplication). For tensor of higher rank, it is useful define element-wise
operations for any binary operation from real numbers.

Let \(\mathbf{T}\) and \(\mathbf{S}\) be tensors of rank \(r\). Let \(\star\)
be a binary operation for real numbers (e.g., ordinary sum or
multiplication). Then, the element-wise operation is defined as follows,
\begin{equation}
  (\mathbf{T} \star \mathbf{S})_i = \mathbf{T}_i \star \mathbf{S}_i,
\end{equation}
for all multi-indices \(i \in \N^r\).


\section{Neural Network Architectures}\index{Neural network}

During the last few years, there have been significant advances in the field of
artificial intelligence and multiple new models of neural networks have
appeared. In a very general classification of neural models we can find
different ones.

\begin{itemize}\index{Neural network!architectures}
  \item \textbf{\acl*{fnn}}. These networks are the simplest type of neural
  model that exists and on which more complex models are built. A more detailed
  description of this type of network can be found in
  \vref{sec:fnn}.
  \item \textbf{\acl*{cnn}}. This type of neural network is especially useful
  in image processing, since it allows preserving the spatial information of
  the relationship between pixels. They will be studied in
  \vref{sec:conv-neur-netw}.
  \item \textbf{\acl*{rnn}}. Ideal neural models for the analysis of time
  series (such as text or audio). They will be studied in
  \vref{sec:rnn}.
  \item \textbf{Transformers}. The transformer-based models are part of the
  current \acl{sota}.\footnote{On \today, current date of the document.} They
  are being used extensively in \gls{nlp} and studying their possible
  applications in computer vision and image processing. They will be described
  in \vref{sec:transformers}.
\end{itemize}

\subsection{\glsentrylong{fnn}}\label{sec:fnn}

Fully connected layered neural networks are the simplest type of neural model
that exists and that constitute the basis for more complex neural structures.

This type of neural network is characterized by a neural structure organized in
different layers, so that the neurons between adjacent layers are connected to
each other by arcs. An example of this type of architecture is shown in
\vref{fig:fully-conn-network}.

\begin{figure}[ht]
  \centering
  \input{Figures/Tikz/Fully Connected Neural Network.tex}
  \caption[Fully Connected Neural Network topology]{Example of Fully Connected
    Layered Neural Network. It consists of 4 differentiated layers of neurons
    and the connections between them are shown graphically with arrows.}
  \label{fig:fully-conn-network}
  \source{Created by the author}
\end{figure}

Next we will discuss the main constituent parts of this type of neural
networks: layers, neurons and connections.

\subsubsection{Layers}\index{Layer}
The layers are just a collection of neurons, we will distinguish between three
types depending on its position in the network: \emph{input} layer (patterns
are presented to the network via this layer), \emph{hidden} layer (all the
inner layers) and \emph{output} layer (is the last layer, where the answer is
obtained).

We will denote with \(L\) the number of layers and with \(n_l\) the size of the
\(l\)-th layer.

\subsubsection{Neurons}\index{Neuron}
Neurons are the core component of any neural network. Basically there are three
subparts that form a neuron.
\begin{itemize}
  \item \textbf{Value}. Each neuron holds a value, it will be denoted by
  \(x_i^l \in \R\) for the \(i\)-th neuron in the \(l\)-th layer. Of course, it
  should be satisfied \(1 \leq i \leq n_l\). We will use the notation
  \(\mathbf{x}^l\) for the vector of all the values in the \(l\)-th level. When
  we speak of the input vector, we may ommit the superindex, i.e., we will use
  \(\mathbf{x}\) to denote \(\mathbf{x}^0\). Similarly, for the output layer,
  we will use \(\mathbf{\hat{y}}\) to refer to \(\mathbf{x}^L\).
  \item \textbf{Bias}. Also each neuron has a bias, denoted as \(b_i^l\) for
  the \(i\)-th neuron in the \(l\)-th layer. Is then true that
  \(1 \leq i \leq n_l\). The vector of all biases in the \(l\)-th layer will be
  denoted by \(\mathbf{b}^l\).
  \item \textbf{Activation function}. All neurons have an activation function
  \(f_i ^ l \in \mathcal{C} ^ 1 (\R, \R)\) for the \(i\)-th neuron in the
  \(l\)-th layer.\footnote{Usually all the activation functions are
    neuron-independent (i.e., \(f_i^l\) does not really depend on \(i\) or
    \(l\)).} Of course, it is needed \(1 \leq i \leq n_l\). The regularity
  assummed for this functions is important, since we will be optimizing in the
  future by taking derivatives.
\end{itemize}

\subsubsection{Connections}\index{Connection}
As we discussed, in the topology of this network, all neurons between adjacent
layers are required to be connected, this are the connections, that should have
associated a \emph{weight}. For the connection between the \(i\)-th neuron in
the \(l\)-th layer and the \(j\)-th neuron in the \(l + 1\)-th layer, we will
denote this weight by \(w_{ij}^l \in \R\). The set of all this weights is as
follows,
\begin{equation}
  \{w_{ij}^l \mid 1 \leq i \leq n_{l}, \, 1 \leq j \leq n_{l+1}, \,
  1 \leq l < L \} \subset \R.
\end{equation}

The \emph{matrix} of all weights in the \(l\)-th layer will be denoted by
\(\mathbf{W}^l\). This is, \((\mathbf{W}^l)_{ij} = w_{ij}^ l\).

\begin{exampleBox}
  To gain some intuition on how this work, let's think about the handwritten
  recognition problem. Suppose we have a set of images with handwritten digits
  in it and that we will like to implement an ANN that is capable of
  recognizing that digits.

  In this case, if the digits images are of \(28 \times 28\) pixels, the input
  layer will consist of 784 (\(28 \times 28\)) neurons and each neuron will
  hold the grayscale value of a pixel. For the output layer we will need 10
  neurons (one for each number between 0 and 9), and we will like that when we
  feed our network with an image holding a handwritten number 3, then the ouput
  is one 1 in the position corresponding to the number 3 and the rest of zeros.
\end{exampleBox}

\subsubsection{Model: feed forward}\label{subsec:forward}
From now on let's suppose we are working with a fully-connected neural network,
with different layers and we will be using the same notation used before. The
values of the neurons can be computed with
\begin{equation}
  x_i^l = f_i^l \left(
    \sum_{k=1}^{n_{l-1}} w_{ik}^{l-1} x_{k}^{l-1} + b_i^l
  \right).
\end{equation}

This formula is sometimes referred to as the feed-forward formula or forward
propagation. It's important to note that it's a recursive formula, once the
values the neurons in the the input layer are known, we can iterate computing
the values of the neurons in the next adjacent layer, until we reach the output
layer. In this network we can think of information traveling in one direction,
forward, from the input layer, through the hidden layers to the output neurons.

\subsubsection{Trainning: backpropagation}%
\label{sec:backward}\index{Backpropgation}
In order to be able to train our neural network, it is mandatory to define an
error function (also known as loss function) that quantifies how good or bad
the neural network is performing when feeded with a particular dataset. We will
use the below notation.
\begin{itemize}
  \item \textbf{Dataset}. Will be denoted by and consists of input-output pairs
  \((\mathbf{x}, \mathbf{y})\), where \(\mathbf{x}\) represents the input and
  \(\mathbf{y}\) the \emph{desired} output. We shall denote the size (cardinal)
  of the dataset by \(N\). Of course, in terms of our ANN \(x\) corresponds to
  the values of the first (or input) layer and \(\mathbf{y}\) to the output (or
  last) layer.
  \item \textbf{Parameters}. Parameters of our ANN are both the connection
  weights \(w_{ij}^l\) and the biases \(b_i^l\), we will denote the set of all
  parameters by \(\bm{\theta}\). Keep in mind that \(\bm{\theta}\) is just a
  set of real vectors of \(\R^D\) where \(D\) denotes the number of weights and
  biases.
\end{itemize}

The error function quantifies how different is the desired output
\(\mathbf{y}\) and the calculated (\emph{predicted}) output
\(\mathbf{\hat{y}}\) of the neural network on input \(\mathbf{x}\) for a set of
input-output pairs \((\mathbf{x} , \mathbf{y}) \in \Omega\) and a particular
value of the parameters \(\theta\). We will denote the error function by
\(E (\Omega, \theta)\) and we will assume that it is continuosly differentiable
(i.e., \(\mathcal{C}^1\)).

\subsection{\glsentrylong{cnn}}\label{sec:conv-neur-netw}

\glspl{cnn} are a class of \glspl{dnn} that are widely used in computer
vision. They are commonly applied in image analysis, since they have the main
characteristic of having the ability to preserve the spatial relationship
between pixels. Likewise, they are regularized versions of fully connected
networks (explained in \vref{sec:fnn}), that is, it is less likely
to overfit the data.

\fhref{http://yann.lecun.com/index.html}{Yann LeCun}, a renowned scientist
known as the father of \glspl{cnn}, defines them as follows:
\begin{quoteBox}
  Convolutional Neural Networks are a special kind of multi-layer neural
  networks. Like almost every other neural networks they are trained with a
  version of the back-propagation algorithm. Where they differ is in the
  architecture.
  \tcblower\href{http://yann.lecun.com/index.html}{Yann LeCun}
\end{quoteBox}

Analogously to the case of fully connected networks, a \gls{cnn} consists of an
input layer, hidden layers and an output layer. In a \gls{cnn} the hidden
layers include layers that calculate convolutions. These convolutional layers
are followed by different ones, among which the pooling layers, fully connected
layers and normalization layers stand out. An example of this type of
architecture can be seen in \vref{fig:lenet-5}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{Images/Architecture LeNet-5.png}
  \caption[Example of topology of a \acs*{cnn} (LeNet-5)]{Example of topology
    of a \gls{cnn}. Specifically, it is
    \href{http://yann.lecun.com/exdb/lenet}{LeNet-5} (see
    \url{http://yann.lecun.com/exdb/lenet}), a convolutional network created by
    Yann LeCun.}
  \label{fig:lenet-5}
  \source{From TODO}
\end{figure}

\subsubsection{Convolutional layers}
The name comes from the fact that these layers use the mathematical operation
of \fhref{https://en.wikipedia.org/wiki/Convolution}{convolution}. This
operation is defined (in its discrete version) as follows: given a pair of
functions \(f, g\) defined on the set of integers \(\Z\), the discrete
convolution between \(f\) and \(g\) is given by,
\begin{equation} \label{eq:math-conv}
  (f \star g)[n] = \sum_{m=-\infty}^{\infty} f[m]g[n - m],
\end{equation}
where \(\star\) is the convolution operator.

This is the mathematical definition of the discrete convolution. However, the
use of convolution in image processing does not work exactly as defined in
\vref{eq:math-conv}. In practice, the input images\footnote{At deeper levels in
  a \gls{cnn} we will stop understanding the layers as images and call them
  \emph{feature maps}.} are three-dimensional tensors for the images
\acs{rgb}. In convolutional layers, a two-dimensional convolution is made with
a three-dimensional filter (also called kernel) on each channel of the input
image and then all these feature maps are stacked in the output tensor, where
the number of output channels coincides with the number of filters in that
layer.

We can mathematically express the operation performed on the convolutional
layers as follows. Let \(\mathbf{X}\) be the input characteristics map,
\(\mathbf{Y}\) the output characteristics map and the filter
\(\mathbf{F}\). The convolution is then defined as follows,
\begin{equation} \label{eq:convolution}
  \mathbf{Y}_{i, j, k} =
  \sum_{l, m, n} \mathbf{X}_{l, j + m, k + n}\mathbf{F}_{i, l, m, n},
\end{equation}
where the sum is performed for all valid \(l, m, n\) indices (this will depend
on \emph{padding}\footnote{There are different techniques to define the
  padding of an image. The most typical is known as zero-padding, which
  consists of adding 0 vectors around the image until it is complete in order
  to carry out the desired convolutions.} of the input image).

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Convolution.png}
  \caption[Example of a convolution]{Example of a convolution with a kernel
    (filter) of dimensions \((3, 3, 1)\) and an input feature map (tensor) of
    size \((8, 8, 1)\).}
  \label{fig:convolution}
  \source{From TODO}
\end{figure}

An example of convolution is shown graphically in \vref{fig:convolution}. As
can be seen for each of the elements of the output tensor, the computation of
\vref{eq:convolution} is performed with the filter shown.

\subsubsection{Pooling Layers}\index{Pooling!layer}
Another type of layer used in this type of network is pooling. Pooling layers
reduce the dimension of the network by combining the output of neurons in one
layer into a single neuron in the next layer. These types of layers also add
non-linearities to the model. They also make \gls{cnn} less sensitive to small
local changes in spatial location.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Max pooling.png}
  \caption[Pooling layer example: max pooling]{Pooling layer example:
    \(2 \times 2\) max pooling. The input tensioner is divided into blocks
    \(2 \times 2\) and the Max Pool returns an output tensor with the maximum
    value of each block. From
    \captioncite{wiki18:max_poolin}.}\label{fig:max-pool}
\end{figure}

There are different types of pooling. The best known are \emph{max} pooling and
\emph{average} pooling. The first uses the maximum value of each cluster of
neurons in the previous level. Average pooling, however, uses the average
value. In \vref{fig:max-pool} we can see a representation of a \(2 \times 2\)
max-pool. In this specific case, the mathematical operation to be performed is
given by the following expression,
\begin{equation}
  f_{X,Y}(S) = \max _{a,b=0}^{1}S_{2X+a,2Y+b},
\end{equation}
where \(S\) is every depth slice in the input. This discharges 75 \% of the
activations.\index{Pooling!operation}

\subsubsection{Activation functions}\index{Activation function}
Analogously to fully connected neural networks, in this case it is also common
to use activation functions with the aim of introducing non-linearities in the
model without affecting the receptive fields of the convolutions. They are
typically added just after convolution.

The main functions used are the following:
\begin{itemize}
  \item \textbf{\gls*{relu}}. It is the function \(f (x) = \max \{0, x\}\),
  which is the most used. Removes negative values from the feature map and sets
  them to zero.
  \item \textbf{Tangente hiperbolica}. \(f (x) = \tanh x\)
  \item \textbf{Sigmoid function}. \(\sigma (x) ={(1 + e - x)} -1\).
  \item \(f (x) = | \tanh x |\)
\end{itemize}

\subsection{\glsentrylong{rnn}}\label{sec:rnn}

\glspl{rnn} are a class of \glspl{ann} that are widely used with temporal
sequences (as for example in the scope of \gls{nlp}). Analogously to the
\glspl{cnn} that are suitable for image processing, the \glspl{rnn} are a type
of neural network specialized to process sequences of values of the type
\(\mathbf{x}^1, \ldots \mathbf{x}^{\tau}\) with \(\mathbf{x}^t \in \R^n\).

Looking at \vref{fig:rnn}, we can see the architecture of a \gls{rnn}. The
input values \(\mathbf{x}^1, \ldots \mathbf{x}^\tau\) are fed into the network
in an orderly manner. Therefore, to obtain the output vector \(\mathbf{y}^t\),
the hidden state \(\mathbf{h}^{t - 1}\) and the input \(\mathbf{y}^{t}\) are
necessary. Prior inputs \(\mathbf{x}^1, \ldots \mathbf{x}^{t - 1}\) are
represented by the hidden state \(\mathbf{h}^{t - 1}\), therefore the output
\(\mathbf{y}^t\), depends exactly of all the inputs until time \(t\).

\begin{figure}[ht]
  \begin{subfigure}[t]{.2\textwidth}
    \centering
    \caption{Basic \gls{rnn}.}
    \includegraphics[height=4cm]{Images/RNN.png}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.8\textwidth}
    \centering
    \caption{Unfolded basic \gls{rnn}.}
    \includegraphics[height=4cm]{Images/Unfolded RNN.png}
  \end{subfigure}
  \caption[Basic topology of \acl*{rnn}]{Basic topology of \acf{rnn}. Two
    versions of the same network are shown, one is the compact version and the
    other is the same network but unfolded in time.}
  \label{fig:rnn}
  \source{From TODO. Citar es de stanford también}
\end{figure}

\subsubsection{Different types of \gls*{rnn}}
Feedforward networks send an input to an output\footnote{The input and output
  can be vector. It does not mean, therefore, that they are only a number, it
  means that they are only \emph{un} multidimensional vector of \(\R^n\).}, but
in the case of \glspl{rnn} it is not. There are different possibilities, in
which the length of the inlet and outlet can vary. All the different types of
architectures that may exist are listed in \vref{fig:rnn-types}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{Images/RNN types.png}
  \caption[Types of architectures for \acl*{rnn}]{Types of architectures for
    \gls{rnn}. All posibilities are considered (i.e., all combinations are
    presented).}
  \label{fig:rnn-types}
  \source{From TODO. Es de Stanford curso}
\end{figure}

These different possibilities then have their application in various fields
such as: image captioning (\re{one to many}), action prediction (\re{many to
  one}), video captioning (\re{many to many} first option), and video
classification on frame label (\re{many to many} second option). The extreme
case of \re{one to one} corresponds precisely to a normal fully connected
layered neural network such as the one studied in \vref{sec:fnn}.

\subsubsection{Model: feed forward}

We can compute the value of the neurons with a recurring formula, involving the
hidden states \(\mathbf{h}^t\) and the values of the input vectors
\(\mathbf{x}^t\). In the simplest case of a \emph{simple} \gls{rnn}, this
computation consists of,
\begin{equation} \label{eq:rnn-hidden}
  \mathbf{h}^t = f_{W} (\mathbf{h}^{t - 1}, \mathbf{x}^t),
\end{equation}
and then the output vector \(\mathbf{y}^t\) can be calculated using the
following expression,
\begin{equation} \label{eq:rnn-out}
  \mathbf{y}^t = W_{hy}\mathbf{h}^t,
\end{equation}
where \(W_{hy}\) is an array of parameters (trainable).

\subsubsection{Variant \gls*{rnn} architectures}
The model defined above is the simplest version for a \gls{rnn}. The
calculation of the output values with the \vref{eq:rnn-hidden,eq:rnn-out} can
cause problems with the gradients and, therefore, limit the ability of the
information to travel in time\footnote{These problems are know as
  vanishing/exploding gradient. The problem of \emph{exploding} gradients can
  be solved by gradient clipping (scaling it if the norm is too big). However
  for the \emph{vanishing} gradient problem, it is necessary to change the
  \gls{rnn} architecture.}. We introduce two new variants to solve these
problems:
\begin{itemize}
  \item \textbf{\gls*{lstm}}. A unit of \gls{lstm} is composed of different
  gates that define its behavior. Its name is given by the function they
  perform: \emph{input} gate (\(i\)), \emph{output} gate (\(o\)) and
  \emph{forget} gate (\(f\)). A graphical representation of this type of
  elements is shown in \vref{fig:lstm}.
  \begin{figure}[ht]
    \centering
    \includesvg[width=.6\textwidth]{SVGs/LSTM.svg}
    \caption[\acl*{lstm}]{\acf{lstm} representation. Input (\(i\)), output
      (\(o\)) and forget (\(f\)) gates are shown in the image. From
      \captioncite{enwiki:1005032489}.}\label{fig:lstm}
  \end{figure}

  The value of the different gates is described by the following equation,
  \begin{equation}
    \begin{pmatrix}
      i \\
      f \\
      o \\
      g
    \end{pmatrix} =
    \begin{pmatrix}
      \sigma \\
      \sigma \\
      \sigma \\
      \tanh
    \end{pmatrix}
    W
    \begin{pmatrix}
      h^{t-1} \\
      x^t
    \end{pmatrix}.
  \end{equation}
  And then, the value of the cell and the hidden state can be computed with,
  \begin{equation}
    \begin{cases}
      c_t &= f \odot c_{t-1} + i \odot g, \\
      h_t &= o \odot \tanh c_{t}.
    \end{cases}
  \end{equation}
  The operator \(\odot\) is the element wise multiplication known as Hadamard
  product.
  \item \textbf{\gls*{gru}}. They are another type of gating mechanism
  introduced by \myCite{cho14:learn_rnn}. Its operation is governed by the
  following system of equations,
  \begin{equation}
    \begin{cases}
      z_{t}&=\sigma _{g}(W_{z}x_{t}+U_{z}h_{t-1}+b_{z}), \\
      r_{t}&=\sigma _{g}(W_{r}x_{t}+U_{r}h_{t-1}+b_{r}), \\
      {\hat {h}}_{t}&=\phi _{h}(W_{h}x_{t}+U_{h}(r_{t}\odot h_{t-1})+b_{h}), \\
      h_{t}&=(1-z_{t})\odot h_{t-1}+z_{t}\odot {\hat {h}}_{t}.
    \end{cases}
  \end{equation}
  Where \(\hat{h}_t\) is known as the candidate activation vector, \(z_t\) is
  the update gate vector and \(r_t\) is the reset gate vector. The activation
  functions \(\sigma_g\) correspond to the sigmoid function and \(\phi_{h}\) is
  the hyperbolic tangent. Other activation functions would also be possible.
\end{itemize}

\subsubsection{Trainning: \acl*{bptt}}

Analogously to other models, here the training of \gls{rnn} is also carried out
using first-order optimization methods, i.e., calculating the partial
derivatives of the error function with respect to the model parameters.

In this specific case of \gls{rnn}, the backpropagation process is known as
\gls{bptt}, which is a generalization of backpropagation in feed-forward
networks.


\subsection{Transformers}\label{sec:transformers}\index{Transformer}

The Transformer is a deep learning model recently introduced by
\myCite{vaswani17:atten_all_you_need}. Here they presented the idea that
recurrent building blocks are not needed in a model to work well in \gls{nlp}
tasks. In the words of its authors:

\begin{quoteBox}
  The Transformer, the first sequence transduction model based entirely
  on attention, replacing the recurrent layers most commonly used in
  encoder-decoder architectures with multi-headed self-attention.
  \tcblower\quotecite{vaswani17:atten_all_you_need}
\end{quoteBox}

They propose a new architecture that is capable of maintaining an attention
mechanism while processing temporal sequences in parallel: the entire sequence
as a whole instead of going element by element.

\begin{figure}[p]
  \centering
  \includegraphics[height=.65\textheight]{Images/Transformer.png}
  \caption[Transformer model architecture]{Transformer model
    architecture/topology. Here you can observe the two main segments: the
    encoder and the decoder. From
    \captioncite{vaswani17:atten_all_you_need}.}\label{fig:transformer}
\end{figure}

Currently there are different types of architectures based on this idea. The
original architecture of the Transformer model is shown in
\vref{fig:transformer}. As you can see, two distinct segments can be
distinguished:
\begin{itemize}
  \item \textbf{Encoder segment}. It takes the inputs, generates an embedding
  of them, encodes the positions, computes where each word has to attend to in
  a multi-context setting and then outputs a new intermediate representation.
  \item \textbf{Decoder segment}. Take the entries in the target language,
  generate an embedding for them with encoded positions, calculate in which
  each word has to attend, and then combine the output of the encoder with the
  output so far. The result is a prediction for the next token.
\end{itemize}


\section{Trainning}\index{Trainning}

The process of trainning a neural networks consists in adjust the parameters of
the model to fit a particular dataset. Moreover, it is mandatory to define a
loss function function (also known as error function) that quantifies how good
or bad the neural network is performing when feeded with a dataset.

A dataset \(\Omega\) consists of input-output pairs \((x, y)\), where \(x\)
represents the input and \(y\) the \emph{desired} output. We shall denote the
size (cardinal) of the dataset by \(N\).

The loss function quantifies how different is the desired output \(y\) and the
calculated (\emph{predicted}) output \(\hat{y}\) of the neural network on input
\(x\) for a set of input-output pairs \((x , y) \in \Omega\) and a particular
value of the model parameters \(\theta\). We will denote the loss function by
\(\loss (\Omega, \theta)\) and we will assume that it is continuosly
differentiable (i.e., \(\mathcal{C}^1\)).\footnote{The regularity assummed for
  this functions is important, since we will be optimizing in the future by
  computing partial derivatives (as discussed in \vref{sec:optimization}).}

It is common (and we will assume it that way) that the loss funcion is a mean
of the errors of a particular pair \((x, y) \in \Omega\). This is, there exists
a continuosly differentiable function \(\ell (x, y, \Omega)\), such that,
\begin{equation} \label{eq:loss}
  \loss(\Omega, \theta) =
  \frac{1}{N} \sum_{(x, y) \in \Omega} \ell (x, y, \theta).
\end{equation}

Now, what we will want to do is to optimize (minimize) this loss function in
\(\theta\). This is, given a dataset \(\Omega\), we will want to approximate,
\begin{equation}
  \hat{\theta} = \argmin_{\theta} \loss(\Omega, \theta),
\end{equation}
given that the above exists.

\subsection{Optimization}\label{sec:optimization}\index{Optimization}

There exists several optimization techniques, amon then, the most important,
are gradient-based optimization algorithms. This techniques are iterative
methods that, given an initial value \(\theta^{(0)}\) for the parameters
proceed, as follows,
\begin{equation}
  \theta^{(t + 1)} = \theta^{(t)} + \alpha\,\Delta\theta^{(t)},
\end{equation}
where \(\alpha\) is called the step size (or learning rate), and
\(\Delta\theta^{(t)}\) is the weight update in step \(t\). The goal is to find
values for the parameters such that the loss function corresponds to a (global)
minimum.

However, obtaining a global minimum is a very hard task, so a \emph{local}
minimum will be enough. It is a well-known result from multivariable calculus
that if \(\hat{\theta}\) is a local minimum of \(\loss\), then
\(\nabla\loss(\hat{\theta}) = 0\), therefore the optimization methods will
focus in finding stationary points (that, hopefully, correspond to global or
local minimums).\footnote{These are called first-order optimization methods
  since they only focus in first-order (partial) derivatives of the loss
  function. Higher order methods exists, but involve computing the Hessian of
  \(\loss\) and in the scope of \gls{dl} it is too expensive in terms of
  computing time.}

\subsubsection{Optimization Methods}\index{Optimization!methods}
Diferentes métodos de optimización existen, el más básico y más conocido método
de first-order optimization es \emph{gradient descent}. La idea es moverse en
la dirección del gradiente y en sentido opuesto, es decir, es un método
iterativo que consiste en lo siguiente,
\begin{equation} \label{eq:gd}
  \Delta\theta^{(t)} = - \nabla\loss(\theta^{(t)}).
\end{equation}
Teniendo en cuenta la forma típica de la losss function (see \vref{eq:loss}),
el gradiente puede ser calculado usando la linealidad de las derivadas,
\begin{equation} \label{eq:nabla-loss}
  \nabla_{\theta}\,\loss(\Omega, \theta) =
  \frac{1}{N} \sum_{(x, y) \in \Omega} \nabla_{\theta}\,\ell (x, y, \theta).
\end{equation}

Típicamente los datasets \(\Omega\) usados para entrenar tienen cardinal muy
grande (\(N > 10^4\)), por lo que el computo de la \vref{eq:nabla-loss} es
demasiado costoso en términos de recursos computacionales. Por ello, en la
práctica, se usa la alternativa conocida como \gls{sgd}, que consiste en
estimar el gradiente en cada iteracción en un random subset from the dataset.

\gls{sgd} es una implementación de la \vref{eq:gd} en la que se estima
\(\nabla\loss(\theta^{(t)})\) usando un randomly chosen subset
\(B \subset \Omega\). Es decir, el cálculo de la \vref{eq:nabla-loss} se
sustituye por la siguiente estimación,
\begin{equation} \label{eq:nabla-loss-sto}
  \nabla_{\theta}\,\loss(\Omega, \theta) \approx
  \frac{1}{\lvert B\rvert} \sum_{(x, y) \in B} \nabla_{\theta}\,\ell (x, y, \theta).
\end{equation}
Es común hacer un abuso de notacion y denotar por \(B\) el cardinal del subset,
y llamarlo \emph{batch} size.

Otros métodos de optimización existen, principalmente debido a la presencia de
saddle points, que harían parar los métodos iterativos anteriores ya que el
gradiente se anularía. Los más conocidos son los siguientes,
\begin{itemize}
  \item \textbf{Momentum}\index{Momentum}. Consiste en realizar el siguiente
  cálculo,
  \begin{equation}
    \Delta\theta^{(t)} =
    -\beta\Delta\theta^{(t-1)} -\nabla\loss(\theta^{(t)}),
  \end{equation}
  donde \(\beta\) es un hiperparámetro. Este algoritmo, mantiene la
  ``velocity'' previa como una media de los anteriores gradientes y el
  parámetro \(\beta\) puede ser entendido como un parámetro de ``friction''
  (pensando en términos físicos)
  \item \textbf{Nesterov Momentum}\index{Nesterov momentum}. Similarmente al
  método anterior se usa un termino de ``velocity'', pero calculando el
  gradiente no en el punto actual, sino en el punto en el que la velocidad nos
  llevaría, i.e.,
    \begin{equation}
    \Delta\theta^{(t)} =
    -\beta\Delta\theta^{(t-1)} -\nabla\loss(\theta^{(t)} +
    \beta\Delta\theta^{(t-1)}).
  \end{equation}
\end{itemize}

TODO. Más métodos de optimización existen como es AdaGrad, RMSProp y Adam.

\subsubsection{Weight Initialization}\index{Weight initialization}
Para todo método iterativo, es necesario un initial guess for
\(\theta^{(0)}\). En la práctica lo que se hace es inicializar los weights
usando una distribución de probabilidad, como puede ser \(U[-\sigma, \sigma]\)
or \(\mathcal{N}(0, \sigma^2)\). El valor de \(\sigma\) puede ser elegido
arbitrariamente, aunque suele ser bastante común la conocida como \emph{Xavier
  initialization}, por \myCite{glorot10:under}, consistente en escoger la
varianza como la square root de la dimensión de entrada
\(D_{\text{in}}\).\footnote{For convolutional layers,
  \(D_{\text{in}} = \text{\code{filter\_size}}^2 \times
  \text{\code{input\_channels}}\).} De esta manera se consigue activations are
nicely scaled in all layers.

Sin embargo, en Xavier initialization asume que la función de activación está
centrada en 0, por ello, cuando se usa la functión de activación \gls{relu}, es
preferible el uso de otro tipo de weight initialization, como la propuesta por
\myCite{he15:delvin_deep_rectif}.

\subsubsection{Error Backpropagation}\index{Backpropgation}
Hey.

\subsection{Regularization Techniques}\index{Regularization}

Uno de los problemas más importantes a lo que se enfrenta el \gls{dl} y más
generalmente la estadística predictiva es el de
\emph{overfitting}\index{Overfitting}. Overfitting es crear un modelo de
análisis a partir de datos que se ajusta demasiado al dataset de entrenamiento
disponible, pero que no es capaz de realizar prediciones fiables de futuras
observaciones. En palabras de \myCite{berners-lee21:introd_web_acces} (TODO:
fix me),
\begin{quoteBox}
  With four parameters I can fit an elephant, and with five I can make him
  wiggle his trunk.
  \tcblower\quotecite{berners-lee21:introd_web_acces}TODO. corregir esto.
\end{quoteBox}

Aquí, a lo que se refiere, es que no debemos sorprendernos de la capacidad de
un modelo suficientemente complejo de adaptarse muy bien a un dataset dado. Los
modelos con una gran cantidad de parámetros son capaces de ajustarse a
cualquier cantidad de datos dada, incluso aunque esta parezca no seguir ningún
patrón. Pero estos ajustes serán meaningless, ya que no capurarán ninguna
información genuina de las estructura de los datos y, por tanto, no serán
capaces de generalizar correctamente. Tendremos un ajuste asombrosamente bueno
(incluso perfecto en algunos casos) para los datos disponibles actualmente,
pero con un error de generalización enorme (ver \vref{fig:overfit}).

\begin{figure}[ht]
  \centering
  \includegraphics[width=.65\textwidth]{Images/Overfit.png}
  \caption[Representación del fenómeno de overfitting.]{Representación del
    fenómeno de overfitting. Los datos con ruidos (que son aproximadamente
    lineales) han sido ajustan usando una función lineal y otra polinomial. La
    función polinomial presenta un ajuste perfecto sobre los datos existentes,
    pero es de esperar que generalice peor. Si ambas funciones se usaran para
    extrapolar fuera de los datos ajustados, la función lineal presentará
    mejores predicciones. From
    \captioncite{contributors21:overf}.}\label{fig:overfit}
\end{figure}

Un dicho conocido en el ámbito del \gls{ml} es el de ``Memorizing is \emph{not}
learning''. Es importante que tengamos en cuenta el efecto de overfitting a la
hora de entrenar los modelos. Para evitarlo existen diferentes técnicas, entre
las que destacan la \(L_2\) regularization, el concepto de early stopping y la
técnica de data augmentation.

\subsubsection{\(L_2\) Regularization}
This regularization technique, also known as \emph{ridge
  regression}\index{Ridge regression} in statistics and
\emph{Tikhonov}\index{Tikhonov regression} regularization in mathematics, it is
widely used in \gls{dl}. It consits in adding a term to the loss function that
takes into account the complexity of the model, i.e., minimize the following,
\begin{equation}
  \hat{\loss}(\Omega, \theta) =
  \loss(\Omega, \theta) + \lambda\,\text{complexity}(\theta),
\end{equation}
where \(\lambda\) is the regularization hyperparameter\footnote{Model
  \emph{hyperparameters} are properties that govern the entire training
  proccess and must be set before the training starts (therefore, they are not
  trainable).}\index{Hyperparameter} and the complexity is measured using the
\(L_2\) norm, i.e.,
\begin{equation}
  \text{complexity}(\theta) = \lVert\theta\rVert_2^2 = \sum_{w \in \theta} w^2.
\end{equation}

\subsubsection{Early Stopping}\index{Early stopping}
Problablemente el método más conocido y más utilizado a la hora de entrenar
modelos neuronales. Lo que se hace es dividir el dataset de \emph{training} en
dos: la parte de training y la parte de validation\index{Validation}
(\code{train/val} split).\footnote{En consecuencia, lo que se debe hacer con el
  dataset completo es divirlo realmente en tres, el famoso
  \code{train/val/test} split.}. Para el proceso de entrenamiento solo se usará
la parte de \code{train} (i.e., para la actualización de los parámetros del
modelo). La parte de \code{val} se usará para determinar cuándo debemos de
parar el proceso de entrenamiento para evitar el
overfitting\index{Overfitting}.

De esta manera, mientras iteramos en el proceso de entrenamiento, podemos
calcular la función de loss tanto en el dataset de \code{train} como en el de
\code{val} (see \vref{fig:early}). Con el objetivo en mente de encontrar
modelos que generalicen bien, pararemos el entrenamiento de la red neuronal en
el momento que la función de error en el dataset de validación comience a
aumentar.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.45\textwidth]{Images/Early stopping.jpg}
  \caption[Early stopping regularization technique]{Early stopping
    regularization technique. En el comienzo del proceso de entrenamiento la
    función de error disminuye en los dos datasets de entrenamiento y de
    validación a la vez. En el momento en que la función de error en el
    validation set comienza a aumentar es cuando comienza a presentarse el
    problema de overfitting, por lo que ahí pararemos el entrenamiento. From
    \captioncite{brownlee28:gentl_introd_early_stopp_avoid}.}\label{fig:early}
\end{figure}

Este método de regularización ha sido usado en este trabajo (ver
\vref{cha:model}) para el entrenamiento de los modelos.

\subsubsection{Data Augmentation}\index{Data augmentation}
El uso de data augmentation como técnica de regularización es el proceso de
generar de manera artificial nuevos training samples a partir de los existentes
usando random transformations. En el tratamiento de imágenes, las
transformaciones más típicas son: color transformation, geometric translation,
rotations, etc.

De esta manera aumentamos el cardinal del dataset (lo cual previene el
overfitting) y también forzamos de manera artificial un dataset más
generalista. Esto, en consecuencia, conseguirá que el modelo entrenado
generalice mejor y sea invariante a las transformaciones realizadas en el
dataset.

\begin{remarkBox}
  Es importante no excederse en esta técnica, ya que nos podría llevar a
  escenarios demasiado sintéticos y que, por tanto, generemos modelos que no
  funcionen bien en el mundo real. Una evaluación cualitativa de los datos
  creados puede ser de utilidad para validar esta técnica.
\end{remarkBox}


\section{Testing}
Hey there. Talk about evaluation measures \textbf{in general}. Talk here about
the importance of the split test/val/test.
