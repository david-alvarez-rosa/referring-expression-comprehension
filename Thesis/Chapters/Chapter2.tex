% -*- TeX-master: "../Thesis.tex" -*-


\chapter{Theoretical Background}\label{cha:theory}

\epigraphhead[75]{
  \epigraph{\itshape
    Wwithout theory, there is nothing to revise. \\
    Without theory, experience has no meaning. \\
    Without theory, one has no questions to ask. \\
    Hence, without theory, there is no learning.}
  {---\scshape William Edwards}}


\drop Neural networks\index{Neural network}, more properly referred to as
\glspl{ann} are computing systems vaguely inspired by the biological neural
networks that constitute animal brains. Dr.\ Robert Hecht-Nielsen (inventor of
one of the first neurocomputers), in an article by
\myCite{caudill87:neural_networ_primer_part_i}, defines a neural network as:

\begin{quoteBox}
  \ldots a computing system made up of a number of simple, highly
  interconnected processing elements, which process information by their
  dynamic state response to external inputs.
  \tcblower\quotecite{caudill87:neural_networ_primer_part_i}
\end{quoteBox}

\subsubsection{Mathematical perspective}

Although the analogy made above of an \gls{ann} with a biological brain, there
is no need for this, we can just think of a neural network as a mathematical
optimization problem. We can think of the whole network to be a function that
takes some inputs to some outputs, and this function dependent on
parameters. The idea is to adjust this parameters to get a function that works
well with some known dataset, and we will trust that it will generalize
well. If the network is big enough and we carefully adjust the parameters, we
will be able to learn and calculate very complex functions.



\section{Tensors}\index{Tensor}

In our context, it will be useful to use the mathematical tool of
tensors. There are different approaches to define this mathematical objetcs,
among then, defining them as multi-dimensional arrays of real numbers, as
follows: a \emph{tensor} is an element
\(\mathbf{T} \in \R^{n_1 \times \cdots \times n_r}\), with
\(n_1, \ldots, n_r \in \N\). The number \(r\) is called the \emph{rank} of the
tensor.

Similarly to real vectors from \(\R^n\), each of the elements of a tensor
\(\mathbf{T}\) can be referred to using a multi-index
\(i = (i_1, \ldots, i_r) \in \N^r\). The notation \(\mathbf{T}_i\), or
\(\mathbf{T}_{i_1, \ldots, i_r}\), refers to the element indexed by \(i\) of
the tensor \(\mathbf{T}\).

\begin{exampleBox}
  Examples of tensors include scalars (view as tensors of rank \(0\)), vectors
  (rank \(1\) tensors) and matrices (rank \(2\)).
\end{exampleBox}

Most popular \gls{dl} frameworks such as PyTorch (used in this thesis), are
just programming libraries that provide efficient tensor data strctures and and
fast tensor computation (with the ability to compute in \gls{gpu}). These
tensors store the data used to train neuronal models. In the case of \gls{cv},
the following tensors appear:
\begin{itemize}
  \item \textbf{\acs*{rgb} Image}. An \acs{rgb} image can be interpreted as a
  tensor of rank 3, \(\mathbf{I} \in \R^{C \times H \times W}\), where \(C\)
  corresponded to the number of channels (i.e., in this case, \(C = 3\)), \(H\)
  corresponds to the height of the image and \(W\) to its
  width.\footnote{Grayscale images also fall into this category with just 1
    channel (i.e., using the above notation \(C = 1\)).}
  \item \textbf{Batch of \acs*{rgb} Images}. A batch of \acs{rgb} images is a
  set of \acs{rgb} images, therefore, can be interpreted as a tensor of rank 4,
  \(\mathbf{I} \in \R^{B \times C \times H \times W}\), where \(B\) corresponds
  to the batch size, and \((C, H, W)\) have the same meaning as in the case of
  an \acs{rgb} image.
\end{itemize}


\subsection{Tensor Operations}\index{Tensor!operations}

Tensors or rank 1 or 2 correspond to real vectors and real matrices, therefore,
all basic operations from linear algebra apply (e.g., vector sum, matrix-vector
multiplication). For tensor of higher rank, it is useful define element-wise
operations for any binary operation from real numbers.

Let \(\mathbf{T}\) and \(\mathbf{S}\) be tensors of rank \(r\). Let \(\star\)
be a binary operation for real numbers (e.g., ordinary sum or
multiplication). Then, the element-wise operation is defined as follows,
\begin{equation}
  (\mathbf{T} \star \mathbf{S})_i = \mathbf{T}_i \star \mathbf{S}_i,
\end{equation}
for all multi-indices \(i \in \N^r\).



\section{Neural Network Architectures}\index{Neural network}

During the last few years, there have been significant advances in the field of
artificial intelligence and multiple new models of neural networks have
appeared. In a very general classification of neural models we can find
different ones.

\begin{itemize}\index{Neural network!architectures}
  \item \textbf{\acl*{fnn}}. These networks are the simplest type of neural
  model that exists and on which more complex models are built. A more detailed
  description of this type of network can be found in
  \vref{sec:fnn}.
  \item \textbf{\acl*{cnn}}. This type of neural network is especially useful
  in image processing, since it allows preserving the spatial information of
  the relationship between pixels. They will be studied in
  \vref{sec:conv-neur-netw}.
  \item \textbf{\acl*{rnn}}. Ideal neural models for the analysis of time
  series (such as text or audio). They will be studied in
  \vref{sec:rnn}.
  \item \textbf{Transformers}. The transformer-based models are part of the
  current state of the art.\footnote{On \today, current date of the document.}
  They are being used extensively in \gls{nlp} and studying their possible
  applications in computer vision and image processing. They will be described
  in \vref{sec:transformers}.
\end{itemize}


\subsection{\glsentrylong{fnn}}\label{sec:fnn}

Fully connected layered neural networks are the simplest type of neural model
that exists and that constitute the basis for more complex neural structures.

This type of neural network is characterized by a neural structure organized in
different layers, so that the neurons between adjacent layers are connected to
each other by arcs. An example of this type of architecture is shown in
\vref{fig:fully-conn-network}.

\begin{figure}[ht]
  \centering
  \input{Figures/Tikz/Fully Connected Neural Network.tex}
  \caption[Fully Connected Neural Network topology]{Example of Fully Connected
    Layered Neural Network. It consists of 4 differentiated layers of neurons
    and the connections between them are shown graphically with
    arrows. \textsl{Source}: figure created by the
    author.}\label{fig:fully-conn-network}
\end{figure}

Next we will discuss the main constituent parts of this type of neural
networks: layers, neurons and connections.

\subsubsection{Layers}\index{Layer}

The layers are just a collection of neurons, we will distinguish between three
types depending on its position in the network: \emph{input} layer (patterns
are presented to the network via this layer), \emph{hidden} layer (all the
inner layers) and \emph{output} layer (is the last layer, where the answer is
obtained).

We will denote with \(L\) the number of layers and with \(n_l\) the size of the
\(l\)-th layer.

\subsubsection{Neurons}\index{Neuron}

Neurons are the core component of any neural network. Basically there are three
subparts that form a neuron.
\begin{itemize}
  \item \textbf{Value}. Each neuron holds a value, it will be denoted by
  \(x_i^l \in \R\) for the \(i\)-th neuron in the \(l\)-th layer. Of course, it
  should be satisfied \(1 \leq i \leq n_l\). We will use the notation
  \(\mathbf{x}^l\) for the vector of all the values in the \(l\)-th level. When
  we speak of the input vector, we may ommit the superindex, i.e., we will use
  \(\mathbf{x}\) to denote \(\mathbf{x}^0\). Similarly, for the output layer,
  we will use \(\mathbf{\hat{y}}\) to refer to \(\mathbf{x}^L\).
  \item \textbf{Bias}. Also each neuron has a bias, denoted as \(b_i^l\) for
  the \(i\)-th neuron in the \(l\)-th layer. Is then true that
  \(1 \leq i \leq n_l\). The vector of all biases in the \(l\)-th layer will be
  denoted by \(\mathbf{b}^l\).
  \item \textbf{Activation function}. All neurons have an activation function
  \(f_i ^ l \in \mathcal{C} ^ 1 (\R, \R)\) for the \(i\)-th neuron in the
  \(l\)-th layer.\footnote{Usually all the activation functions are
    neuron-independent (i.e., \(f_i^l\) does not really depend on \(i\) or
    \(l\)).} Of course, it is needed \(1 \leq i \leq n_l\). The regularity
  assummed for this functions is important, since we will be optimizing in the
  future by taking derivatives.
\end{itemize}

\subsubsection{Connections}\index{Connection}

As we discussed, in the topology of this network, all neurons between adjacent
layers are required to be connected, this are the connections, that should have
associated a \emph{weight}. For the connection between the \(i\)-th neuron in
the \(l\)-th layer and the \(j\)-th neuron in the \(l + 1\)-th layer, we will
denote this weight by \(w_{ij}^l \in \R\). The set of all this weights is as
follows,
\begin{equation}
  \{w_{ij}^l \mid 1 \leq i \leq n_{l}, \, 1 \leq j \leq n_{l+1}, \,
  1 \leq l < L \} \subset \R.
\end{equation}

The \emph{matrix} of all weights in the \(l\)-th layer will be denoted by
\(\mathbf{W}^l\). This is, \((\mathbf{W}^l)_{ij} = w_{ij}^ l\).

\begin{exampleBox}
  To gain some intuition on how this work, let's think about the handwritten
  recognition problem. Suppose we have a set of images with handwritten digits
  in it and that we will like to implement an ANN that is capable of
  recognizing that digits.

  In this case, if the digits images are of \(28 \times 28\) pixels, the input
  layer will consist of 784 (\(28 \times 28\)) neurons and each neuron will
  hold the grayscale value of a pixel. For the output layer we will need 10
  neurons (one for each number between 0 and 9), and we will like that when we
  feed our network with an image holding a handwritten number 3, then the ouput
  is one 1 in the position corresponding to the number 3 and the rest of zeros.
\end{exampleBox}

\subsubsection{Model: feed forward}\label{subsec:forward}\index{Feed forward}

From now on let's suppose we are working with a fully-connected neural network,
with different layers and we will be using the same notation used before. The
values of the neurons can be computed with
\begin{equation}
  x_i^l = f_i^l \left(
    \sum_{k=1}^{n_{l-1}} w_{ik}^{l-1} x_{k}^{l-1} + b_i^l
  \right).
\end{equation}

This formula is sometimes referred to as the feed-forward formula or forward
propagation. It's important to note that it's a recursive formula, once the
values the neurons in the the input layer are known, we can iterate computing
the values of the neurons in the next adjacent layer, until we reach the output
layer. In this network we can think of information traveling in one direction,
forward, from the input layer, through the hidden layers to the output neurons.

\subsubsection{Trainning: backpropagation}%
\label{sec:backward}\index{Backpropagation}

In order to be able to train our neural network, it is mandatory to define an
error function (also known as loss function) that quantifies how good or bad
the neural network is performing when feeded with a particular dataset. We will
use the below notation.
\begin{itemize}
  \item \textbf{Dataset}. Will be denoted by and consists of input-output pairs
  \((\mathbf{x}, \mathbf{y})\), where \(\mathbf{x}\) represents the input and
  \(\mathbf{y}\) the \emph{desired} output. We shall denote the size (cardinal)
  of the dataset by \(N\). Of course, in terms of our ANN \(x\) corresponds to
  the values of the first (or input) layer and \(\mathbf{y}\) to the output (or
  last) layer.
  \item \textbf{Parameters}. Parameters of our ANN are both the connection
  weights \(w_{ij}^l\) and the biases \(b_i^l\), we will denote the set of all
  parameters by \(\bm{\theta}\). Keep in mind that \(\bm{\theta}\) is just a
  set of real vectors of \(\R^D\) where \(D\) denotes the number of weights and
  biases.
\end{itemize}

The error function quantifies how different is the desired output
\(\mathbf{y}\) and the calculated (\emph{predicted}) output
\(\mathbf{\hat{y}}\) of the neural network on input \(\mathbf{x}\) for a set of
input-output pairs \((\mathbf{x} , \mathbf{y}) \in \Omega\) and a particular
value of the parameters \(\theta\). We will denote the error function by
\(E (\Omega, \theta)\) and we will assume that it is continuosly differentiable
(i.e., \(\mathcal{C}^1\)).


\subsection{\glsentrylong{cnn}}%
\label{sec:conv-neur-netw}\index{Convolutional!network}

\glspl{cnn} are a class of \glspl{dnn} that are widely used in computer
vision. They are commonly applied in image analysis, since they have the main
characteristic of having the ability to preserve the spatial relationship
between pixels. Likewise, they are regularized versions of fully connected
networks (explained in \vref{sec:fnn}), that is, it is less likely
to overfit the data.

\fhref{http://yann.lecun.com/index.html}{Yann LeCun}, a renowned scientist
known as the father of \glspl{cnn}, defines them as follows:
\begin{quoteBox}
  Convolutional Neural Networks are a special kind of multi-layer neural
  networks. Like almost every other neural networks they are trained with a
  version of the back-propagation algorithm. Where they differ is in the
  architecture.
  \tcblower\href{http://yann.lecun.com/index.html}{Yann LeCun}
\end{quoteBox}

Analogously to the case of fully connected networks, a \gls{cnn} consists of an
input layer, hidden layers and an output layer. In a \gls{cnn} the hidden
layers include layers that calculate convolutions. These convolutional layers
are followed by different ones, among which the pooling layers, fully connected
layers and normalization layers stand out. An example of this type of
architecture can be seen in \vref{fig:lenet-5}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{Images/Architecture LeNet-5.png}
  \caption[Example of topology of a \glsentryshort{cnn} (LeNet-5)]{Example of
    topology of a \gls{cnn}. Specifically, it is
    \href{http://yann.lecun.com/exdb/lenet}{LeNet-5} (see
    \url{http://yann.lecun.com/exdb/lenet}), a convolutional network created by
    Yann LeCun.}\label{fig:lenet-5}
  \source{From TODO}
\end{figure}

\subsubsection{Convolutional layers}\index{Convolutional!layer}

The name comes from the fact that these layers use the mathematical operation
of \fhref{https://en.wikipedia.org/wiki/Convolution}{convolution}. This
operation is defined (in its discrete version) as follows: given a pair of
functions \(f, g\) defined on the set of integers \(\Z\), the discrete
convolution between \(f\) and \(g\) is given by,
\begin{equation} \label{eq:math-conv}
  (f \star g)[n] = \sum_{m=-\infty}^{\infty} f[m]g[n - m],
\end{equation}
where \(\star\) is the convolution operator.

This is the mathematical definition of the discrete convolution. However, the
use of convolution in image processing does not work exactly as defined in
\vref{eq:math-conv}. In practice, the input images\footnote{At deeper levels in
  a \gls{cnn} we will stop understanding the layers as images and call them
  \emph{feature maps}.} are three-dimensional tensors for the images
\acs{rgb}. In convolutional layers, a two-dimensional convolution is made with
a three-dimensional filter (also called kernel) on each channel of the input
image and then all these feature maps are stacked in the output tensor, where
the number of output channels coincides with the number of filters in that
layer.

We can mathematically express the operation performed on the convolutional
layers as follows. Let \(\mathbf{X}\) be the input characteristics map,
\(\mathbf{Y}\) the output characteristics map and the filter
\(\mathbf{F}\). The convolution is then defined as follows,
\begin{equation} \label{eq:convolution}
  \mathbf{Y}_{i, j, k} =
  \sum_{l, m, n} \mathbf{X}_{l, j + m, k + n}\mathbf{F}_{i, l, m, n},
\end{equation}
where the sum is performed for all valid \(l, m, n\) indices (this will depend
on \emph{padding}\footnote{There are different techniques to define the
  padding of an image. The most typical is known as zero-padding, which
  consists of adding 0 vectors around the image until it is complete in order
  to carry out the desired convolutions.} of the input image).

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Convolution.png}
  \caption[Example of a convolution]{Example of a convolution with a kernel
    (filter) of dimensions \((3, 3, 1)\) and an input feature map (tensor) of
    size \((8, 8, 1)\).}
  \label{fig:convolution}
  \source{From TODO}
\end{figure}

An example of convolution is shown graphically in \vref{fig:convolution}. As
can be seen for each of the elements of the output tensor, the computation of
\vref{eq:convolution} is performed with the filter shown.

\subsubsection{Pooling Layers}\index{Pooling!layer}
Another type of layer used in this type of network is pooling. Pooling layers
reduce the dimension of the network by combining the output of neurons in one
layer into a single neuron in the next layer. These types of layers also add
non-linearities to the model. They also make \gls{cnn} less sensitive to small
local changes in spatial location.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Max pooling.png}
  \caption[Pooling layer example: max pooling]{Pooling layer example:
    \(2 \times 2\) max pooling. The input tensioner is divided into blocks
    \(2 \times 2\) and the Max Pool returns an output tensor with the maximum
    value of each block. \textsc{\textsl{source:}} from
    \captioncite{wiki18:max_poolin}.}\label{fig:max-pool}
\end{figure}

There are different types of pooling. The best known are \emph{max} pooling and
\emph{average} pooling. The first uses the maximum value of each cluster of
neurons in the previous level. Average pooling, however, uses the average
value. In \vref{fig:max-pool} we can see a representation of a \(2 \times 2\)
max-pool. In this specific case, the mathematical operation to be performed is
given by the following expression,
\begin{equation}
  f_{X,Y}(S) = \max _{a,b=0}^{1}S_{2X+a,2Y+b},
\end{equation}
where \(S\) is every depth slice in the input. This discharges 75 \% of the
activations.\index{Pooling!operation}

\subsubsection{Activation functions}\index{Activation function}

Analogously to fully connected neural networks, in this case it is also common
to use activation functions with the aim of introducing non-linearities in the
model without affecting the receptive fields of the convolutions. They are
typically added just after convolution.

The main functions used are the following:
\begin{itemize}
  \item \textbf{\gls*{relu}}. It is the function \(f (x) = \max \{0, x\}\),
  which is the most used. Removes negative values from the feature map and sets
  them to zero.
  \item \textbf{Tangente hiperbolica}. \(f (x) = \tanh x\)
  \item \textbf{Sigmoid function}. \(\sigma (x) ={(1 + e - x)} -1\).
  \item \(f (x) = | \tanh x |\)
\end{itemize}


\subsection{\glsentrylong{rnn}}\label{sec:rnn}\index{Recurrent network}

\glspl{rnn} are a class of \glspl{ann} that are widely used with temporal
sequences (as for example in the scope of \gls{nlp}). Analogously to the
\glspl{cnn} that are suitable for image processing, the \glspl{rnn} are a type
of neural network specialized to process sequences of values of the type
\(\mathbf{x}^1, \ldots \mathbf{x}^{\tau}\) with \(\mathbf{x}^t \in \R^n\).

Looking at \vref{fig:rnn}, we can see the architecture of a \gls{rnn}. The
input values \(\mathbf{x}^1, \ldots \mathbf{x}^\tau\) are fed into the network
in an orderly manner. Therefore, to obtain the output vector \(\mathbf{y}^t\),
the hidden state \(\mathbf{h}^{t - 1}\) and the input \(\mathbf{y}^{t}\) are
necessary. Prior inputs \(\mathbf{x}^1, \ldots \mathbf{x}^{t - 1}\) are
represented by the hidden state \(\mathbf{h}^{t - 1}\), therefore the output
\(\mathbf{y}^t\), depends exactly of all the inputs until time \(t\).

\begin{figure}[ht]
  \begin{subfigure}[t]{.2\textwidth}
    \centering
    \caption{Basic \gls{rnn}.}
    \includegraphics[height=4cm]{Images/RNN.png}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.8\textwidth}
    \centering
    \caption{Unfolded basic \gls{rnn}.}
    \includegraphics[height=4cm]{Images/Unfolded RNN.png}
  \end{subfigure}
  \caption[Basic topology of \glsentrylong{rnn}]{Basic topology of
    \acf{rnn}. Two versions of the same network are shown, one is the compact
    version and the other is the same network but unfolded in time.}
  \label{fig:rnn}
  \source{From TODO. Citar es de stanford también}
\end{figure}

\subsubsection{Different types of \glsentryshort{rnn}}

Feedforward networks send an input to an output\footnote{The input and output
  can be vector. It does not mean, therefore, that they are only a number, it
  means that they are only \emph{un} multidimensional vector of \(\R^n\).}, but
in the case of \glspl{rnn} it is not. There are different possibilities, in
which the length of the inlet and outlet can vary. All the different types of
architectures that may exist are listed in \vref{fig:rnn-types}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{Images/RNN types.png}
  \caption[Types of architectures for \glsentrylong{rnn}]{Types of
    architectures for \gls{rnn}. All posibilities are considered (i.e., all
    combinations are presented).}
  \label{fig:rnn-types}
  \source{From TODO. Es de Stanford curso}
\end{figure}

These different possibilities then have their application in various fields
such as: image captioning (\re{one to many}), action prediction (\re{many to
  one}), video captioning (\re{many to many} first option), and video
classification on frame label (\re{many to many} second option). The extreme
case of \re{one to one} corresponds precisely to a normal fully connected
layered neural network such as the one studied in \vref{sec:fnn}.

\subsubsection{Model: feed forward}\index{Feed forward}

We can compute the value of the neurons with a recurring formula, involving the
hidden states \(\mathbf{h}^t\) and the values of the input vectors
\(\mathbf{x}^t\). In the simplest case of a \emph{simple} \gls{rnn}, this
computation consists of,
\begin{equation} \label{eq:rnn-hidden}
  \mathbf{h}^t = f_{W} (\mathbf{h}^{t - 1}, \mathbf{x}^t),
\end{equation}
and then the output vector \(\mathbf{y}^t\) can be calculated using the
following expression,
\begin{equation} \label{eq:rnn-out}
  \mathbf{y}^t = W_{hy}\mathbf{h}^t,
\end{equation}
where \(W_{hy}\) is an array of parameters (trainable).

\subsubsection{Variant \glsentryshort{rnn} architectures}

The model defined above is the simplest version for a \gls{rnn}. The
computation of the output values with the \vref{eq:rnn-hidden,eq:rnn-out} can
cause problems with the gradients and, therefore, limit the ability of the
information to travel in time\footnote{These problems are know as
  vanishing/exploding gradient. The problem of \emph{exploding} gradients can
  be solved by gradient clipping (scaling it if the norm is too big). However
  for the \emph{vanishing} gradient problem, it is necessary to change the
  \gls{rnn} architecture.}. We introduce two new variants to solve these
problems:
\begin{itemize}
  \item \textbf{\gls*{lstm}}. A unit of \gls{lstm} is composed of different
  gates that define its behavior. Its name is given by the function they
  perform: \emph{input} gate (\(i\)), \emph{output} gate (\(o\)) and
  \emph{forget} gate (\(f\)). A graphical representation of this type of
  elements is shown in \vref{fig:lstm}.
  \begin{figure}[ht]
    \centering
    \includesvg[width=.6\textwidth]{SVGs/LSTM.svg}
    \caption[\glsentrylong{lstm}]{\acf{lstm} representation. Input (\(i\)),
      output (\(o\)) and forget (\(f\)) gates are shown in the image. From
      \captioncite{enwiki:1005032489}.}\label{fig:lstm}
  \end{figure}

  The value of the different gates is described by the following equation,
  \begin{equation}
    \begin{pmatrix}
      i \\
      f \\
      o \\
      g
    \end{pmatrix} =
    \begin{pmatrix}
      \sigma \\
      \sigma \\
      \sigma \\
      \tanh
    \end{pmatrix}
    W
    \begin{pmatrix}
      h^{t-1} \\
      x^t
    \end{pmatrix}.
  \end{equation}
  And then, the value of the cell and the hidden state can be computed with,
  \begin{equation}
    \begin{cases}
      c_t &= f \odot c_{t-1} + i \odot g, \\
      h_t &= o \odot \tanh c_{t}.
    \end{cases}
  \end{equation}
  The operator \(\odot\) is the element wise multiplication known as Hadamard
  product.
  \item \textbf{\gls*{gru}}. They are another type of gating mechanism
  introduced by \myCite{cho14:learn_rnn}. Its operation is governed by the
  following system of equations,
  \begin{equation}
    \begin{cases}
      z_{t}&=\sigma _{g}(W_{z}x_{t}+U_{z}h_{t-1}+b_{z}), \\
      r_{t}&=\sigma _{g}(W_{r}x_{t}+U_{r}h_{t-1}+b_{r}), \\
      {\hat {h}}_{t}&=\phi _{h}(W_{h}x_{t}+U_{h}(r_{t}\odot h_{t-1})+b_{h}), \\
      h_{t}&=(1-z_{t})\odot h_{t-1}+z_{t}\odot {\hat {h}}_{t}.
    \end{cases}
  \end{equation}
  Where \(\hat{h}_t\) is known as the candidate activation vector, \(z_t\) is
  the update gate vector and \(r_t\) is the reset gate vector. The activation
  functions \(\sigma_g\) correspond to the sigmoid function and \(\phi_{h}\) is
  the hyperbolic tangent. Other activation functions would also be possible.
\end{itemize}

\subsubsection{Trainning: \glsentrylong{bptt}}%
\index{Backpropagation!through time}

Analogously to other models, here the training of \gls{rnn} is also carried out
using first-order optimization methods, i.e., calculating the partial
derivatives of the error function with respect to the model parameters.

In this specific case of \gls{rnn}, the backpropagation process is known as
\gls{bptt}, which is a generalization of backpropagation in feed-forward
networks.


\subsection{Transformers}\label{sec:transformers}\index{Transformer}

The Transformer is a deep learning model recently introduced by
\myCite{vaswani17:atten_all_you_need}. Here they presented the idea that
recurrent building blocks are not needed in a model to work well in \gls{nlp}
tasks. In the words of its authors:

\begin{quoteBox}
  The Transformer, the first sequence transduction model based entirely
  on attention, replacing the recurrent layers most commonly used in
  encoder-decoder architectures with multi-headed self-attention.
  \tcblower\quotecite{vaswani17:atten_all_you_need}
\end{quoteBox}

They propose a new architecture that is capable of maintaining an attention
mechanism while processing temporal sequences in parallel: the entire sequence
as a whole instead of going element by element.

\begin{figure}[p]
  \centering
  \includegraphics[height=.65\textheight]{Images/Transformer.png}
  \caption[Transformer model architecture]{Transformer model
    architecture/topology. Here you can observe the two main segments: the
    encoder and the decoder. From
    \captioncite{vaswani17:atten_all_you_need}.}\label{fig:transformer}
\end{figure}

Currently there are different types of architectures based on this idea. The
original architecture of the Transformer model is shown in
\vref{fig:transformer}. As you can see, two distinct segments can be
distinguished:
\begin{itemize}
  \item \textbf{Encoder segment}. It takes the inputs, generates an embedding
  of them, encodes the positions, computes where each word has to attend to in
  a multi-context setting and then outputs a new intermediate representation.
  \item \textbf{Decoder segment}. Take the entries in the target language,
  generate an embedding for them with encoded positions, calculate in which
  each word has to attend, and then combine the output of the encoder with the
  output so far. The result is a prediction for the next token.
\end{itemize}



\section{Trainning}\index{Trainning}

The process of trainning a neural networks consists in adjust the parameters of
the model to fit a particular dataset. Moreover, it is mandatory to define a
loss function function (also known as error function) that quantifies how good
or bad the neural network is performing when feeded with a dataset.

A dataset \(\Omega\) consists of input-output pairs \((x, y)\), where \(x\)
represents the input and \(y\) the \emph{desired} output. We shall denote the
size (cardinal) of the dataset by \(N\).

The loss function quantifies how different is the desired output \(y\) and the
calculated (\emph{predicted}) output \(\hat{y}\) of the neural network on input
\(x\) for a set of input-output pairs \((x , y) \in \Omega\) and a particular
value of the model parameters \(\theta\). We will denote the loss function by
\(\loss (\Omega, \theta)\) and we will assume that it is continuosly
differentiable (i.e., \(\mathcal{C}^1\)).\footnote{The regularity assummed for
  this functions is important, since we will be optimizing in the future by
  computing partial derivatives (as discussed in \vref{sec:optimization}).}

It is common (and we will assume it that way) that the loss funcion is a mean
of the errors of a particular pair \((x, y) \in \Omega\). This is, there exists
a continuosly differentiable function \(\ell (x, y, \Omega)\), such that,
\begin{equation} \label{eq:loss}
  \loss(\Omega, \theta) =
  \frac{1}{N} \sum_{(x, y) \in \Omega} \ell (x, y, \theta).
\end{equation}

Now, what we will want to do is to optimize (minimize) this loss function in
\(\theta\). This is, given a dataset \(\Omega\), we will want to approximate,
\begin{equation}
  \hat{\theta} = \argmin_{\theta} \loss(\Omega, \theta),
\end{equation}
given that the above exists.


\subsection{Optimization}\label{sec:optimization}\index{Optimization}

There exists several optimization techniques, amon then, the most important,
are gradient-based optimization algorithms. This techniques are iterative
methods that, given an initial value \(\theta^{(0)}\) for the parameters
proceed, as follows,
\begin{equation}
  \theta^{(t + 1)} = \theta^{(t)} + \alpha\,\Delta\theta^{(t)},
\end{equation}
where \(\alpha\) is called the step size (or learning rate), and
\(\Delta\theta^{(t)}\) is the weight update in step \(t\). The goal is to find
values for the parameters such that the loss function corresponds to a (global)
minimum.

However, obtaining a global minimum is a very hard task, so a \emph{local}
minimum will be enough. It is a well-known result from multivariable calculus
that if \(\hat{\theta}\) is a local minimum of \(\loss\), then
\(\nabla\loss(\hat{\theta}) = 0\), therefore the optimization methods will
focus in finding stationary points (that, hopefully, correspond to global or
local minimums).\footnote{These are called first-order optimization methods
  since they only focus in first-order (partial) derivatives of the loss
  function. Higher order methods exists, but involve computing the Hessian of
  \(\loss\) and in the scope of \gls{dl} it is too expensive in terms of
  computing time.}

\subsubsection{Optimization Methods}\index{Optimization!methods}

Different optimization methods exist, the most basic and best known method of
first-order optimization is \emph{gradient descent}. The idea is to move in the
direction of the gradient and in the opposite direction, that is, it is a
method iterative consisting of the following,
\begin{equation}\label{eq:gd}
  \Delta\theta^{(t)} = - \nabla\loss(\theta^{(t)}).
\end{equation}
Considering the typical form of the losss function (see \vref{eq:loss}), the
gradient can be calculated using the linearity of the derivatives,
\begin{equation}\label{eq:nabla-loss}
  \nabla_{\theta}\,\loss(\Omega, \theta) =
  \frac{1}{N} \sum_{(x, y) \in \Omega} \nabla_{\theta}\,\ell (x, y, \theta).
\end{equation}

Typically the datasets \(\Omega\) used for training have very cardinal large
(\(N > 10^4\)), so the computation of \vref{eq:nabla-loss} is too expensive in
terms of computational resources. Therefore, in the In practice, the
alternative known as \gls{sgd} is used, which consists of estimate the gradient
at each iteration in a random subset from the dataset.

\gls{sgd} is an implementation of \vref{eq:gd} in which it is estimated
\(\nabla \loss (\theta^{(t)})\) using a randomly chosen subset
\(B \subset \Omega\). That is, the computation of \vref{eq:nabla-loss} is
replace with the following estimate,
\begin{equation}\label{eq:nabla-loss-sto}
  \nabla_{\theta}\,\loss(\Omega, \theta) \approx
  \frac{1}{\lvert B\rvert} \sum_{(x, y) \in B} \nabla_{\theta}\,\ell (x, y, \theta).
\end{equation}
It is common to abuse the notation and denote by \(B\) the cardinal of the
subset, and call it \emph{batch} size.

Other optimization methods exist, mainly due to the presence of saddle points,
which would stop the previous iterative methods since the gradient would cancel
out. The best known are the following,
\begin{itemize}
  \item \textbf{Momentum}\index{Momentum}. It consists of performing the
  following computation,
  \begin{equation}
    \Delta\theta^{(t)} =
    -\beta\Delta\theta^{(t-1)} -\nabla\loss(\theta^{(t)}),
  \end{equation}
  where \(\beta\) is a hyperparameter. This algorithm maintains the previous
  velocity as an average of the above gradients and the parameter \(\beta\) can
  be understood as a parameter of ``friction'' (thinking in physical terms).
  \item \textbf{Nesterov Momentum}\index{Nesterov momentum}. Similarly to
  previous method uses a term of ``velocity'', but calculating the gradient not
  at the current point, but at the point where the velocity would carry, i.e.,
  \begin{equation}
    \Delta\theta^{(t)} =
    -\beta\Delta\theta^{(t-1)} -\nabla\loss(\theta^{(t)} +
    \beta\Delta\theta^{(t-1)}).
  \end{equation}
\end{itemize}

TODO. More optimization methods exist such as AdaGrad, RMSProp and Adam.

\subsubsection{Weight Initialization}\index{Weight initialization}

For every iterative method, an initial guess for is necessary
\(\theta^{(0)}\). In practice what you do is initialize the weights using a
probability distribution, such as \(U[-\sigma, \sigma]\) or
\(\mathcal{N}(0, \sigma ^ 2)\). The value of \(\sigma\) can be chosen
arbitrarily, although the one known as \emph{Xavier initialization}, by
\myCite{glorot10:under}, consisting of choosing the variance as the square root
of the input dimension \(D_{\text{in}}\).\footnote{For convolutional layers,
  \(D_{\text{in}} = \text{\code{filter\_size}}^2 \times
  \text{\code{input\_channels}}\).} This way we get activations are nicely
scaled in all layers.

However, in Xavier initialization it assumes that the activation function is
centered at 0, therefore, when the activation function \gls{relu} is used, it
is It is preferable to use another type of weight initialization, such as that
proposed by \myCite{he15:delvin_deep_rectif}.

\subsubsection{Backpropagation Error}\index{Backpropagation}

Hey. TODO.


\subsection{Regularization Techniques}\index{Regularization}

One of the biggest problems facing \gls{dl} and more generally predictive
statistics is that of \emph{overfitting}\index{Overfitting}. Overfitting is
creating a model of analysis from data that fits too closely the training
dataset available, but not capable of making reliable predictions of future
observations. In the words of \myCite{berners-lee21:introd_web_acces} (TODO:
fix me),
\begin{quoteBox}
  With four parameters I can fit an elephant, and with five I can make him
  wiggle his trunk.
  \tcblower\quotecite{berners-lee21:introd_web_acces} TODO. correct this.
\end{quoteBox}

What he is referring to here is that we should not be surprised at the ability
to a model complex enough to fit a given dataset very well. Models with a large
number of parameters are capable of adjusting to any given amount of data, even
if it appears not to follow any pattern. But these settings will be
meaningless, as they won't capture any genuine information on the structure of
the data and, therefore, will not be able to generalize correctly. We will have
an astonishingly good fit (even perfect in some cases) for currently available
data, but with a huge generalization error (see \vref{fig:overfit}).

\begin{figure}[ht]
  \centering
  \includegraphics[width=.65\textwidth]{Images/Overfit.png}
  \caption[Representation of the overfitting phenomenon.]{Representation of
    overfitting phenomenon. Noisy data (which is approximately linear) have
    been adjusted using a linear and a polynomial function. The polynomial
    function presents a perfect fit on existing data, but it is to be expected
    that it generalizes worse. If both functions were used to extrapolate out
    of the fitted data, the linear function will display better
    predictions. From \captioncite{contributors21:overf}.}\label{fig:overfit}
\end{figure}

A well-known saying in the field of \gls{ml} is that of ``Memorizing is
\emph{not} learning''. It is important that we take into account the effect of
overfitting to the time to train the models. To avoid this there are different
techniques, among which highlight the \(L_2\) regularization, the concept of
early stopping and the data augmentation technique.

\subsubsection{\(L_2\) Regularization}

This regularization technique, also known as \emph{ridge
  regression}\index{Ridge regression} in statistics and
\emph{Tikhonov}\index{Tikhonov regression} regularization in mathematics, it is
widely used in \gls{dl}. It consits in adding a term to the loss function that
takes into account the complexity of the model, i.e., minimize the following,
\begin{equation}
  \hat{\loss}(\Omega, \theta) =
  \loss(\Omega, \theta) + \lambda\,\text{complexity}(\theta),
\end{equation}
where \(\lambda\) is the regularization hyperparameter\footnote{Model
  \emph{hyperparameters} are properties that govern the entire training
  proccess and must be set before the training starts (therefore, they are not
  trainable).}\index{Hyperparameter} and the complexity is measured using the
\(L_2\) norm, i.e.,
\begin{equation}
  \text{complexity}(\theta) = \lVert\theta\rVert_2^2 = \sum_{w \in \theta} w^2.
\end{equation}

\subsubsection{Early Stopping}\index{Early stopping}

Probably the best known and most used method when it comes to training neural
models. What you do is divide the dataset of \emph{training} into two: the
training part and the validation part\index{Validation} (\code{train/val}
split).\footnote{Consequently, what to do with the full dataset is actually to
  divide it into three, the famous \code{train/val/test} split.}. For the
training process only the part of \code{train} (i.e., for updating the
parameters of the model). The part of \code{val} will be used to determine when
we should stop the training process to avoid overfitting\index{Overfitting}.

In this way, as we iterate through the training process, we can compute the
loss function both in the dataset of \code{train} and in that of \code{val}
(see \vref{fig:early}). With the goal in mind of finding models that generalize
well, we will stop training the neural network at the moment the error function
in the validation dataset starts to increase.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.45\textwidth]{Images/Early stopping.jpg}
  \caption[Early stopping regularization technique]{Early stopping
    regularization technique. At the beginning of the training process the
    error function decreases in the two datasets of training and validation at
    the same time. At the time when the error function in the validation set
    begins to increase is when the overfitting problem, so we will stop the
    training there. From
    \captioncite{brownlee28:gentl_introd_early_stopp_avoid}.}\label{fig:early}
\end{figure}

This regularization method has been used in this work (see \vref{cha:model})
for model training.

\subsubsection{Data Augmentation}\index{Data augmentation}

The use of data augmentation as a regularization technique is the process of
artificially generate new training samples from existing ones using random
transformations. In image processing, more typical transformations are: color
transformation, geometric translation, rotations, etc.

In this way we increase the cardinal of the dataset (which prevents the
overfitting) and we also artificially force one more dataset generalist. This,
consequently, will make the trained model better generalize and be invariant to
the transformations performed in the dataset.

\begin{remarkBox}
  It is important not to overdo this technique, as it could lead to scenarios
  that are too synthetic and that, therefore, we generate models that do not
  work well in the real world. A qualitative evaluation of the data created can
  be useful to validate this technique.
\end{remarkBox}



\section{Testing}\index{Testing}
Hey there. Talk about evaluation measures \textbf{in general}. Talk here about
the importance of the split \code{test/val/test}.
