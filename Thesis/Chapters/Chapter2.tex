% -*- TeX-master: "../Thesis.tex" -*-


\chapter{Theoretical Background} \label{cha:theory}

\epigraphhead [75]{
  \epigraph{\itshape
    Wwithout theory, there is nothing to revise. \\
    Without theory, experience has no meaning. \\
    Without theory, one has no questions to ask. \\
    Hence, without theory, there is no learning.}
  {---\scshape William Edwards}}

\lettrine{A}{neural} network, more properly referred to as an \gls{ann} are
computing systems vaguely inspired by the biological neural networks that
constitute animal brains. Dr
\fhref{https://en.wikipedia.org/wiki/Robert_Hecht-Nielsen}{Robert
  Hecht-Nielsen} (inventor of one of the first neurocomputers) defines a neural
network as:

\begin{quoteBox}
  \ldots a computing system made up of a number of simple, highly
  interconnected processing elements, which process information by their
  dynamic state response to external inputs.
  \tcblower --- \href{https://en.wikipedia.org/wiki/Robert_Hecht-Nielsen}
{Robert Hecht-Nielsen}
\end{quoteBox}

\subsubsection{Mathematical perspective}
Although the analogy made above of an \gls{ann} with a biological brain, there
is no need for this, we can just think of a neural network as a mathematical
optimization problem. We can think of the whole network to be a function that
takes some inputs to some outputs, and this function dependent on
parameters. The idea is to adjust this parameters to get a function that works
well with some known dataset, and we will trust that it will generalize
well. If the network is big enough and we carefully adjust the parameters, we
will be able to learn and calculate very complex functions.

\subsubsection{Types of neural networks}
During the last few years, there have been significant advances in the field of
artificial intelligence and multiple new models of neural networks have
appeared. In a very general classification of neural models we can find
different ones.

\begin{itemize}
  \item \textbf{Fully connected layered neural networks}. These networks are
  the simplest type of neural model that exists and on which more complex
  models are built. A more detailed description of this type of network can be
  found in \vref{sec:fully-conn-layer}.
  \item \textbf{\acl*{cnn}}. This type of neural network is especially useful
  in image processing, since it allows preserving the spatial information of
  the relationship between pixels. They will be studied in
  \vref{sec:conv-neur-netw}.
  \item \textbf{\acl*{rnn}}. Ideal neural models for the analysis of time
  series (such as text or audio). They will be studied in
  \vref{sec:recurr-neur-netw}.
  \item \textbf{Transformers}. The transformer-based models are part of the
  current \acl{sota}.\footenote{In \today, current date of the document.} They
  are being used extensively in \gls{nlp} and studying their possible
  applications in computer vision and image processing. They will be described
  in \vref{sec:transformers}.
\end{itemize}


\section{Fully Connected Layered Neural networks}
\label{sec:fully-conn-layer}

Fully connected layered neural networks are the simplest type of neural model
that exists and that constitute the basis for more complex neural structures.

\subsection{Topology} \label{sec:topology}

This type of neural network is characterized by a neural structure organized in
different layers, so that the neurons between adjacent layers are connected to
each other by arcs. An example of this type of architecture is shown in
\vref{fig:fully-conn-network}.

\begin{figure}[ht]
  \centering
  \input{Figures/Tikz/Fully Connected Neural Network.tex}
  \caption[Fully Connected Neural Network topology]{Example of Fully Connected
    Layered Neural Network. It consists of 4 differentiated layers of neurons
    and the connections between them are shown graphically with arrows.}
  \label{fig:fully-conn-network}
  \source{Created by the author}
\end{figure}

Next we will discuss the main constituent parts of this type of neural
networks: layers, neurons and connections.

\subsubsection{Layers}
The layers are just a collection of neurons, we will distinguish between three
types depending on its position in the network: \emph{input} layer (patterns
are presented to the network via this layer), \emph{hidden} layer (all the
inner layers) and \emph{output} layer (is the last layer, where the answer is
obtained).

We will denote with \(L\) the number of layers and with \(n_l\) the size of the
\(l\)-th layer.

\begin{exampleBox}
  To gain some intuition on how this work, let's think about the handwritten
  recognition problem. Suppose we have a set of images with handwritten digits
  in it and that we will like to implement an ANN that is capable of
  recognizing that digits. In this case, if the digits images are of
  \(28 \times 28\) pixels, the input layer will consist of 784
  (\(28 \times 28\)) neurons and each neuron will hold the grayscale value of a
  pixel. For the output layer we will need 10 neurons (one for each number
  between 0 and 9), and we will like that when we feed our network with an
  image holding a handwritten number 3, then the ouput is one 1 in the position
  corresponding to the number 3 and the rest of zeros.
\end{exampleBox}

\subsubsection{Neurons}
Neurons are the core component of any neural network. Basically there are three
subparts that form a neuron.

\begin{itemize}
  \item \textbf{Value}. Each neuron holds a value, it will be denoted by
  \(x_i^l \in \R\) for the \(i\)-th neuron in the \(l\)-th layer. Of course, it
  should be satisfied \(1 \leq i \leq n_l\). We will use the notation
  \(\mathbf{x}^l\) for the vector of all the values in the \(l\)-th level. When
  we speak of the input vector, we may ommit the superindex, i.e., we will use
  \(\mathbf{x}\) to denote \(\mathbf{x}^0\). Similarly, for the output layer,
  we will use \(\mathbf{\hat{y}}\) to refer to \(\mathbf{x}^L\).
  \item \textbf{Bias}. Also each neuron has a bias, denoted as \(b_i^l\) for
  the \(i\)-th neuron in the \(l\)-th layer. Is then true that
  \(1 \leq i \leq n_l\). The vector of all biases in the \(l\)-th layer will be
  denoted by \(\mathbf{b}^l\).
  \item \textbf{Activation function}. All neurons have an activation function
  \(f_i ^ l \in \mathcal{C} ^ 1 (\R, \R)\) for the \(i\)-th neuron in the
  \(l\)-th layer.\footnote{Usually all the activation functions are
    neuron-independent (ie, \(f_i^l\) does not really depend on \(i\) or
    \(l\)).} Of course, it is needed \(1 \leq i \leq n_l\). The regularity
  assummed for this functions is important, since we will be optimizing in the
  future by taking derivatives.
\end{itemize}

\subsubsection{Connections}
As we discussed in the topology section (\ref{sec:topology}), all neurons
between adjacent layers are required to be connected, this are the connections,
that should have associated a \emph{weight}. For the connection between the
\(i\)-th neuron in the \(l\)-th layer and the \(j\)-th neuron in the
\(l + 1\)-th layer, we will denote this weight by \(w_{ij}^l \in \R\). The set
of all this weights is as follows,
\begin{equation}
  \{w_{ij}^l \mid 1 \leq i \leq n_{l}, \, 1 \leq j \leq n_{l+1}, \, 1 \leq l <
  L \}
  \subset \R.
 \end{equation}

 The \emph{matrix} of all weights in the \(l\)-th layer will be denoted by
 \(\mathbf{W}^l\). This is, \((\mathbf{W}^l)_{ij} = w_{ij}^ l\).

\subsection{Model: feed forward} \label{subsec:forward}

From now on let's suppose we are working with a fully-connected neural network,
with different layers and we will be using the same notation used before. The
values of the neurons can be computed with
\begin{equation}
  x_i^l = f_i^l \left(
    \sum_{k=1}^{n_{l-1}} w_{ik}^{l-1} x_{k}^{l-1} + b_i^l
  \right).
\end{equation}

This formula is sometimes referred to as the feed-forward formula or forward
propagation. It's important to note that it's a recursive formula, once the
values the neurons in the the input layer are known, we can iterate computing
the values of the neurons in the next adjacent layer, until we reach the output
layer. In this network we can think of information traveling in one direction,
forward, from the input layer, through the hidden layers to the output neurons.

\subsection{Trainning: backpropagation} \label{sec:backward}

In order to be able to train our neural network, it is mandatory to define an
error function (also known as loss function) that quantifies how good or bad
the neural network is performing when feeded with a particular dataset. We will
you the below notation.
\begin{itemize}
  \item \textbf{Dataset}. Will be denoted by and consists of input-output pairs
  \((\mathbf{x}, \mathbf{y})\), where \(\mathbf{x}\) represents the input and
  \(\mathbf{y}\) the \emph{desired} output. We shall denote the size (cardinal)
  of the dataset by \(N\). Of course, in terms of our ANN \(x\) corresponds to
  the values of the first (or input) layer and \(\mathbf{y}\) to the output (or
  last) layer.
  \item \textbf{Parameters}. Parameters of our ANN are both the connection
  weights \(w_{ij}^l\) and the biases \(b_i^l\), we will denote the set of all
  parameters by \(\bm{\theta}\). Keep in mind that \(\bm{\theta}\) is just a
  set of real vectors of \(\R^D\) where \(D\) denotes the number of weights and
  biases.
\end{itemize}

The error function quantifies how different is the desired output
\(\mathbf{y}\) and the calculated (\emph{predicted}) output
\(\mathbf{\hat{y}}\) of the neural network on input \(\mathbf{x}\) for a set of
input-output pairs \((\mathbf{x} , \mathbf{y}) \in \Omega\) and a particular
value of the parameters \(\bm{\theta}\). We will denote the error function by
\(E (\Omega, \theta)\) and we will assume that it is continuosly
differentiable (i.e., \(\mathcal{C}^1\)).


\section{Convolutional Neural Networks}
\label{sec:conv-neur-netw}

\glspl{cnn} are a class of \glspl{dnn} that are widely used in computer
vision. They are commonly applied in image analysis, since they have the main
characteristic of having the ability to preserve the spatial relationship
between pixels. Likewise, they are regularized versions of fully connected
networks (explained in \vref{sec:fully-conn-layer}), that is, it is less likely
to overfit the data.

\fhref{http://yann.lecun.com/index.html}{Yann LeCun}, a renowned scientist
known as the father of \glspl{cnn}, defines them as follows:
\begin{quoteBox}
  Convolutional Neural Networks are a special kind of multi-layer neural
  networks. Like almost every other neural networks they are trained with a
  version of the back-propagation algorithm. Where they differ is in the
  architecture.
  \tcblower
  ---\href{http://yann.lecun.com/index.html}{Yann LeCun}
\end{quoteBox}

Therefore, we will study the architecture or topology of this type of networks
below in \vref{sec:topology-1}.

\subsection{Topology} \label{sec:topology-1}

Analogously to the case of fully connected networks, a \gls{cnn} consists of an
input layer, hidden layers and an output layer. In a \gls{cnn} the hidden
layers include layers that calculate convolutions. These convolutional layers
are followed by different ones, among which the pooling layers, fully connected
layers and normalization layers stand out. An example of this type of
architecture can be seen in \vref{fig:lenet-5}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{Images/Architecture LeNet-5.png}
  \caption[Example of topology of a \acs*{cnn} (LeNet-5)]{Example of topology
    of a \gls{cnn}. Specifically, it is
    \href{http://yann.lecun.com/exdb/lenet}{LeNet-5} (see
    \url{http://yann.lecun.com/exdb/lenet}), a convolutional network created by
    Yann LeCun.}
  \label{fig:lenet-5}
  \source{From TODO}
\end{figure}

\subsubsection{Convolutional layers}
The name comes from the fact that these layers use the mathematical operation
of \fhref{https://en.wikipedia.org/wiki/Convolution}{convolution}. This
operation is defined (in its discrete version) as follows: given a pair of
functions \(f, g\) defined on the set of integers \(\Z\), the discrete
convolution between \(f\) and \(g\) is given by,
\begin{equation} \label{eq:math-conv}
  (f \star g)[n] = \sum_{m=-\infty}^{\infty} f[m]g[n - m],
\end{equation}
where \(\star\) is the convolution operator.

This is the mathematical definition of the discrete convolution. However, the
use of convolution in image processing does not work exactly as defined in
\vref{eq:math-conv}. In practice, the input images\footnote{At deeper levels in
  a \gls{cnn} we will stop understanding the layers as images and call them
  \emph{feature maps}.} are three-dimensional tensors for the images
\acs{rgb}. In convolutional layers, a two-dimensional convolution is made with
a three-dimensional filter (also called kernel) on each channel of the input
image and then all these feature maps are stacked in the output tensor, where
the number of output channels coincides with the number of filters in that
layer.

We can mathematically express the operation performed on the convolutional
layers as follows. Let \(\mathbf{X}\) be the input characteristics map,
\(\mathbf{Y}\) the output characteristics map and the filter
\(\mathbf{F}\). The convolution is then defined as follows,
\begin{equation} \label{eq:convolution}
  \mathbf{Y}_{i, j, k} =
  \sum_{l, m, n} \mathbf{X}_{l, j + m, k + n}\mathbf{F}_{i, l, m, n},
\end{equation}
where the sum is performed for all valid \(l, m, n\) indices (this will depend
on \emph{padding}\footnote{There are different techniques to define the
  padding of an image. The most typical is known as zero-padding, which
  consists of adding 0 vectors around the image until it is complete in order
  to carry out the desired convolutions.} of the input image).

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Convolution.png}
  \caption[Example of a convolution]{Example of a convolution with a kernel
    (filter) of dimensions \((3, 3, 1)\) and an input feature map (tensor) of
    size \((8, 8, 1)\).}
  \label{fig:convolution}
  \source{From TODO}
\end{figure}

An example of convolution is shown graphically in \vref{fig:convolution}. As
can be seen for each of the elements of the output tensor, the computation of
\vref{eq:convolution} is performed with the filter shown.

\subsubsection{Pooling Layers}
Another type of layer used in this type of network is pooling. Pooling layers
reduce the dimension of the network by combining the output of neurons in one
layer into a single neuron in the next layer. These types of layers also add
non-linearities to the model. They also make \gls{cnn} less sensitive to small
local changes in spatial location.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Max pooling.png}
  \caption[Pooling layer example: max pooling]{Pooling layer example:
    \(2 \times 2\) max pooling. The input tensioner is divided into blocks
    \(2 \times 2\) and the Max Pool returns an output tensor with the maximum
    value of each block.}
  \label{fig:max-pool}
  \source{From \cite{wiki18:max_poolin}}
\end{figure}

There are different types of pooling. The best known are \emph{max} pooling and
\emph{average} pooling. The first uses the maximum value of each cluster of
neurons in the previous level. Average pooling, however, uses the average
value. In \vref{fig:max-pool} we can see a representation of a \(2 \times 2\)
max-pool. In this specific case, the mathematical operation to be performed is
given by the following expression,
\begin{equation}
  f_{X,Y}(S) = \max _{a,b=0}^{1}S_{2X+a,2Y+b},
\end{equation}
where \(S\) is every depth slice in the input. This discharges 75 \% of the
activations.

\subsection{Activation functions}
Analogously to fully connected neural networks, in this case it is also common
to use activation functions with the aim of introducing non-linearities in the
model without affecting the receptive fields of the convolutions. They are
typically added just after convolution.

The main functions used are the following:
\begin{itemize}
  \item \textbf{\gls*{relu}}. It is the function \(f (x) = \max \{0, x\}\),
  which is the most used. Removes negative values from the feature map and sets
  them to zero.
  \item \textbf{Tangente hiperbolica}. \(f (x) = \tanh x\)
  \item \textbf{Sigmoid function}. \(\sigma (x) ={(1 + e - x)} -1\).
  \item \(f (x) = | \tanh x |\)
\end{itemize}


\section{Recurrent Neural Networks} \label{sec:recurr-neur-netw}

\glspl{rnn} are a class of \glspl{ann} that are widely used with temporal
sequences (as for example in the scope of \gls{nlp}). Analogously to the
\glspl{cnn} that are suitable for image processing, the \glspl{rnn} are a type
of neural network specialized to process sequences of values of the type
\(\mathbf{x}^1, \ldots \mathbf{x}^{\tau}\) with \(\mathbf{x}^t \in \R^n\).

\subsection{Topology}

Looking at \vref{fig:rnn}, we can see the architecture of a \gls{rnn}. The
input values \(\mathbf{x}^1, \ldots \mathbf{x}^\tau\) are fed into the network
in an orderly manner. Therefore, to obtain the output vector \(\mathbf{y}^t\),
the hidden state \(\mathbf{h}^{t - 1}\) and the input \(\mathbf{y}^{t}\) are
necessary. Prior inputs \(\mathbf{x}^1, \ldots \mathbf{x}^{t - 1}\) are
represented by the hidden state \(\mathbf{h}^{t - 1}\), therefore the output
\(\mathbf{y}^t\), depends exactly of all the inputs until time \(t\).

\begin{figure}[ht]
  \begin{subfigure}[t]{.2\textwidth}
    \centering
    \caption{Basic \gls{rnn}.}
    \includegraphics[height=4cm]{Images/RNN.png}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.8\textwidth}
    \centering
    \caption{Unfolded basic \gls{rnn}.}
    \includegraphics[height=4cm]{Images/Unfolded RNN.png}
  \end{subfigure}
  \caption[Basic topology of \acl*{rnn}]{Basic topology of \acf{rnn}. Two
    versions of the same network are shown, one is the compact version and the
    other is the same network but unfolded in time.}
  \label{fig:rnn}
  \source{From TODO. Citar es de stanford también}
\end{figure}

\subsubsection{Different types of \gls*{rnn}}
Feedforward networks send an input to an output\footnote{The input and output
  can be vector. It does not mean, therefore, that they are only a number, it
  means that they are only \emph{un} multidimensional vector of \(\R^n\)}, but
in the case of \glspl{rnn} it is not. There are different possibilities, in
which the length of the inlet and outlet can vary. All the different types of
architectures that may exist are listed in \vref{fig:rnn-types}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{Images/RNN types.png}
  \caption[Types of architectures for \acl*{rnn}]{Types of architectures for
    \gls{rnn}. All posibilities are considered (i.e., all combinations are
    presented).}
  \label{fig:rnn-types}
  \source{From TODO. Es de Stanford curso}
\end{figure}

These different possibilities then have their application in various fields
such as: image captioning (``one to many''), action prediction (``many to
one``), video captioning (``many to many'' first option ), and video
classification on frame label (``many to many'' second option). The extreme
case of ``one to one'' corresponds precisely to a normal fully connected
layered neural network such as the one studied in \vref{sec:fully-conn-layer}.

\subsection{Model: feed forward}

We can compute the value of the neurons with a recurring formula, involving the
hidden states \(\mathbf{h}^t\) and the values of the input vectors
\(\mathbf{x}^t\). In the simplest case of a \emph{simple} \gls{rnn}, this
computation consists of,
\begin{equation} \label{eq:rnn-hidden}
  \mathbf{h}^t = f_{W} (\mathbf{h}^{t - 1}, \mathbf{x}^t),
\end{equation}
and then the output vector \(\mathbf{y}^t\) can be calculated using the
following expression,
\begin{equation} \label{eq:rnn-out}
  \mathbf{y}^t = W_{hy}\mathbf{h}^t,
\end{equation}
where \(W_{hy}\) is an array of parameters (trainable).

\subsubsection{Variant \gls*{rnn} architectures}
The model defined above is the simplest version for a \gls{rnn}. The
calculation of the output values with the \vref{eq:rnn-hidden,eq:rnn-out} can
cause problems with the gradients and, therefore, limit the ability of the
information to travel in time\footnote{These problems are know as vanishing /
  exploding gradient. The problem of \emph{exploding} gradients can be solved
  by gradient clipping (scaling it if the norm is too big). However for the
  \emph{vanishing} gradient problem, it is necessary to change the \gls{rnn}
  architecture.}. We introduce two new variants to solve these problems:
\begin{itemize}
  \item \textbf{\gls*{lstm}}. A unit of \gls{lstm} is composed of different
  gates that define its behavior. Its name is given by the function they
  perform: \emph{input} gate (\(i\)), \emph{output} gate (\(o\)) and
  \emph{forget} gate (\(f\)). A graphical representation of this type of
  elements is shown in \vref{fig:lstm}.
  \begin{figure}[ht]
    \centering
    \includesvg[width=.6\textwidth]{LSTM.svg}
    \caption[\acl*{lstm}]{\acf{lstm} representation. Input (\(i\)), output
      (\(o\)) and forget (\(f\)) gates are shown in the image.}
    \label{fig:lstm}
    \source{From \cite{enwiki:1005032489}}
  \end{figure}

  The value of the different gates is described by the following equation,
  \begin{equation}
    \begin{pmatrix}
      i \\
      f \\
      o \\
      g
    \end{pmatrix} =
    \begin{pmatrix}
      \sigma \\
      \sigma \\
      \sigma \\
      \tanh
    \end{pmatrix}
    W
    \begin{pmatrix}
      h^{t-1} \\
      x^t
    \end{pmatrix}.
  \end{equation}
  And then, the value of the cell and the hidden state can be computed with,
  \begin{equation}
    \begin{cases}
      c_t &= f \odot c_{t-1} + i \odot g, \\
      h_t &= o \odot \tanh c_{t}.
    \end{cases}
  \end{equation}
  The operator \(\odot\) is the element wise multiplication known as Hadamard
  product.
  \item \textbf{\gls*{gru}}. They are another type of gating mechanism
  introduced by \myCite{cho14:learn_rnn}. Its operation is governed by the
  following system of equations,
  \begin{equation}
    \begin{cases}
      z_{t}&=\sigma _{g}(W_{z}x_{t}+U_{z}h_{t-1}+b_{z}), \\
      r_{t}&=\sigma _{g}(W_{r}x_{t}+U_{r}h_{t-1}+b_{r}), \\
      {\hat {h}}_{t}&=\phi _{h}(W_{h}x_{t}+U_{h}(r_{t}\odot h_{t-1})+b_{h}), \\
      h_{t}&=(1-z_{t})\odot h_{t-1}+z_{t}\odot {\hat {h}}_{t}.
    \end{cases}
  \end{equation}
  Where \(\hat{h}_t\) is known as the candidate activation vector, \(z_t\) is
  the update gate vector and \(r_t\) is the reset gate vector. The activation
  functions \(\sigma_g\) correspond to the sigmoid function and \(\phi_{h}\) is
  the hyperbolic tangent. Other activation functions would also be possible.
\end{itemize}

\subsection{Trainning: \acl*{bptt}}

Analogously to other models, here the training of \gls{rnn} is also carried out
using first-order optimization methods, i.e., calculating the partial
derivatives of the error function with respect to the model parameters.

In this specific case of \gls{rnn}, the backpropagation process is known as
\gls{bptt}, which is a generalization of backpropagation in feed-forward
networks.


\section{Transformers} \label{sec:transformers}

The Transformer is a deep learning model recently introduced by
\myCite{vaswani17:atten_all_you_need}. Here they presented the idea that
recurrent building blocks are not needed in a model to work well in \gls{nlp}
tasks. In the words of its authors:

\begin{quoteBox}
  \ldots the Transformer, the first sequence transduction model based entirely
  on attention, replacing the recurrent layers most commonly used in
  encoder-decoder architectures with multi-headed self-attention.
  \tcblower
  ---Ashish Vaswani et al.
\end{quoteBox}

They propose a new architecture that is capable of maintaining an attention
mechanism while processing temporal sequences in parallel: the entire sequence
as a whole instead of going element by element.

\begin{figure}[p]
  \centering
  \includegraphics[height=.65\textheight]{Images/Transformer.png}
  \caption[Transformer model architecture]{Transformer model
    architecture/topology. Here you can observe the two main segments: the
    encoder and the decoder.}
  \label{fig:transformer}
  \source{From \cite{vaswani17:atten_all_you_need}}
\end{figure}

Currently there are different types of architectures based on this idea. The
original architecture of the Transformer model is shown in
\vref{fig:transformer}. As you can see, two distinct segments can be
distinguished:
\begin{itemize}
  \item \textbf{Encoder segment}. It takes the inputs, generates an embedding
  of them, encodes the positions, computes where each word has to attend to in
  a multi-context setting and then outputs a new intermediate representation.
  \item \textbf{Decoder segment}. Take the entries in the target language,
  generate an embedding for them with encoded positions, calculate in which
  each word has to attend, and then combine the output of the encoder with the
  output so far. The result is a prediction for the next token.
\end{itemize}
