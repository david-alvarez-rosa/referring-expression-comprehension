% -*- TeX-master: "../Thesis.tex" -*-


\chapter{Referring Expression Comprehension}%
\label{cha:rec}\index{Referring expresion!comprehension}

\epigraphhead[75]{
  \epigraph{\itshape We may hope that machines will eventually compete with men
in all purely intellectual fields.}
  {---\scshape Alan Turing}}

\lettrine{R}{eferring} expression comprehension is the task of, given a
linguistic phrase (\gls{re}) and an image, generate binary masks for the object
which the phrase refers to. In this chapter we will specifically formulate the
problem to be solved (\vref{sec:formulation}), we will analyze the existing
datasets and the evaluation measures (\vref{sec:data-eval}) and finally, we
will make an exhaustive study of \gls{sota} works in this area by reviewing the
more recent literature (\vref{sec:sota}).


\section{Problem Formulation}\label{sec:formulation}

In the task of \gls{rec} two different entries must be given, one of them
related to language and the other to vision.
\begin{itemize}
  \item \textbf{Vision}. It can be an image or a video. In our case we will
  only deal with images.\footnote{The same model would also apply in the case
    of video if we worked frame by frame, but we will not offer a model that
    takes into account the temporal evolution of the frames.}
  \item \textbf{\gls{re}}. It is a linguistic phrase that refers to an
  object. It can occur in two media: audio and text. In this thesis, both
  representations will be admitted.\index{Referring expression}
\end{itemize}
And, the output of this problem will be the generation of a binary segmentation
mask with the referred object or a bounding box (in this thesis functionality
will be added to generate both).

In order to understand it, multiple examples of this problem are shown in
\vref{fig:rec-examples}. They have tried to show all the possibilities, from
the simplest to the most complex.
\begin{itemize}
  \item \textbf{Multiple objects}. In \vref{fig:rec-man} an example is shown
  with two people who differ from each other by a differentiating element
  (\re{cap}). In \vref{fig:rec-laptop} the different objects are differentiated
  by their relative position (\re{right}). In \vref{fig:rec-army} they are
  differentiated by one quality (\re{white suit}).
  \item \textbf{Multiple categories}. As can be seen in
  \vref{fig:rec-umbrella,fig:rec-parent,fig:rec-girl}, it can refer to both
  objects and people within the same image.
  \item \textbf{Specialized vocabulary}. In \vref{fig:rec-elephant} we refer to
  a specific type of animal (\re{elephant}) and in \vref{fig:rec-couch} the
  expression \re{couch} is used.
  \item \textbf{Secondary objects}. In \vref{fig:rec-car} we refer to a small
  secondary object in the image (\re{car}), which is part of the same category
  (car) as the main object (bus).
\end{itemize}

\begin{figure}[tp]
  \centering
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Man with cap.}
    \label{fig:rec-man}
    \includegraphics[width=\textwidth]{Images/Man with cap.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Laptop on the right.}
    \label{fig:rec-laptop}
    \includegraphics[width=\textwidth]{Images/Laptop on the right.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Army officer white suit.}
    \label{fig:rec-army}
    \includegraphics[width=\textwidth]{Images/Army officer.jpg}
  \end{subfigure}

  \bigskip
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Black car.}
    \label{fig:rec-car}
    \includegraphics[width=\textwidth]{Images/Black car.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Small middle elephant.}
    \label{fig:rec-elephant}
    \includegraphics[width=\textwidth]{Images/Small middle elephant.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Two seat couch.}
    \label{fig:rec-couch}
    \includegraphics[width=\textwidth]{Images/Couch.jpg}
  \end{subfigure}

  \bigskip
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Umbrella.}
    \label{fig:rec-umbrella}
    \includegraphics[width=\textwidth]{Images/Umbrella.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Parent holding umbrella.}
    \label{fig:rec-parent}
    \includegraphics[width=\textwidth]{Images/Parent.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Little girl pink coat.}
    \label{fig:rec-girl}
    \includegraphics[width=\textwidth]{Images/Little girl.jpg}
  \end{subfigure}
  \caption[Examples of \acl*{rec}]{Examples of \acl{rec}. As can be seen, we
    can refer to objects of the image with \gls{re} in natural language and
    segmentation occurs.}
  \label{fig:rec-examples}
  \source{Created by the author}
\end{figure}


\section{Trainning}

TODO. Aquí hablar de que en trainning son necesarias dos cosas: dataset y una
loss function. Referenciar cada cosa.

\subsection{Datasets}\label{sec:datasets}

There are different datasets created exclusively for the training and
evaluation of neural models created to solve the problem discussed here. The
first three datasets considered (RefCOCO, RefCOCO+ and RefCOCOg) take their
images from the well-known dataset \gls{coco}, createt by
\myCite{lin14:micros}, while the last one (CLEVR-Ref+) uses synthetic images.

The dataset \gls{coco}, as its name suggests, contains images of everyday life
in everyday environments. Contains multi-object labeling, segmentation mask
annotations, image captioning, key-point detection and panoptic segmentation
annotations. They have a total of 81 categories, divided into 13
supercategories.
\begin{itemize}
  \item \textbf{Supercategories}. They are as follows: person, vehicle,
  vehicle, outdoor, animal, accessory, sports, kitchen, food, furniture,
  electronic, appliance, indoor.
  \item \textbf{Categories}. They are as follows: person, bicycle, car,
  motorcycle, airplane, bus, train, truck, boat, traffic light, fire hydrant,
  stop sign, parking meter, bench, bird, cat, dog, horse, sheep, cow, elephant,
  bear, zebra, giraffe, backpack, umbrella, handbag, tie, suitcase, frisbee,
  skis, snowboard, sports ball, kite, baseball bat, baseball glove, skateboard,
  surfboard, tennis racket, bottle, wine glass, cup, fork, knife , spoon, bowl,
  banana, apple, sandwich, orange, broccoli, carrot, hot dog, pizza, donut,
  cake, chair, couch, potted plant, bed, dining table, toilet, tv, laptop,
  mouse, remote, keyboard, cell phone, microwave, oven, toaster, sink,
  refrigerator, book, clock, vase, scissors, teddy bear, hair drier, toothbrus.
\end{itemize}
Contains a total of \num{2500000} images.

\subsubsection{RefCOCO, RefCOCO+ and RefCOCOg}
These datasets were created from \gls{coco} by \myCite{kazemzadeh14:refer_game}
using the game called ReferIt Game. In this two-player game, one of them wrote
a \gls{re} based on an object in an image and the second player, given the
image and \gls{re}, had to click on the correct location of the object that was
being described. If the second user's click coincided in the correct region,
each of the players received a point in the game and the roles were exchanged
for the next image. In this way, a process was created in which \gls{re} were
generated and validated for different objects in images in the same game.

The main difference between RefCOCO and RefCOCO+ is that in RefCOCO+ the
\emph{location} information was disallowded. In total, RefCOCO has \num{142209}
\gls{re} for a total of \num{50000} objects in \num{19994} images. RefCOCO+ has
a similar number of \gls{re} objects and images. An example of each of these
datasets is shown in \vref{fig:refcoco}.

\begin{figure}[htb]
  \centering
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \caption{RefCOCO dataset.}
    \includegraphics[width=.9\textwidth]{Images/RefCOCO.png}
  \end{subfigure}
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \caption{RefCOCO+ dataset.}
    \includegraphics[width=.9\textwidth]{Images/RefCOCO+.png}
  \end{subfigure}
  \caption[Examples from RefCOCO and RefCOCO+ datasets]{Examples from RefCOCO
    and RefCOCO+ datasets. It can be seen that in RefCOCO+ the use of location
    information is not allowed, while in RefCOCO it is valid.}
  \label{fig:refcoco}
  \source{Modified from \cite{kazemzadeh14:refer_game}}
\end{figure}

The difference between RefCOCOg and these two previous ones is that, it
contains only elements \emph{non-trivial}, i.e., there is at least one more
object of the same class as the target object in the image. Regarding size,
RefCOCOg contains \num{104560} expressions, \num{54822} objects and \num{26711}
images.

\subsubsection{CLEVR-Ref+}
The above datasets have been created expressly for \gls{rec} and are made up of
real-world images (which are highly complex). Furthermore, these datasets,
whose images come from \gls{coco}, could be skewed. Therefore,
\myCite{liu19:clevr}, created the CLEVR-Ref+ dataset with images and \gls{re}
generated synthetically. Different situations are considered where objects are
placed in the image with different variable options (such as colors, sizes, and
spatial relationships).

This dataset, however, will not be used in the work that concerns this thesis,
since here we are looking for models that work in the real world with
non-fictional images and natural language.

\subsection{Loss Functions}\index{Loss function}

Para entrenar los modelos es necesario tener unas funciones de pérdida, que
deberán ser diferenciables ya que en el proceso de optimización necesitaremos
usar las derivadas parciales de la función de pérdida respecto a los diferentes
parámetros a entrenar.

En nuestro caso, siempre trataremos con dos clases (la segmentación será una
máscara binaria).

\subsubsection{Cross Entropy}\index{Cross entropy}
Una de las funciones de pérdida más conocidas en la segmentación de imágenes es
la de \gls{ce}. Tenemos dos distribuciones de probabilidad

\begin{enumerate}
  \item La \textbf{predicción} puede ser \(P(\hat{Y} = 0) = \hat{p}\) or
  \(P(\hat{Y} = 1) = 1 - \hat{p}\).
  \item The \textbf{ground truth} can either be \(P(Y = 0) = p\) or
  \(P(Y = 1) = 1 - p\). Siempre se cumplirá \(p \in \{0, 1\}\).
\end{enumerate}

La función de pérdida se define entonces como
\begin{equation}
  \text{CE}(p, \hat{p}) = -(p\log\hat{p} + (1 - p)\log(1 - \hat{p})).
\end{equation}
Teniendo en cuenta que \(p \in \{0, 1\}\), la función de pérdida puede ser
reescrita de la siguiente manera,
\begin{equation}
  \text{\gls*{ce}}(p, \hat{p}) =
  \begin{cases}
    -\log(1 - \hat{p}) & p = 0 \\
    -\log\hat{p} & p = 1.
  \end{cases}
\end{equation}
Es decir, si \(p = 1\), la función de pérdida será \(0\) \gls{iff} \(\hat{p} =
1\) y será más grande cuanto más diferentes sean \(p\) y \(\hat{p}\). La
penalización crecerá exponencialmente hasta hacerse infinita para el valor
\(\hat{p} = 0\). El caso \(p = 0\) es simétrico.

A continuación se discutirán diversas variaciones de esta función de pérdida
que pueden ser útiles para entrenar diversos modelos neuronales.

\begin{itemize}
  \item \textbf{\gls*{wce}}. Es una variante de \gls{ce} en la que los ejemplos
  positivos son weighted por un coeficiente \(\beta\). Se define de la
  siguiente manera,
  \begin{equation}
    \text{\gls*{wce}}(p, \hat{p}) =
    -(\beta p\log\hat{p} + (1 - p)\log(1 - \hat{p})).
  \end{equation}
  Típicamente se usa cuando aparecen clases desequilibradas. No es demasiado
  interesante para este caso.
  \item \textbf{\gls*{bce}}. Es similar a \gls{wce} con la única diferencia que
  también se añade un peso a los ejemplos negativos. Se define de la siguiente
  manera,
  \begin{equation}
    \text{\gls*{bce}}(p, \hat{p}) =
    -(\beta p\log\hat{p} + (1 - \beta)(1 - p)\log(1 - \hat{p})).
  \end{equation}
  \item \textbf{\gls*{fl}}. Es una variante de \gls{ce} en la que se incide aún
  más en los elementos del dataset más \emph{complicados}. Estos son los que
  tienen un valor de \(\hat{p}\) intermedio entre \(0\) y \(1\). Se define de
  la siguiente manera,
  \begin{equation}
    \text{\gls*{fl}}(p, \hat{p}) =
    -(\alpha (1 - \hat{p})^\gamma p\log\hat{p} +
    (1 - \alpha)p^\gamma (1 - p)\log(1 - \hat{p})).
  \end{equation}
  Cuando \(\gamma = 0\) obtenemos \gls{bce}.
  \item \textbf{\gls*{dnc}}. TODO:
  \url{https://lars76.github.io/2018/09/27/loss-functions-for-segmentation.html} Esto
  faltaría por explicar.
\end{itemize}

\subsubsection{Overlap measures}
Otro tipo de medidades surgen con el uso de la intersección y la unión de la
segmentación predicha y el ground truth. Este tipo de funciones de pérdida nos
aportan información \emph{global}. El conocido Jaccard index o coeficiente \gls{iou},
\begin{equation}
  J(A,B) = \frac{|A \cap B|}{|A \cup B|}
  = \frac{|A \cap B|}{|A| + |B| - |A \cap B|},
\end{equation}
es típicamente usado para medir el accuracy de un modelo, pero no puede ser
usado como función de pérdida al no ser una aplicación diferenciable. Sí que
será usado para la evaluación del modelo en la \vref{sec:evaluation}

\begin{itemize}
  \item \textbf{\gls*{dil}}. Se basa en el \gls{dc}, un coeficiente similar a
  \gls{iou}, que se define de la siguiente manera,
  \begin{equation}
    \text{\gls*{dc}}(X, Y) = \frac{2|X \cap Y|}{|X| + |Y|}.
  \end{equation}
  Este dice coefficiente puede ser definido como una función de pérdida,
  \begin{equation}
    \text{\gls*{dil}}(p, \hat{p})
    = 1 - \frac{2\sum p_{h, w}\hat{p}_{h, w}}{\sum p_{h, w} + \sum \hat{p}_{h, w}},
  \end{equation}
  donde \(p_{h, w} \in \{0, 1\}\), \(0 \leq \hat{p}_{h, w} \leq 1\) y los
  sumatorios se extienden por toda la imagen en el width \(w\) y el height
  \(h\).
  \item \textbf{\gls*{ti}}. Es una generalización de \gls{dil}. Se define de la
  siguiente manera,
  \begin{equation}
    \text{\gls*{ti}}(p, \hat{p})
    = 1 - \frac{p\hat{p}}{p\hat{p} + \beta(1 - p)\hat{p} + (1 - \beta)p(1 - \hat{p})}.
  \end{equation}
  Con el valor \(\beta = \frac{1}{2}\), recuperamos la función anterior
  \gls{dil}.
\end{itemize}


\subsubsection{\gls*{iou} loss}

\cite{yu16:unitb}

\subsubsection{Combinations}
Muchas más funciones de pérdida pueden ser obtenidas por simple combinación
lineal de las anteriores. La combinación,
\begin{equation}
  \text{\gls*{ce}}(p, \hat{p}) + \text{\gls*{dil}}(p, \hat{p}),
\end{equation}
es bastante popular, ya que combina información local (\gls{ce}) con
información global (\gls{dil}).


\section{Evaluation Techniques}\label{sec:eval-measure}

TODO. Explain here that both numerical (quantitative) and visual (qualitative)
techniques can be used for evaluation.

\subsection{Quantitative Measures}

This corresponds to an evaluation of the model in a numerical way. The
different evaluation measures typically used to address this problem are
related to the calculation of \gls{iou} (in \vref{sec:evaluation} it will be
explained in depth).

It is also possible to define an accuracy index in the following way: if
\gls{iou} is greater than \num{0.5} the prediction is considered a true
positive, and if not, it is considered as a false positive. You can then
average over the entire dataset.

Para la evaluación del modelo se usará el concepto de intersección y de unión
entre la segmentación predecida (que es una máscara \emph{binaria}) y el ground
truth (en la \vref{fig:sets} se muestran unos diagramas con estos
conceptos).

\begin{figure}[htb]
  \begin{subfigure}[t]{.5\textwidth}
    \centering
    \caption{Union of sets \(A\) and \(B\).}
    \includesvg[width=.55\textwidth]{Union_of_sets_A_and_B.svg}
  \end{subfigure}
  \begin{subfigure}[t]{.5\textwidth}
    \centering
    \caption{Intersection of sets \(A\) and \(B\).}
    \includesvg[width=.55\textwidth]{Intersection_of_sets_A_and_B.svg}
  \end{subfigure}
  \caption[Union and intersection of sets \(A\) and \(B\)]{Graphic
    representation of union and intersection of sets \(A\) and \(B\).}
  \label{fig:sets}
\end{figure}

De aquí surge el conocido Jaccard index o coeficiente \gls{iou},
\begin{equation}
  J(A,B) = \frac{|A \cap B|}{|A \cup B|}
  = \frac{|A \cap B|}{|A| + |B| - |A \cap B|},
\end{equation}
que es típicamente usado para medir el accuracy de un modelo, pero ---como
hemos comentado anteriormente--- no puede ser usado como función de pérdida al
no ser una aplicación diferenciable.

\begin{figure}[htb]
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \caption{Bounding boxes example.}
    \includegraphics[width=.8\textwidth]{Images/Object detection Bounding Boxes.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \caption{\gls{iou} visual equation.}
    \includegraphics[width=.8\textwidth]{Images/Intersection over Union.png}
  \end{subfigure}
  \caption[Explicación del Jaccard Index]{Explicación y ejemplo del Jaccard
    Index en el caso de bounding boxes.}
\end{figure}

Este índice aporta información relevante sobre cómo de ajustada está una
bounding box.\footnote{Se estudia el caso de bounding box por simplicidad, pero
  el mismo concepto aplica en el caso de segmentación pixel a pixel.} Es
evidente que el Jaccard index toma un valor entre \(0\) y \(1\), siendo \(0\)
cuando no hay intersección entre las bounding boxes y tomando el valor de \(1\)
cuando la correspondencia es exacta.


\subsection{Qualitative Evaluation}

Visually evaluate the model.


\section{Related Work} \label{sec:sota}

The current methods \gls{sota} for \gls{rec} can be divided into three main
classes: joint embedding (see \vref{sec:joint}), modular models (see
\vref{sec:modular}) and graph convolution based models (see \vref{sec:graph}).

\subsection{Multimodal Embedding}%
\label{sec:joint}\index{Multimodal!embedding}
Multimodal embedding methods are very typical in any of the multimodal learning
tasks. What is sought in them is to find a multidimensional space where
encodings of image and language can ``coexist'' in common. This idea is
represented graphically in \vref{fig:joint}. This multidmensional space will
typically be \(\R^n\), which is a normed space. One of the desirable
characteristics would be that the encodings of images and language similar to
each other were ``close'' in this space (in terms of norm).

\begin{figure}[htb]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Joint.png}
  \caption[Multimodal embedding technique]{Multimodal embedding into
    visual-semantic space. As you can see, matching pairs are closer (in terms
    of norm) that non-matching pairs in the joint space.}\label{fig:joint}
  \source{From~\cite{cornia18:towar_cycle_consis_model_text_image_retriev}}
\end{figure}

Therefore, here, to perform \gls{rec}, the first thing we will do is encode the
image and \gls{re} separately in the same vector space. For this, \gls{cnn} are
very useful to generate image representations (extracting the most relevant
features) and for the coding of phrases \gls{rnn} (with, for example,
\gls{lstm}) and transformers are used.

The first deep learning model for referring expression generation and
comprehension is from \myCite{mao16:gener}, where they use a model \gls{cnn}
with which they extract the visual features and a network of type \gls{lstm}
for \emph{generating} the referring expression. It also gives a solution for the
inverse problem of \emph{comprehension}.

Within this type of model fits the one proposed by \myCite{bellver20:refvos}
where a neural network of type \gls{cnn} is also used for the encoding of the
image, but the transformer is used as a language encoder. Then to achieve
multimodal embedding, the encoded linguistic phrase is converted into a
256-dimensional vector and multiplied element-wise with the visual
features. This model will be studied in depth in \vref{cha:model}.

\subsection{Modular Models}\label{sec:modular}\index{Modular model}
Modular models have been used successfully in many tasks both in the scope of
\gls{cv}, and in \gls{nlp}. The technique used in these cases is to decompose
\gls{re} into different components, in which it is sought to attack different
reasoning.

\begin{figure}[htb]
  \centering
  \includegraphics[width=.75\textwidth]{Images/MattNet.png}
  \caption[\acl*{mattnet}]{\gls{mattnet}: given an expression, it is divided
    into three phrase embeddings, which are input to three visual modules that
    process the described visual region in different ways and compute
    individual matching scores.}
  \label{fig:mattnet}
  \source{From~\cite{yu18:mattn}}
\end{figure}

An example of these modular models is the one presented by \myCite{yu18:mattn},
which is graphically represented in \vref{fig:mattnet}. In this case there are
three differentiated modules: the \emph{subject} module, the \emph{location}
module and the \emph{relationship} module. Each of them computes different
scores, which are then used to calculate an overall score.

Starting from the base of \gls{mattnet},
\myCite{liu19:improv_refer_expres_groun_cross_atten_erasin} propose
\gls{cmatterase}, which is a training strategy for this type of task. It is
based on the idea of erasing the part most used by the model from the
linguistic or visual part, so that the model is forced to learn more complex
structures.\footnote{It is partly similar to the strategy of \emph{dropout}
  used in the training of fully connected neural networks, which is used to
  avoid dependency on specific nuerones and thus prevent overfitting of the
  model.} Likewise, it modifies the initial model (\gls{mattnet}), considering
the global image as one more characteristic.

\subsection{Graph Generation}\label{sec:graph}
In the task that concerns us, the understanding by the model of \gls{re} is
essential. These types of expressions contain different objects and
relationships between them. In other words, it is common to refer to an object
not only because of its intrinsic properties, but also because of its
relationship with the objects that surround it. The mathematical tool that best
represents this phenomenon is that of a graph: the nodes represent different
objects and the different edges are the existing relationship between objects
(see \vref{fig:graph}).

\begin{figure}[htb]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Graph model.png}
  \caption[Graph based model representation]{Summary representation of
    graph-based models. From the image, the graph representation is built,
    which is then updated with the expression embedding and computed a matching
    score between objects and expression.}
  \label{fig:graph}
  \source{From~\cite{qiao20:refer_expres_compr}}
\end{figure}

The use of graphs in the task of \gls{rec} has been used with success by
various authors. Among them \myCite{wang19:neigh}, he proposes
\gls{lgran}. This model consists of three differentiated modules: language-self
attention module, language-guided graph attention module, and matching
module. The first of these modules is responsible for decomposing the \gls{re}
into three different parts (subject description, intra-class relationships, and
inter-class relationships). The language-guided graph attention module is
responsible for generating the graph representation of the image (the nodes it
generates will be the candidate objects). Finally, the matching module is the
one that computes the matching score between \gls{re} and object (for each of
the candidate objects).

Other authors in exploiting the graphs in this context are
\myCite{yang19:dynam}, who created the model \gls{dga}, which allows multi-step
reasoning. Initially, the model works the same as others with the generation of
a graph from the image and with the mixing of an embedding of the expression in
the graph. But from here on they use a module they call ``analyzer'' and that
is capable of exploring the linguistic structure of \gls{re} and dividing it
into a sequence of constituent expressions. In this way \gls{dga} is able to
carry out a step-by-step reasoning process on these constituent
expressions. Finally, as is common in these models (see \vref{fig:graph}), a
matching score between objects and expression is computed.

\myCite{yang19:cross_modal_relat_infer_groun_refer_expres} also create a
graph-based model that they call \gls{cmrin}. This network consists of a
\gls{cmre}, which is in charge of obtaining the information for the
construction of the graph with ``cross-modal attention'', and a \gls{ggcn} that
uses the information from the previous graph and propagates the information
(which is multi-modal) to be able to compute the matching score.
