% -*- TeX-master: "../Thesis.tex" -*-


\chapter{Referring Expression Comprehension}

\epigraphhead[75]{
  \epigraph{\itshape Begin at the beginning, the King said
    gravely, ``and go on till you come to the end: then stop.''}
  {---\textsc{Lewis Carroll}\\ \textit{Alice in Wonderland}}
}


\lettrine{L}{anguage} ipsum dolor sit amet, consectetur
adipiscing elit, sed do eiusmod and incididunt ut labore et dolore magna
aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi
ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in
voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint
occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim
id est laborum.


\section{Problem formulation}

Hey


\section{Datasets and Evaluation}

Hey


\section{Related work}

Los métodos actuales \gls{sota} para \gls{rec} se pueden dividir en tres
grandes clases: joint embedding (see \vref{sec:joint}), modelos modulares (see
\vref{sec:modular}) and graph convolution based models (see
\vref{sec:graph}).

\subsection{Multimodal embedding} \label{sec:joint}
Los métodos de multimodal embedding son muy típicos en cualquiera de las tareas del
aprendizaje multimodal. En ellos lo que se busca es encontrar un espacio
multidimensional donde puedan ``convivir''\footnote{Este espacio
multidmensional será típicamente \(\R^n\), que es un espacio normado. Una de
las caracterísitcas deseables sería que las codificaciones de imágnes y
lenguaje similares entre ellas quedasen ``cerca'' en este espacio (en términos
de norma).} codificaciones de imagen y lenguaje en común. Esta idea viene
representada de manera gráfica en la \vref{fig:joint}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Joint.png}
  \caption[Multimodal embedding technique]{Multimodal embedding into
    visual-semantic space. Como se puede ver, las matching pairs quedan más
    cerca (en términos de norma) que las non-matching pairs en el joint space.}
  \label{fig:joint}
\end{figure}

Por tanto, aquí, para realizar \gls{rec}, lo primero que haremos es codificar
la imagen y la \gls{re} por separado en un mismo espacio vectorial. Para ello,
\gls{cnn} son muy útiles para generar representaciones de la imagen (extrayendo
las features más relevantes) y para la codificación de frases se usan \gls{rnn}
(con, por ejemplo, \gls{lstm}) y transformers.

El primer modelo de deep learning para referring expression generation and
comprehension, es de \myCite{mao16:gener}, donde usan un modelo \gls{cnn} con
el que extraen las features visuales y una red de tipo \gls{lstm} para
\emph{generar} la referring expression. También da una solución para el
problema inverso de \emph{comprehension}.

Dentro de este tipo de modelos encaja el propuesto por
\myCite{bellver20:refvos} donde también se usa una red neuronal de tipo
\gls{cnn} para la codificación de la imagen, pero como codificador de lenguaje
se usa el transformer. Después para conseguir el multimodal embedding, se
convierte la frase linguistica codificada en un vector 256-dimensional y se
multiplica element-wise con las features visuales. Este modelo será estudiado
en profundidad en \vref{cha:model}.

\todo{Aquí se puede hablar de otros attention mechanism.}

\subsection{Modelos modulares} \label{sec:modular}
Los modelos modulares han sido usado con éxito en muchas tareas tanto en el
ámbito de \gls{cv}, como en \gls{nlp}. La técnica usada en estos casos, es la
de descomponer la \gls{re} en diferentes componentes, en los que se busca
atacar diferentes razonamientos.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/MattNet.png}
  \caption[\acl*{mattnet}]{\gls{mattnet}: given an expression, it is divided
    into three phrase embeddings, which are input to three visual modules that
    process the described visual region in different ways and compute
    individual matching scores.}
  \label{fig:mattnet}
  \source{From \cite{yu18:mattn}}
\end{figure}

Un ejemplo de estos modelos modulares es el presentado por \myCite{yu18:mattn},
que se encuentra representado gráficamente en la \vref{fig:mattnet}. En este
caso hay tres módulos diferenciados: el \emph{subject} module, el
\emph{location} module y el módulo de \emph{relationship}. Cada uno de ellos
computa diferentes scores, que después son usados para el cálculo de un overall
score.

\subsection{Graph convolution} \label{sec:graph}
