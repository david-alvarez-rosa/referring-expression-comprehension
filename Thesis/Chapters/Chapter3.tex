% -*- TeX-master: "../Thesis.tex" -*-


\chapter{Referring Expression Comprehension} \label{cha:rec}

\epigraphhead[75]{
  \epigraph{\itshape We may hope that machines will eventually compete with men
in all purely intellectual fields.}
  {---\scshape Alan Turing}}

\lettrine{R}{eferring} expression comprehension is the task of, given a
linguistic phrase (\gls{re}) and an image, generate binary masks for the object
which the phrase refers to. In this chapter we will specifically formulate the
problem to be solved (\vref{sec:formulation}), we will analyze the existing
datasets and the evaluation measures (\vref{sec:data-eval}) and finally, we
will make an exhaustive study of \gls{sota} works in this area by reviewing the
more recent literature (\vref{sec:sota}).


\section{Problem formulation} \label{sec:formulation}

In the task of \gls{rec} two different entries must be given, one of them
related to language and the other to vision.
\begin{itemize}
  \item \textbf{Vision}. It can be an image or a video. In our case we will
  only deal with images.\footnote{The same model would also apply in the case
    of video if we worked frame by frame, but we will not offer a model that
    takes into account the temporal evolution of the frames.}
  \item \textbf{\gls{re}}. It is a linguistic phrase that refers to an
  object. It can occur in two media: audio and text. In this thesis, both
  representations will be admitted.
\end{itemize}
And, the output of this problem will be the generation of a binary segmentation
mask with the referred object or a bounding box (in this thesis functionality
will be added to generate both).

In order to understand it, multiple examples of this problem are shown in
\vref{fig:rec-examples}. They have tried to show all the possibilities, from
the simplest to the most complex.
\begin{itemize}
  \item \textbf{Multiple objects}. In \vref{fig:rec-man} an example is shown
  with two people who differ from each other by a differentiating element
  (``cap''). In \vref{fig:rec-laptop} the different objects are differentiated
  by their relative position (``right''). In \vref{fig:rec-army} they are
  differentiated by one quality (``white suit'').
  \item \textbf{Multiple categories}. As can be seen in
  \vref{fig:rec-umbrella,fig:rec-parent,fig:rec-girl}, it can refer to both
  objects and people within the same image.
  \item \textbf{Specialized vocabulary}. In \vref{fig:rec-elephant} we refer to
  a specific type of animal (``elephant'') and in \vref{fig:rec-couch} the
  expression ``couch'' is used.
  \item \textbf{Secondary objects}. In \vref{fig:rec-car} we refer to a small
  secondary object in the image (``car''), which is part of the same category
  (car) as the main object (bus).
\end{itemize}

\begin{figure}[tp]
  \centering
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Man with cap.}
    \label{fig:rec-man}
    \includegraphics[width=\textwidth]{Images/Man with cap.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Laptop on the right.}
    \label{fig:rec-laptop}
    \includegraphics[width=\textwidth]{Images/Laptop on the right.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Army officer white suit.}
    \label{fig:rec-army}
    \includegraphics[width=\textwidth]{Images/Army officer.jpg}
  \end{subfigure}

  \bigskip
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Black car.}
    \label{fig:rec-car}
    \includegraphics[width=\textwidth]{Images/Black car.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Small middle elephant.}
    \label{fig:rec-elephant}
    \includegraphics[width=\textwidth]{Images/Small middle elephant.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Two seat couch.}
    \label{fig:rec-couch}
    \includegraphics[width=\textwidth]{Images/Couch.jpg}
  \end{subfigure}

  \bigskip
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Umbrella.}
    \label{fig:rec-umbrella}
    \includegraphics[width=\textwidth]{Images/Umbrella.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Parent holding umbrella.}
    \label{fig:rec-parent}
    \includegraphics[width=\textwidth]{Images/Parent.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Little girl pink coat.}
    \label{fig:rec-girl}
    \includegraphics[width=\textwidth]{Images/Little girl.jpg}
  \end{subfigure}
  \caption[Examples of \acl*{rec}]{Examples of \acl{rec}. As can be seen, we
    can refer to objects of the image with \gls{re} in natural language and
    segmentation occurs.}
  \label{fig:rec-examples}
  \source{Created by the author}
\end{figure}


\section{Datasets and Evaluation} \label{sec:data-eval}

For the development and validation of the models presented to solve the problem
in \gls{rec}, two things are necessary mainly: datasets and evaluation
measures. Every neural model will need a database with which to train (adjust)
the parameters it has. Likewise, it will be necessary to reserve part of this
dataset to be able to carry out an evaluation of it using different
measurements, this is what will make it possible to compare different models.

This section will present the most important \emph{datasets} currently existing
in \vref{sec:datasets} and the \emph{evaluation measures} most used in
\vref{sec:eval-measure}.

\subsection{Datasets} \label{sec:datasets}

There are different datasets created exclusively for the training and
evaluation of neural models created to solve the problem discussed here. The
first three datasets considered (RefCOCO, RefCOCO+ and RefCOCOg) take their
images from the well-known dataset \gls{coco} \cite{lin14:micros}
\footfullcite{lin14:micros}, while the last one (CLEVR-Ref+) uses synthetic
images.

The dataset \gls{coco}, as its name suggests, contains images of everyday life
in everyday environments. Contains multi-object labeling, segmentation mask
annotations, image captioning, key-point detection and panoptic segmentation
annotations. They have a total of 81 categories, divided into 13
supercategories.
\begin{itemize}
  \item \textbf{Supercategories}. They are as follows: person, vehicle,
  vehicle, outdoor, animal, accessory, sports, kitchen, food, furniture,
  electronic, appliance, indoor.
  \item \textbf{Categories}. They are as follows: person, bicycle, car,
  motorcycle, airplane, bus, train, truck, boat, traffic light, fire hydrant,
  stop sign, parking meter, bench, bird, cat, dog, horse, sheep, cow, elephant,
  bear, zebra, giraffe, backpack, umbrella, handbag, tie, suitcase, frisbee,
  skis, snowboard, sports ball, kite, baseball bat, baseball glove, skateboard,
  surfboard, tennis racket, bottle, wine glass, cup, fork, knife , spoon, bowl,
  banana, apple, sandwich, orange, broccoli, carrot, hot dog, pizza, donut,
  cake, chair, couch, potted plant, bed, dining table, toilet, tv, laptop,
  mouse, remote, keyboard, cell phone, microwave, oven, toaster, sink,
  refrigerator, book, clock, vase, scissors, teddy bear, hair drier, toothbrus.
\end{itemize}
Contains a total of \num{2500000} images.

\subsubsection{RefCOCO, RefCOCO+ and RefCOCOg}
These datasets were created from \gls{coco} by \myCite{kazemzadeh14:refer_game}
using the game called ReferIt Game. In this two-player game, one of them wrote
a \gls{re} based on an object in an image and the second player, given the
image and \gls{re}, had to click on the correct location of the object that was
being described. If the second user's click coincided in the correct region,
each of the players received a point in the game and the roles were exchanged
for the next image. In this way, a process was created in which \gls{re} were
generated and validated for different objects in images in the same game.

The main difference between RefCOCO and RefCOCO+ is that in RefCOCO+ the
\emph{location} information was disallowded. In total, RefCOCO has \num{142209}
\gls{re} for a total of \num{50000} objects in \num{19994} images. RefCOCO+ has
a similar number of \gls{re} objects and images. An example of each of these
datasets is shown in \vref{fig:refcoco}.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \caption{RefCOCO dataset.}
    \includegraphics[width=.9\textwidth]{Images/RefCOCO.png}
  \end{subfigure}
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \caption{RefCOCO+ dataset.}
    \includegraphics[width=.9\textwidth]{Images/RefCOCO+.png}
  \end{subfigure}
  \caption[Examples from RefCOCO and RefCOCO+ datasets]{Examples from RefCOCO
    and RefCOCO+ datasets. It can be seen that in RefCOCO+ the use of location
    information is not allowed, while in RefCOCO it is valid.}
  \label{fig:refcoco}
  \source{Modified from \cite{kazemzadeh14:refer_game}}
\end{figure}

The difference between RefCOCOg and these two previous ones is that, it
contains only elements \emph{non-trivial}, i.e., there is at least one more
object of the same class as the target object in the image. Regarding size,
RefCOCOg contains \num{104560} expressions, \num{54822} objects and \num{26711}
images.

\subsubsection{CLEVR-Ref+}
The above datasets have been created expressly for \gls{rec} and are made up of
real-world images (which are highly complex). Furthermore, these datasets,
whose images come from \gls{coco}, could be skewed. Therefore,
\myCite{liu19:clevr}, created the CLEVR-Ref+ dataset with images and \gls{re}
generated synthetically. Different situations are considered where objects are
placed in the image with different variable options (such as colors, sizes, and
spatial relationships).

This dataset, however, will not be used in the work that concerns this thesis,
since here we are looking for models that work in the real world with
non-fictional images and natural language.

\subsection{Evaluation measures} \label{sec:eval-measure}

This corresponds to an evaluation of the model in a numerical way. The
different evaluation measures typically used to address this problem are
related to the calculation of \gls{iou} (in \vref{sec:evaluation} it will be
explained in depth).

It is also possible to define an accuracy index in the following way: if
\gls{iou} is greater than \num{0.5} the prediction is considered a true
positive, and if not, it is considered as a false positive. You can then
average over the entire dataset.


\section{Related work} \label{sec:sota}

The current methods \gls{sota} for \gls{rec} can be divided into three main
classes: joint embedding (see \vref{sec:joint}), modular models (see
\vref{sec:modular}) and graph convolution based models (see \vref{sec:graph}).

\subsection{Multimodal embedding} \label{sec:joint}
Multimodal embedding methods are very typical in any of the multimodal learning
tasks. What is sought in them is to find a multidimensional space where
encodings of image and language can ``coexist'' in common. This idea is
represented graphically in \vref{fig:joint}. This multidmensional space will
typically be \(\R^n\), which is a normed space. One of the desirable
characteristics would be that the encodings of images and language similar to
each other were ``close'' in this space (in terms of norm).

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Joint.png}
  \caption[Multimodal embedding technique]{Multimodal embedding into
    visual-semantic space. As you can see, matching pAIRS are closer (in terms
    of norm) that non-matching pairs in the joint space.}
  \label{fig:joint}
  \source{From \cite{cornia18:towar_cycle_consis_model_text_image_retriev}}
\end{figure}

Therefore, here, to perform \gls{rec}, the first thing we will do is encode the
image and \gls{re} separately in the same vector space. For this, \gls{cnn} are
very useful to generate image representations (extracting the most relevant
features) and for the coding of phrases \gls{rnn} (with, for example,
\gls{lstm}) and transformers are used.

The first deep learning model for referring expression generation and
comprehension is from \myCite{mao16:gener}, where they use a model \gls{cnn}
with which they extract the visual features and a network of type \gls{lstm}
for \emph{generating} the referring expression. It also gives a solution for the
inverse problem of \emph{comprehension}.

Within this type of model fits the one proposed by \myCite{bellver20:refvos}
where a neural network of type \gls{cnn} is also used for the encoding of the
image, but the transformer is used as a language encoder. Then to achieve
multimodal embedding, the encoded linguistic phrase is converted into a
256-dimensional vector and multiplied element-wise with the visual
features. This model will be studied in depth in \vref{cha:model}.

\subsection{Modular models} \label{sec:modular}
Modular models have been used successfully in many tasks both in the scope of
\gls{cv}, and in \gls{nlp}. The technique used in these cases is to decompose
\gls{re} into different components, in which it is sought to attack different
reasoning.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/MattNet.png}
  \caption[\acl*{mattnet}]{\gls{mattnet}: given an expression, it is divided
    into three phrase embeddings, which are input to three visual modules that
    process the described visual region in different ways and compute
    individual matching scores.}
  \label{fig:mattnet}
  \source{From \cite{yu18:mattn}}
\end{figure}

An example of these modular models is the one presented by \myCite{yu18:mattn},
which is graphically represented in \vref{fig:mattnet}. In this case there are
three differentiated modules: the \emph{subject} module, the \emph{location}
module and the \emph{relationship} module. Each of them computes different
scores, which are then used to calculate an overall score.

Starting from the base of \gls{mattnet},
\myCite{liu19:improv_refer_expres_groun_cross_atten_erasin} propose
\gls{cmatterase}, which is a training strategy for this type of task. It is
based on the idea of erasing the part most used by the model from the
linguistic or visual part, so that the model is forced to learn more complex
structures.\footnote{It is partly similar to the strategy of \emph{dropout}
  used in the training of fully connected neural networks, which is used to
  avoid dependency on specific nuerones and thus prevent overfitting of the
  model.} Likewise, it modifies the initial model (\gls{mattnet}), considering
the global image as one more characteristic.

\subsection{Graph generation} \label{sec:graph}
In the task that concerns us, the understanding by the model of \gls{re} is
essential. These types of expressions contain different objects and
relationships between them. In other words, it is common to refer to an object
not only because of its intrinsic properties, but also because of its
relationship with the objects that surround it. The mathematical tool that best
represents this phenomenon is that of a graph: the nodes represent different
objects and the different edges are the existing relationship between objects
(see \vref{fig:graph}).

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Graph model.png}
  \caption[Graph based model representation]{Summary representation of
    graph-based models. From the image, the graph representation is built,
    which is then updated with the expression embedding and computed a matching
    score between objects and expression.}
  \label{fig:graph}
  \source{From \cite{qiao20:refer_expres_compr}}
\end{figure}

The use of graphs in the task of \gls{rec} has been used with success by
various authors. Among them \myCite{wang19:neigh}, he proposes
\gls{lgran}. This model consists of three differentiated modules: language-self
attention module, language-guided graph attention module, and matching
module. The first of these modules is responsible for decomposing the \gls{re}
into three different parts (subject description, intra-class relationships, and
inter-class relationships). The language-guided graph attention module is
responsible for generating the graph representation of the image (the nodes it
generates will be the candidate objects). Finally, the matching module is the
one that computes the matching score between \gls{re} and object (for each of
the candidate objects).

Other authors in exploiting the graphs in this context are
\myCite{yang19:dynam}, who created the model \gls{dga}, which allows multi-step
reasoning. Initially, the model works the same as others with the generation of
a graph from the image and with the mixing of an embedding of the expression in
the graph. But from here on they use a module they call ``analyzer'' and that
is capable of exploring the linguistic structure of \gls{re} and dividing it
into a sequence of constituent expressions. In this way \gls{dga} is able to
carry out a step-by-step reasoning process on these constituent
expressions. Finally, as is common in these models (see \vref{fig:graph}), a
matching score between objects and expression is computed.

\myCite{yang19:cross_modal_relat_infer_groun_refer_expres} also create a
graph-based model that they call \gls{cmrin}. This network consists of a
\gls{cmre}, which is in charge of obtaining the information for the
construction of the graph with ``cross-modal attention'', and a \gls{ggcn} that
uses the information from the previous graph and propagates the information
(which is multi-modal) to be able to compute the matching score.
