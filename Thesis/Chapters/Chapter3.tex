% -*- TeX-master: "../Thesis.tex" -*-


\chapter{Referring Expression Comprehension}%
\label{cha:rec}\index{Referring expresion!comprehension}

\epigraphhead[75]{
  \epigraph{\itshape{}We may hope that machines will eventually compete with
    men in all purely intellectual fields.}
  {---Alan \textsc{Turing}}}


\lettrine{T}{he task of} \gls{rec} consists in, given a linguistic phrase
(\gls{re}) and an image, generate binary masks for the object which the phrase
refers to. In this chapter we will specifically formulate the problem to be
solved (see\ \vref{sec:formulation}), we will analyze the existing datasets and
the evaluation measures (see\ \vref{sec:rec-trainning}) and finally, we will
make an exhaustive study of state-of-the-art works in this area by reviewing
the more recent literature (see\ \vref{sec:sota}).

Regarding the nomenclature, some publications do not agree with the name to
use. Several authors make use of the expression ``Referring Expression
\textit{Segmentation}'' instead of \textit{``Comprehension''} to specify that
the segmentation is being carried out and not just the generation of a bounding
box. However, in this work we will use the term \gls{rec} in the most general
sense, possibly, encompassing both the models that generate the bounding box
and those that generate the segmentation. It is clear that the step from
segmentation to bounding box is trivial, while the opposite conversion has more
complexity (but it can be done using neural models).



\section{Problem Formulation}\label{sec:formulation}

In the task of \gls{rec} two different entries must be given, one of them
related to language and the other to vision.
\begin{itemize}
  \item \textbf{Vision.} It can be an image or a video. In our case we will
  only deal with images.\footnote{The same model would also apply in the case
    of video if we worked frame by frame, but we will not offer a model that
    takes into account the temporal evolution of the frames.}
  \item \textbf{\gls{re}.} It is a linguistic phrase that refers to an
  object. It can occur in two media: audio and text. In this thesis, both
  representations will be admitted.\index{Referring expression}
\end{itemize}
And, the output of this problem will be the generation of a binary segmentation
mask with the referred object or a bounding box\index{Bounding box} (in this
thesis functionality will be added to generate both).

In order to understand it, multiple examples of this problem are shown in\
\vref{fig:rec-examples}. They have tried to show all the possibilities, from
the simplest to the most complex.
\begin{itemize}
  \item \textbf{Multiple objects.} In\ \vref{fig:rec-man} an example is shown
  with two people who differ from each other by a differentiating element
  (\re{cap}). In\ \vref{fig:rec-laptop} the different objects are differentiated
  by their relative position (\re{right}). In\ \vref{fig:rec-army} they are
  differentiated by one quality (\re{white suit}).
  \item \textbf{Multiple categories.} As can be seen in
 \ \vref{fig:rec-umbrella,fig:rec-parent,fig:rec-girl}, it can refer to both
  objects and people within the same image.
  \item \textbf{Specialized vocabulary.} In\ \vref{fig:rec-elephant} we refer to
  a specific type of animal (\re{elephant}) and in\ \vref{fig:rec-couch} the
  expression \re{couch} is used.
  \item \textbf{Secondary objects.} In\ \vref{fig:rec-car} we refer to a small
  secondary object in the image (\re{car}), which is part of the same category
  (car) as the main object (bus).
\end{itemize}

\begin{figure}[p]
  \centering
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Man with cap}\label{fig:rec-man}
    \includegraphics[width=\textwidth]{Images/Man with cap.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Laptop on the right}\label{fig:rec-laptop}
    \includegraphics[width=\textwidth]{Images/Laptop on the right.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Army officer white suit}\label{fig:rec-army}
    \includegraphics[width=\textwidth]{Images/Army officer.jpg}
  \end{subfigure}

  \bigskip
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Black car}\label{fig:rec-car}
    \includegraphics[width=\textwidth]{Images/Black car.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Small middle elephant}\label{fig:rec-elephant}
    \includegraphics[width=\textwidth]{Images/Small middle elephant.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Two seat couch}\label{fig:rec-couch}
    \includegraphics[width=\textwidth]{Images/Couch.jpg}
  \end{subfigure}

  \bigskip
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Umbrella}\label{fig:rec-umbrella}
    \includegraphics[width=\textwidth]{Images/Umbrella.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Parent holding umbrella}\label{fig:rec-parent}
    \includegraphics[width=\textwidth]{Images/Parent.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Little girl pink coat}\label{fig:rec-girl}
    \includegraphics[width=\textwidth]{Images/Little girl.jpg}
  \end{subfigure}
  \caption[Examples of \glsentrylong{rec}]{Examples of \acl{rec}. As can be
    seen, we can refer to objects of the image with \gls{re} in natural
    language and segmentation occurs. Figures created by the author (all). View
    images in color to better appreciate segmentation.}%
  \label{fig:rec-examples}
\end{figure}



\section{Training}\label{sec:rec-trainning}

There are two fundamental things in the process of training a model: a dataset
and a loss function. For the specific task of this work (\gls{rec}), we will
present the most used datasets (see\ \vref{sec:datasets}) and the loss functions
(see\ \vref{sec:loss-functions}).


\subsection{Datasets}\label{sec:datasets}

There are different datasets created exclusively for the training and
evaluation of neural models created to solve the problem discussed here. The
first three datasets considered (RefCOCO, RefCOCO+ and RefCOCOg) take their
images from the well-known dataset \gls{coco}, created by
\myCite{lin14:micros}, while the last one (CLEVR-Ref+) uses synthetic images.

The dataset \gls{coco}, as its name suggests, contains images of everyday life
in everyday environments. Contains multi-object labeling, segmentation mask
annotations, image captioning, key-point detection and panoptic segmentation
annotations. They have a total of 81 categories, divided into 13
super-categories.
\begin{itemize}
  \item \textbf{Super-categories.} They are as follows: person, vehicle,
  vehicle, outdoor, animal, accessory, sports, kitchen, food, furniture,
  electronic, appliance, indoor.
  \item \textbf{Categories.} They are as follows: person, bicycle, car,
  motorcycle, airplane, bus, train, truck, boat, traffic light, fire hydrant,
  stop sign, parking meter, bench, bird, cat, dog, horse, sheep, cow, elephant,
  bear, zebra, giraffe, backpack, umbrella, handbag, tie, suitcase, frisbee,
  skis, snowboard, sports ball, kite, baseball bat, baseball glove, skateboard,
  surfboard, tennis racket, bottle, wine glass, cup, fork, knife, spoon, bowl,
  banana, apple, sandwich, orange, broccoli, carrot, hot dog, pizza, donut,
  cake, chair, couch, potted plant, bed, dining table, toilet, tv, laptop,
  mouse, remote, keyboard, cell phone, microwave, oven, toaster, sink,
  refrigerator, book, clock, vase, scissors, teddy bear, hair drier,
  toothbrush.
\end{itemize}
Contains a total of \num{2500000} images.

\subsubsection{RefCOCO, RefCOCO+ and RefCOCOg}

These datasets were created from \gls{coco} by \myCite{kazemzadeh14:refer_game}
using the game called ReferIt Game. In this two-player game, one of them wrote
a \gls{re} based on an object in an image and the second player, given the
image and \gls{re}, had to click on the correct location of the object that was
being described. If the second user's click coincided in the correct region,
each of the players received a point in the game and the roles were exchanged
for the next image. In this way, a process was created in which \gls{re} were
generated and validated for different objects in images in the same game.

The main difference between RefCOCO and RefCOCO+ is that in RefCOCO+ the
\emph{location} information was disallowed. In total, RefCOCO has \num{142209}
\gls{re} for a total of \num{50000} objects in \num{19994} images. RefCOCO+ has
a similar number of \gls{re} objects and images. An example of each of these
datasets is shown in\ \vref{fig:refcoco}.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \caption{RefCOCO dataset}
    \includegraphics[width=.9\textwidth]{Images/RefCOCO.png}
  \end{subfigure}
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \caption{RefCOCO+ dataset}
    \includegraphics[width=.9\textwidth]{Images/RefCOCO+.png}
  \end{subfigure}
  \caption[Examples from RefCOCO and RefCOCO+ datasets]{Examples from RefCOCO
    and RefCOCO+ datasets. It can be seen that in RefCOCO+ the use of location
    information is not allowed, while in RefCOCO it is valid. Adapted from
    \figcite{kazemzadeh14:refer_game}.}%
  \label{fig:refcoco}
\end{figure}

The difference between RefCOCOg and these two previous ones is that, it
contains only elements \emph{non-trivial}, i.e., there is at least one more
object of the same class as the target object in the image. Regarding size,
RefCOCOg contains \num{104560} expressions, \num{54822} objects and \num{26711}
images.

\subsubsection{CLEVR-Ref+}

The above datasets have been created expressly for \gls{rec} and are made up of
real-world images (which are highly complex). Furthermore, these datasets,
whose images come from \gls{coco}, could be skewed. Therefore,
\myCite{liu19:clevr}, created the CLEVR-Ref+ dataset with images and \gls{re}
generated synthetically. Different situations are considered where objects are
placed in the image with different variable options (such as colors, sizes, and
spatial relationships).

This dataset, however, will not be used in the work that concerns this thesis,
since here we are looking for models that work in the real world with
non-fictional images and natural language.


\subsection{Loss Functions}\label{sec:loss-functions}\index{Loss function}

To train the models it is necessary to have loss functions, which must be
differentiable since in the optimization process we will need to use the
partial derivatives of the loss function with respect to the different
parameters to be trained.

In our case, we will always deal with two classes (the segmentation will be a
binary mask).

\subsubsection{\glsentrylong{ce}}\index{Cross entropy}

One of the best known loss functions in image segmentation is that of
\gls{ce}. We have two probability distributions

\begin{enumerate}
  \item \textbf{Prediction} can be \(P(\hat{Y} = 0) = \hat{p}\) or
  \(P(\hat{Y} = 1) = 1 - \hat{p}\).
  \item The \textbf{ground truth} can either be \(P(Y = 0) = p\) or
  \(P(Y = 1) = 1 - p\). It will always be \(p \in \{0, 1\}\).
\end{enumerate}

The loss function is then defined as
\begin{equation}
  \text{CE}(p, \hat{p}) = -(p\log\hat{p} + (1 - p)\log(1 - \hat{p})).
\end{equation}
Taking into account that \(p \in \{0, 1\}\), the loss function can be rewritten
as follows,
\begin{equation}
  \text{\gls*{ce}}(p, \hat{p}) =
  \begin{cases}
    -\log(1 - \hat{p}) & p = 0 \\
    -\log\hat{p} & p = 1.
  \end{cases}
\end{equation}
That is, if \(p = 1\), the loss function will be \(0\) if and only if
\(\hat{p} = 1\) and it will be larger the more different \(p\) and \(\hat{p}\)
are. The penalty will grow exponentially until it becomes infinite for the
value \(\hat{p} = 0\). Case \(p = 0\) is symmetric.

Various variations of this loss function will be discussed below that may be
useful for training various neural models. TODO. rewrite this section.

\begin{itemize}
  \item \textbf{\gls{wce}.} It is a variant of \gls{ce} in which the positive
  examples are weighted by a coefficient \(\beta\). It is defined as follows,
  \begin{equation}
    \text{\gls*{wce}}(p, \hat{p}) =
    -(\beta p\log\hat{p} + (1 - p)\log(1 - \hat{p})).
  \end{equation}
  Typically used when unbalanced classes appear. It is not too interesting for
  this case. (TODO. es interesante cuando la ground truth es un trozo muy
  pequeño de la imagen)
  \item \textbf{\gls{bce}.} It is similar to \gls{wce} with the only difference
  that a weight is also added to the negative examples. It is defined as
  follows,
  \begin{equation}
    \text{\gls*{bce}}(p, \hat{p}) =
    -(\beta p\log\hat{p} + (1 - \beta)(1 - p)\log(1 - \hat{p})).
  \end{equation}
  \item \textbf{\gls{fl}.} It is a variant of \gls{ce} in which the most
  \emph{complicated} elements of the dataset are affected even more. These are
  the ones with a value of \(\hat{p}\) intermediate between \(0\) and \(1\). It
  is defined as follows,
  \begin{equation}
    \text{\gls*{fl}}(p, \hat{p}) =
    -(\alpha {(1 - \hat{p})}^\gamma p\log\hat{p} +
    (1 - \alpha)p^\gamma (1 - p)\log(1 - \hat{p})).
  \end{equation}
  When \(\gamma = 0\) we obtain \gls{bce}.
  \item \textbf{\gls{dnc}.} This loss function, introduced by
  \myCite{ronneberger15:u}, forces the separation between contiguous
  objects. It is similar to \gls{bce}, but with an additional term of distance
  between objects,
  \begin{equation}
    \text{\gls*{dnc}}(p, \hat{p}) =
    -(w(p)\log\hat{p} + w(p)(1 - p)\log(1 - \hat{p})),
  \end{equation}
  where,
  \begin{equation}
    w(p) = \beta +
    w_0 \cdot \exp\left(-\frac{{(d_1(p) + d_2(p))}^2}{2\sigma^2}\right).
  \end{equation}
  Here \(d_1(p)\) denotes the distance to the edge of the nearest cell and
  \(d_2(p)\) the distance to the edge of the second nearest cell. The rest are
  hyperparameters of the loss function.\footnote{The authors use \(w_0 = 10\)
    and \(\sigma \approx 5\) in their experiments (see Section~3
    from~\cite{ronneberger15:u}).}
\end{itemize}

\subsubsection{Overlap Measures}\index{Overlap measures}

Another type of measure arises with the use of the intersection and union of
the predicted segmentation and the ground truth. This type of loss functions
provide us with \emph{global} information. The well-known Jaccard
index\index{Jaccard index} or coefficient \gls{iou},
\begin{equation}
  J(A,B) = \frac{|A \cap B|}{|A \cup B|} =
  \frac{|A \cap B|}{|A| + |B| - |A \cap B|},
\end{equation}
is typically used to measure the accuracy of a model, but it cannot be used as
a loss function as it is not a differentiable mapping. Yes, it will be used for
the evaluation of the model in\ \vref{sec:quantitative-eval}.

\begin{itemize}
  \item \textbf{\gls{dil}.} It is based on \gls{dc}, a coefficient similar to
  \gls{iou}, which is defined as follows,
  \begin{equation}
    \text{\gls*{dc}}(X, Y) = \frac{2|X \cap Y|}{|X| + |Y|}.
  \end{equation}
  This says coefficient can be defined as a loss function, TODO, explain more
  in depth (see Loss\ functions.pdf), also for intuititve explanation see
  \url{https://www.jeremyjordan.me/semantic-segmentation/#loss}.
  \begin{equation}
    \text{\gls*{dil}}(p, \hat{p}) = 1 -
    \frac{2\sum p_{h, w}\hat{p}_{h, w}}{\sum p_{h, w} + \sum \hat{p}_{h, w}},
  \end{equation}
  where \(p_{h, w} \in \{0, 1\}\), \(0 \leq \hat{p}_{h, w} \leq 1\) and the
  sums are spread over the entire image at width \(w\) and height \(h\).
  \item \textbf{\gls{ti}.} It is a generalization of \gls{dil}. It is defined
  as follows,
  \begin{equation}
    \text{\gls*{ti}}(p, \hat{p}) = 1 -
    \frac{p\hat{p}}{p\hat{p} + \beta(1 - p)\hat{p} + (1 - \beta)p(1 - \hat{p})}.
  \end{equation}
  With the value \(\beta = \frac{1}{2}\), we recover the previous function
  \gls{dil}.
\end{itemize}

\subsubsection{\glsentrylong{giou} Loss}

See first\ \vref{sec:quantitative-measures} for a detailed explanation of the
Jaccard index\index{Jaccard index} or \gls{iou}, which is a quantitative
measure widely used as an evaluation technique.

\begin{remarkBox}
  The reason why \gls{iou} cannot be used directly as a loss function is that
  optimization is infeasible in the case of non-overlapping bounding
  boxes\index{Bounding box} (since, in this case, \gls{iou} has no value and
  therefore no gradient).
\end{remarkBox}

\myCite{rezatofighi19:gener} solve this problem by introducing a loss function
based on \gls{iou} and which they call \gls{giou}. This generalization
guarantees the existence of a gradient in all cases and, therefore, makes it
suitable for use in an optimization process.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.65\textwidth]{Images/GIoU.png}
  \caption[\glsentrylong{giou} algorithm]{\acf{giou} general algorithm applied
    to arbitrary multi-dimensional sets. In the specific case of \gls{rec} they
    will be bounding boxes\index{Bounding box} in \(\R^2\). From
    \figcite{rezatofighi19:gener}.}%
  \label{fig:giou}
\end{figure}

This loss function is summarized in\ \vref{fig:giou}. It is a generalization
that preserves the relevant properties of \gls{iou}, but that corrects the
problems related to its differentiability.

\subsubsection{Combinations}

Many more loss functions can be obtained by simple linear combination of the
above. The combination,
\begin{equation}
  \text{\gls*{ce}}(p, \hat{p}) + \text{\gls*{dil}}(p, \hat{p}),
\end{equation}
is quite popular, since it combines local information (\gls{ce}) with global
information (\gls{dil}).



\section{Evaluation Techniques}\label{sec:eval-measure}

In any area of \gls{ml} it will be necessary to have evaluation techniques to
be able to decide if the results obtained with the model are good enough and
also to be able to make comparisons.

The most useful techniques are usually quantitative metrics, which will be
discussed in\ \vref{sec:quantitative-measures}. Furthermore, in the specific
case of \gls{rec}, due to the use of images, it will also be possible to carry
out a visual / qualitative evaluation (see\ \vref{sec:qualitative-eval}).


\subsection{Quantitative Measures}\label{sec:quantitative-measures}

This corresponds to an evaluation of the model in a numerical way with
metrics. The different evaluation measures typically used to address this
problem are related to the computation of \gls{iou} or Jaccard
index\index{Jaccard index}. This index is based on the concepts of intersection
and union between the predicted segmentation (which is a \emph{binary} mask)
and the ground truth (diagrams with these concepts are shown in\
\vref{fig:sets}).

\begin{figure}[ht]
  \begin{subfigure}[t]{.5\textwidth}
    \centering
    \caption{Union of sets \(A\) and \(B\).}
    \includesvg[width=.55\textwidth]{SVGs/Union_of_sets_A_and_B.svg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.5\textwidth}
    \centering
    \caption{Intersection of sets \(A\) and \(B\).}
    \includesvg[width=.55\textwidth]{SVGs/Intersection_of_sets_A_and_B.svg}
  \end{subfigure}
  \caption[Union and intersection of sets \(A\) and \(B\)]{Graphic
    representation of the union and intersection of sets called \(A\) and
    \(B\). From \figcite{contributors21:jaccar} (both).}%
  \label{fig:sets}
\end{figure}

From here arises the well-known Jaccard index\index{Jaccard index} or
coefficient \gls{iou},
\begin{equation}
  J(A,B) = \frac{|A \cap B|}{|A \cup B|} =
  \frac{|A \cap B|}{|A| + |B| - |A \cap B|},
\end{equation}
which is typically used to measure the accuracy of a model, but---as we have
commented previously---it cannot be used as a loss function as it is not a
differentiable application.

\begin{figure}[ht]
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \caption[Bounding boxes example]{Bounding boxes\index{Bounding box}
      example}
    \includegraphics[width=.8\textwidth]{Images/Object detection Bounding Boxes.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \caption{\gls{iou} visual equation}
    \includegraphics[width=.8\textwidth]{Images/Intersection over Union.png}
  \end{subfigure}
  \caption[Jaccard index explanation]{Explanation and example of the Jaccard
    index\index{Jaccard index} in the case of bounding boxes. From
    \figcite{contributors21:jaccar} (both).}
\end{figure}

This index provides relevant information on how tight a bounding
box\index{Bounding box} is.\footnote{The case of bounding box is studied for
  simplicity, but the same concept applies in the case of pixel by pixel
  segmentation.} It is evident that the Jaccard index\index{Jaccard index}
takes a value between \(0\) and \(1\), being \(0\) when there is no
intersection between the bounding boxes\index{Bounding box} and taking the
value of \(1\) when the correspondence is exact.

\subsubsection{Mean and Overall \glsentryshort{iou}}

Using \gls{iou} as a metric to evaluate a segmentation or bounding
box\index{Bounding box} can be done in two ways. The first is to average all
the \gls{iou} values on the test dataset, as follows,
\begin{equation}
  \text{\Acl{miou}} = \frac{1}{N}\sum_{i=0}^N \text{\gls{iou}}_i,
\end{equation}
where \(N\) corresponds to the size of the test dataset and
\(\text{\gls{iou}}_i\) is the \gls{iou} value corresponding to the \(i\)-th
image. This metric, for obvious reasons, is called \gls{miou}.

Another possibility for using the \gls{iou} as a metric is the \emph{overall}
\gls{iou}, defined as the division between the total intersection area and the
total union area. This areas are accumulated by iterating throughout the
dataset, i.e.,
\begin{equation}
  \text{Overall \gls{iou}} =
  \frac{\sum_{i=0}^N I_i}{\sum_{i=0}^n U_i},
\end{equation}
where \(I_i\) and \(U_i\) correspond to the intersection and union
(respectively) between the prediction and the ground truth for the \(i\)-th
image in the test dataset.

One of the problems that \emph{overall} \gls{iou} has is that it favors
large regions like the ground or the sky.

\subsubsection{Precision at Threshold}

This metric is commonly used in segmentation task (e.g., is used in
\acs{pascal} \gls{voc} challenge, by
\myCite{everingham10:pascal_visual_objec_class_voc_chall}).

Here, for each sample from the test dataset, it will be judged as true/false
positive by using the \gls{iou} index. A particular sample will be considered a
correct detection iff the \gls{iou} between the prediction and the ground truth
is greater than some predefined threshold. For example, Prec@0.5 is the
percentage of samples where the predicted segmentation overlaps with the ground
truth region by at least 50\%.

Different thresholds can be used for evaluation, for instance computing
Prec@0.5, Prec@0.7 and Prec@0.9. Of course, higher thresholds correspond to
a harder metric and, thus, accuracy will decrease.


\subsection{Qualitative Evaluation}\label{sec:qualitative-eval}

In addition to making a quantitative evaluation, which provides us with
numerical values for the different models, it is also interesting to carry out
a qualitative evaluation.

In this specific case of \gls{rec} this evaluation can be done visually, since
it involves text and images. To perform this evaluation in the most general way
it is important to test a very diverse range of both \gls{re} and images. To do
this, \gls{re} can be used by involving different elements each time (and
combining them): spatial information within the image, name of the object,
relative positioning, color characteristic of the object, position of the
object, number of existing objects, relative size of the object, etc.



\section{Related Work}\label{sec:sota}

The current state-of-the-art methods for \gls{rec} can be divided into three
main classes: joint embedding (see\ \vref{sec:joint}), modular models (see\
\vref{sec:modular}) and graph convolution based models (see\ \vref{sec:graph}).


\subsection{Multimodal Embedding}%
\label{sec:joint}\index{Multimodal!embedding}

Multimodal embedding methods are very typical in any of the multimodal learning
tasks. What is sought in them is to find a multidimensional space where
encodings of image and language can ``coexist'' in common. This idea is
represented graphically in\ \vref{fig:joint}. This multidimensional space will
typically be \(\R^n\), which is a normed space. One of the desirable
characteristics would be that the encodings of images and language similar to
each other were ``close'' in this space (in terms of norm).

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Joint.png}
  \caption[Multimodal embedding technique]{Multimodal embedding into
    visual-semantic space. As you can see, matching pairs are closer (in terms
    of norm) that non-matching pairs in the joint space. From
    \figcite{cornia18:towar_cycle_consis_model_text_image_retriev}.}%
  \label{fig:joint}
\end{figure}

Therefore, here, to perform \gls{rec}, the first thing we will do is encode the
image and \gls{re} separately in the same vector space. For this, \gls{cnn} are
very useful to generate image representations (extracting the most relevant
features) and for the coding of phrases \gls{rnn} (with, for example,
\gls{lstm}) and transformers are used.

The first deep learning model for referring expression generation and
comprehension is from \myCite{mao16:gener}, where they use a model \gls{cnn}
with which they extract the visual features and a network of type \gls{lstm}
for \emph{generating} the referring expression. It also gives a solution for the
inverse problem of \emph{comprehension}.

Within this type of model fits the one proposed by \myCite{bellver20:refvos}
where a neural network of type \gls{cnn} is also used for the encoding of the
image, but the transformer is used as a language encoder. Then to achieve
multimodal embedding, the encoded linguistic phrase is converted into a
256-dimensional vector and multiplied element-wise with the visual
features. This model will be studied in depth in\ \vref{cha:models}.


\subsection{Modular Models}\label{sec:modular}\index{Modular model}
Modular models have been used successfully in many tasks both in the scope of
\gls{cv}, and in \gls{nlp}. The technique used in these cases is to decompose
\gls{re} into different components, in which it is sought to attack different
reasoning.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/MattNet.png}
  \caption[\glsentrylong{mattnet}]{\gls{mattnet}: given an expression, it is
    divided into three phrase embeddings, which are input to three visual
    modules that process the described visual region in different ways and
    compute individual matching scores. From \figcite{yu18:mattn}.}%
  \label{fig:mattnet}
\end{figure}

An example of these modular models is the one presented by \myCite{yu18:mattn},
which is graphically represented in\ \vref{fig:mattnet}. In this case there are
three differentiated modules: the \emph{subject} module, the \emph{location}
module and the \emph{relationship} module. Each of them computes different
scores, which are then used to calculate an overall score.

Starting from the base of \gls{mattnet},
\myCite{liu19:improv_refer_expres_groun_cross_atten_erasin} propose
\gls{cmatterase}, which is a training strategy for this type of task. It is
based on the idea of erasing the part most used by the model from the
linguistic or visual part, so that the model is forced to learn more complex
structures.\footnote{It is partly similar to the strategy of \emph{dropout}
  used in the training of fully connected neural networks, which is used to
  avoid dependency on specific neurons and thus prevent overfitting of the
  model.} Likewise, it modifies the initial model (\gls{mattnet}), considering
the global image as one more characteristic.


\subsection{Graph Generation}\label{sec:graph}
In the task that concerns us, the understanding by the model of \gls{re} is
essential. These types of expressions contain different objects and
relationships between them. In other words, it is common to refer to an object
not only because of its intrinsic properties, but also because of its
relationship with the objects that surround it. The mathematical tool that best
represents this phenomenon is that of a graph: the nodes represent different
objects and the different edges are the existing relationship between objects
(see\ \vref{fig:graph}).

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Graph model.png}
  \caption[Graph based model representation]{Summary representation of
    graph-based models. From the image, the graph representation is built,
    which is then updated with the expression embedding and computed a matching
    score between objects and expression. From
    \figcite{qiao20:refer_expres_compr}.}%
  \label{fig:graph}
\end{figure}

The use of graphs in the task of \gls{rec} has been used with success by
various authors. Among them \myCite{wang19:neigh}, he proposes
\gls{lgran}. This model consists of three differentiated modules: language-self
attention module, language-guided graph attention module, and matching
module. The first of these modules is responsible for decomposing the \gls{re}
into three different parts (subject description, intra-class relationships, and
inter-class relationships). The language-guided graph attention module is
responsible for generating the graph representation of the image (the nodes it
generates will be the candidate objects). Finally, the matching module is the
one that computes the matching score between \gls{re} and object (for each of
the candidate objects).

Other authors in exploiting the graphs in this context are
\myCite{yang19:dynam}, who created the model \gls{dga}, which allows multi-step
reasoning. Initially, the model works the same as others with the generation of
a graph from the image and with the mixing of an embedding of the expression in
the graph. But from here on they use a module they call ``analyzer'' and that
is capable of exploring the linguistic structure of \gls{re} and dividing it
into a sequence of constituent expressions. In this way \gls{dga} is able to
carry out a step-by-step reasoning process on these constituent
expressions. Finally, as is common in these models (see\ \vref{fig:graph}), a
matching score between objects and expression is computed.

\myCite{yang19:cross_modal_relat_infer_groun_refer_expres} also create a
graph-based model that they call \gls{cmrin}. This network consists of a
\gls{cmre}, which is in charge of obtaining the information for the
construction of the graph with ``cross-modal attention'', and a \gls{ggcn} that
uses the information from the previous graph and propagates the information
(which is multi-modal) to be able to compute the matching score.
