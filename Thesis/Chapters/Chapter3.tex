% -*- TeX-master: "../Thesis.tex" -*-


\chapter{Referring Expression Comprehension} \label{cha:rec}

\epigraphhead[75]{
  \epigraph{\itshape We may hope that machines will eventually compete with men
    in all purely intellectual fields.}
  {---\scshape Alan Turing}
}


\lettrine{R}{eferring} expression comprehension is the task of, given a
linguistic phrase (\gls{re}) and an image, generate binary masks for the object
which the phrase refers to. En esta capítulo formularemos de manera concreta el
problema a resolver (\vref{sec:formulation}), analizaremos los datasets
existentes y las medidas de evaluación (\vref{sec:data-eval}) y por último,
haremos un estudio exhaustivo del \gls{sota} en este ámbito revisando la
literatura más reciente (\vref{sec:sota}).


\section{Problem formulation} \label{sec:formulation}

En la tarea de \gls{rec} se deben dar dos entradas diferenciadas, una de ella
relacionada con lenguaje y la otra con visión.
\begin{itemize}
  \item \textbf{Vision}. Puede ser una imagen o un video. En nuestro caso
  trataremos únicamente con imágenes\footnote{El mismo modelo también aplicaría
  en el caso de vídeo si se trabajra frame por frame, pero no ofreceremos un
  modelo que tenga en cuenta la evolución temporal de los frames.}.
  \item \textbf{\gls{re}}. Es un frase linguistica que se refiere a un
  objeto. Puede darse en dos medios: audio y texto. En este tesis se admitirán
  ambas dos representaciones.
\end{itemize}
Y, la salida de este problema será la generación de una máscara binaria de
segmentación con el objeto referido o una bounding box (en esta tesis se
añadirá funcionalidad para generar ambas).

Para poder entenderlo se muestra en la \vref{fig:rec-examples} múltiples
ejemplos de este problema. Se han intentado mostrar todas las posibilidades,
desde las más simples a las más complejas.
\begin{itemize}
  \item \textbf{Multiple objects}. En la \vref{fig:rec-man} se muestra un
  ejemplo con dos personas en las que se diferencia una de otra por un elemento
  diferenciador (``cap''). En la \vref{fig:rec-laptop} se diferencian los
  diferentes objetos por su posición relativa (``right''). En la
  \vref{fig:rec-army} se diferencian por una cualidad (``white suit'').
  \item \textbf{Categorías multiples}. Como se puede ver en las
  \vref{fig:rec-umbrella,fig:rec-parent,fig:rec-girl}, se puede referrir tanto
  a objetos como personas dentro de una misma imagen.
  \item \textbf{Vocabulario especializado}. En la \vref{fig:rec-elephant} nos
  referimos a un tipo de animal concreto (``elephant'') y en la
  \vref{fig:rec-couch} se usa la expresión de ``couch''.
  \item \textbf{Objetos secundarios}. En la \vref{fig:rec-car} nos referimos a
  un objeto secundario y pequeño de la imagen (``car''), que forma parte de la
  misma categoría (automóvil) que el objeto principal (autobús).
\end{itemize}

\begin{figure}[tp]
  \centering
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Man with cap.} \label{fig:rec-man}
    \includegraphics[width=\textwidth]{Images/Man with cap.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Laptop on the right.} \label{fig:rec-laptop}
    \includegraphics[width=\textwidth]{Images/Laptop on the right.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Army officer white suit.} \label{fig:rec-army}
    \includegraphics[width=\textwidth]{Images/Army officer.jpg}
  \end{subfigure}

  \bigskip
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Black car.} \label{fig:rec-car}
    \includegraphics[width=\textwidth]{Images/Black car.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Small middle elephant.} \label{fig:rec-elephant}
    \includegraphics[width=\textwidth]{Images/Small middle elephant.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Two seat couch.} \label{fig:rec-couch}
    \includegraphics[width=\textwidth]{Images/Couch.jpg}
  \end{subfigure}

  \bigskip
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Umbrella.} \label{fig:rec-umbrella}
    \includegraphics[width=\textwidth]{Images/Umbrella.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Parent holding umbrella.} \label{fig:rec-parent}
    \includegraphics[width=\textwidth]{Images/Parent.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Little girl pink coat.} \label{fig:rec-girl}
    \includegraphics[width=\textwidth]{Images/Little girl.jpg}
  \end{subfigure}
  \caption[Examples of \acl*{rec}]{Examples of \acl{rec}. Como se puede ver,
    podemos referirnos a objetos de la imagen con \gls{re} en lenguaje natural
    y se produce la segmentación.}
  \label{fig:rec-examples}
  \source{Created by the author}
\end{figure}


\section{Datasets and Evaluation} \label{sec:data-eval}

Para el desarrollo y validación de los modelos presentados para la resolución
del problema de \gls{rec}, son necesarias dos cosas principalmente: datasets y
evaluation measures. Todo modelo neuronal necesitará de una base de datos con
la que poder entrenar (ajustar) los parámetros de los que dispone. Así mismo,
será necesario reservar parte de este dataset para poder realizar una
evaluación del mismo haciendo uso de diferentes medidas, esto es lo que hará
posible la comparación entre diferentes modelos.

En este apartado se presentarán los \emph{datasets} más importantes existentes en la
actualidad en la \vref{sec:datasets} y las \emph{evaluation measures}
más utilizadas en la \vref{sec:eval-measure}.

\subsection{Datasets} \label{sec:datasets}

Existen diferentes datasets creados exclusivamente para el entrenamiento y
evaluación de modelos neuronales creados para resolver el problema aquí
tratado. Los tres primeros datasets considerados (RefCOCO, RefCOCO+ y RefCOCOg)
toman sus imágenes del conocido dataset \gls{coco}
\cite{lin14:micros}\footfullcite{lin14:micros}, mientras que el último
(CLEVR-Ref+) usa imágenes sintéticas.

El dataset \gls{coco}, como su nombre indica, contiene imágenes del día a día
en entornos cotidianos. Contiene multi-object labelling, segmentation mask
annotations, image captioning, key-point detection and panoptic segmentation
annotations. Tienen un total de 81 categorías, dividias en 13 supercategorías.
\begin{itemize}
  \item \textbf{Supercategories}. Son las siguientes: person, vehicle, vehicle,
  outdoor, animal, accessory, sports, kitchen, food, furniture, electronic,
  appliance, indoor.
  \item \textbf{Categories}. Son las siguientes: person, bicycle, car,
  motorcycle, airplane, bus, train, truck, boat, traffic light, fire hydrant,
  stop sign, parking meter, bench, bird, cat, dog, horse, sheep, cow, elephant,
  bear, zebra, giraffe, backpack, umbrella, handbag, tie, suitcase, frisbee,
  skis, snowboard, sports ball, kite, baseball bat, baseball glove, skateboard,
  surfboard, tennis racket, bottle, wine glass, cup, fork, knife, spoon, bowl,
  banana, apple, sandwich, orange, broccoli, carrot, hot dog, pizza, donut,
  cake, chair, couch, potted plant, bed, dining table, toilet, tv, laptop,
  mouse, remote, keyboard, cell phone, microwave, oven, toaster, sink,
  refrigerator, book, clock, vase, scissors, teddy bear, hair drier, toothbrus.
\end{itemize}
Contiene un total de \num{2500000} imágenes.

\subsubsection{RefCOCO, RefCOCO+ and RefCOCOg}
Estos datasets fueron creados a partir de \gls{coco} por
\myCite{kazemzadeh14:refer_game} usando el juego llamado ReferIt Game. En este
juego de dos jugadores consistía en que uno de ellos escribía una \gls{re}
basada en un objeto de una imagen y el segundo jugador, dada la imagen y la
\gls{re}, tenía que clicar en la localización correcta del objeto que se
describía. Si el click del segundo usuario coincidía en la región correcta,
cada uno de los jugadores recibía un punto en el juego y los roles se
intercambiaban para la siguiente imagen. De esta manera se creaba un proceso en
el cual se generaban y se validaban \gls{re} para diferentes objetos en
imágenes en el mismo juego.

La principal diferencia entre RefCOCO y RefCOCO+, es que en RefCOCO+ la
\emph{location} information was disallowded. En total, RefCOCO cuenta con
\num{142209} \gls{re} para un total de \num{50000} objetos en \num{19994}
imágenes. RefCOCO+ cuenta con un número similar de \gls{re}, objetos e
imágenes. En la \vref{fig:refcoco} se muestra un ejemplo de cada uno de estos
datasets.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \caption{RefCOCO dataset.}
    \includegraphics[width=.9\textwidth]{Images/RefCOCO.png}
  \end{subfigure}
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \caption{RefCOCO+ dataset.}
    \includegraphics[width=.9\textwidth]{Images/RefCOCO+.png}
  \end{subfigure}
  \caption[Examples from RefCOCO and RefCOCO+ datasets]{Examples from RefCOCO
    and RefCOCO+ datasets. Se puede observar que en RefCOCO+ no se permite el
    uso de location information, mientras que en RefCOCO sí que es válida.}
  \label{fig:refcoco}
  \source{Modified from \cite{kazemzadeh14:refer_game}}
\end{figure}

La diferencia entre RefCOCOg y estos dos anteriores es que, contiene solo
elementos \emph{non-trivial}, i.e., hay al menos un objeto más de la misma
clase que el objeto target en la imagen. En cuanto a tamaño, RefCOCOg contiene
\num{104560} expresiones, \num{54822} objetos y \num{26711} imágenes.

\subsubsection{CLEVR-Ref+}
Los datasets anteriores, han sido creados expresamente para \gls{rec} y están
formados por imágenes del real-world (que presentan una alta
complejidad). Además, estos datasets, cuyas imágenes provienen de \gls{coco}
podrían estar sesgados. Por ello, \myCite{liu19:clevr}, crearon el CLEVR-Ref+
dataset con imágenes y \gls{re} generadas de manera sintética. Se consideran
diferentes situaciones en las que los objetos se situan en la imagen con
diferentes opciones variables (como colors, sizes and spatial relationships).

Este dataset, sin embargo, no será usado en el trabajo que concierne esta
tesis, ya que, aquí se busca modelos que funcionen en el mundo real con
imágenes no-ficticias y lenguaje natural.

\subsection{Evaluation measures} \label{sec:eval-measure}

Esto se corresponde con una evaluación del modelo de manera numérica. Las
diferentes evaluation measures usadas típicamente para abordar este problema
están relacionadas con el cálculo del \gls{iou} (en la \vref{sec:evaluation}
será explicado en profundidad).

También es posible definir un índice de accuracy de la siguiente manera: si el
\gls{iou} es más grande que \num{0.5} la predicción se considera un true
positivie, y, si no, se considera como un false positive. Después se puede
hacer una media sobre todo el dataset.


\section{Related work} \label{sec:sota}

Los métodos actuales \gls{sota} para \gls{rec} se pueden dividir en tres
grandes clases: joint embedding (see \vref{sec:joint}), modelos modulares (see
\vref{sec:modular}) and graph convolution based models (see
\vref{sec:graph}).

\subsection{Multimodal embedding} \label{sec:joint} Los métodos de multimodal
embedding son muy típicos en cualquiera de las tareas del aprendizaje
multimodal. En ellos lo que se busca es encontrar un espacio multidimensional
donde puedan ``convivir'' codificaciones de imagen y lenguaje en común. Esta
idea viene representada de manera gráfica en la \vref{fig:joint}. Este espacio
multidmensional será típicamente \(\R^n\), que es un espacio normado. Una de
las caracterísitcas deseables sería que las codificaciones de imágnes y
lenguaje similares entre ellas quedasen ``cerca'' en este espacio (en términos
de norma).

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Joint.png}
  \caption[Multimodal embedding technique]{Multimodal embedding into
    visual-semantic space. Como se puede ver, las matching pairs quedan más
    cerca (en términos de norma) que las non-matching pairs en el joint space.}
  \label{fig:joint}
  \source{From \cite{cornia18:towar_cycle_consis_model_text_image_retriev}}
\end{figure}

Por tanto, aquí, para realizar \gls{rec}, lo primero que haremos es codificar
la imagen y la \gls{re} por separado en un mismo espacio vectorial. Para ello,
\gls{cnn} son muy útiles para generar representaciones de la imagen (extrayendo
las features más relevantes) y para la codificación de frases se usan \gls{rnn}
(con, por ejemplo, \gls{lstm}) y transformers.

El primer modelo de deep learning para referring expression generation and
comprehension, es de \myCite{mao16:gener}, donde usan un modelo \gls{cnn} con
el que extraen las features visuales y una red de tipo \gls{lstm} para
\emph{generar} la referring expression. También da una solución para el
problema inverso de \emph{comprehension}.

Dentro de este tipo de modelos encaja el propuesto por
\myCite{bellver20:refvos} donde también se usa una red neuronal de tipo
\gls{cnn} para la codificación de la imagen, pero como codificador de lenguaje
se usa el transformer. Después para conseguir el multimodal embedding, se
convierte la frase linguistica codificada en un vector 256-dimensional y se
multiplica element-wise con las features visuales. Este modelo será estudiado
en profundidad en \vref{cha:model}.

\subsection{Modelos modulares} \label{sec:modular}
Los modelos modulares han sido usado con éxito en muchas tareas tanto en el
ámbito de \gls{cv}, como en \gls{nlp}. La técnica usada en estos casos, es la
de descomponer la \gls{re} en diferentes componentes, en los que se busca
atacar diferentes razonamientos.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/MattNet.png}
  \caption[\acl*{mattnet}]{\gls{mattnet}: given an expression, it is divided
    into three phrase embeddings, which are input to three visual modules that
    process the described visual region in different ways and compute
    individual matching scores.}
  \label{fig:mattnet}
  \source{From \cite{yu18:mattn}}
\end{figure}

Un ejemplo de estos modelos modulares es el presentado por \myCite{yu18:mattn},
que se encuentra representado gráficamente en la \vref{fig:mattnet}. En este
caso hay tres módulos diferenciados: el \emph{subject} module, el
\emph{location} module y el módulo de \emph{relationship}. Cada uno de ellos
computa diferentes scores, que después son usados para el cálculo de un overall
score.

Partiendo de la base de \gls{mattnet},
\myCite{liu19:improv_refer_expres_groun_cross_atten_erasin} proponen
\gls{cmatterase}, que es una estrategia de entrenamiento para este tipo de
tareas. Se base en la idea de borrar la parte más usada por el modelo de la
parte lingüistica o visual, de manera que se fuerza al modelo a aprender
estructuras más complejas\footnote{Es en parte similar a la estrategia de
  \emph{dropout} usada en el entrenamiento de fully connected neural networks,
  que se usa para evitar la dependencia en nueronas concretas y así prevenir el
  overfitting del modelo.}. Así mismo, modifica el modelo inicial
(\gls{mattnet}), considerando la imagen global como una característica más.

\subsection{Graph generation} \label{sec:graph}
En la tarea que nos concierne, es fundamental la comprensión por parte del
modelo de la \gls{re}. Este tipo de expresiones contienen diferentes objetos y
relaciones entre ellos. Es decir, es habitual referirse a un objeto no solo por
sus propiedades intrínsecas, sino también por su relación con los objetos que
lo rodean. La herramienta matemática que mejor representa este fenómeno es el
de un grafo: los nodos representan diferentes objetos y las diferentes arisas
son la relación entre objetos existente (ver \vref{fig:graph}).

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Graph model.png}
  \caption[Graph based model representation]{Representación resumida de los
    modelos basados en grafos. De la imagen se construye la representación en
    forma de grafo, que después es actualizada con el embedding de la expresión
    y computado un matching score entre objetos y expresión.}
  \label{fig:graph}
  \source{From \cite{qiao20:refer_expres_compr}}
\end{figure}

El uso de grafos en la tarea de \gls{rec} ha sido usado con éxito por diversos
autores. Entre ellos \myCite{wang19:neigh}, propone \gls{lgran}. Este modelo
consiste en tres modulos diferenciados: language-self attention module,
language-guided graph attention module, and matching module. El primero de
estos modulos se encarga en descomponer las \gls{re} en tres partes diferentes
(subject description, intra-class relationships, and inter-class
relationships). El modulo de language-guided graph attention es el que se
encarga de generar la representación en forma de grafo de la imagen (los nodos
que genera serán los objetos candidatos). Por último, el matching module es el
que compute el matching score entre \gls{re} y objeto (para cada uno de los
objetos candidatos).

Otros autores en explotar los grafos en este contexto son
\myCite{yang19:dynam}, que crean el modelo \gls{dga}, que permite el
razonamiento multi-step. En un comienzo, el modelo funciona igual que otros con
la generación de un grafo desde la imagen y con la mezcla de un embedding de la
expresión en el grafo. Pero a partir de aquí utilizan un módulo que llaman
``analyzer'' y que es capaz de explorar la estructura linguistica de la
\gls{re} y dividirla en una secuencia de expresiones constituyentes. De esta
manera \gls{dga} es capaz de realizar un proceso de razonamiento paso a paso
sobre estas expresiones constituyentes. Finalmente, como es común en estos
modelos (ver \vref{fig:graph}) se computa un matching score entre objetos y
expresión.

\myCite{yang19:cross_modal_relat_infer_groun_refer_expres} también crean un
modelo basado en grafos al que llaman \gls{cmrin}. Esta red consiste en un
\gls{cmre}, que se encarga de obtener la información para la construción del
grafo con ``cross-modal attention'', y de un \gls{ggcn} que usa la información
del grafo anterior y propaga la información (que es multi-modal) para poder
computar el matching score.