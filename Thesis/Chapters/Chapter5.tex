% -*- TeX-master: "../Thesis.tex" -*-


\chapter{Results and Comparison}\label{cha:results}

\epigraphhead[75]{
  \epigraph{\itshape However beautiful the strategy, \\
    you should occasionally look at the results.}
  {---Winston \textsc{Churchill}}
}


The results obtained with the model described in \vref{cha:models} will now be
studied and compared with other state-of-the-art models. Here the evolution of
the model in its different iterations will not be shown, but only the last
version selected will be considered. The evaluation will be carried out both
quantitatively (see \vref{sec:quantitative-eval}) and qualitatively (see
\vref{sec:qualitative-eval-res}).



\section{Quantitative Evaluation}\label{sec:quantitative-eval}

Regarding the quantitative evaluation of the model (as already discussed in
\vref{sec:quantitative-measures}) there are different metrics to use. Among
them, three stand out: the mean and overall \gls{iou}, and precision at
threshold. In the case of the model created in this work, we can evaluate it
with any metric that we consider appropriate and in any dataset, since we have
its implementation. Now, what is really interesting is being able to compare it
with other state-of-the-art models that currently exist. The literature
consulted in this work typically uses two fundamental metrics: overall
\gls{iou} and precision at 0.5. The Prec@0.5 is used as a measure of accuracy,
i.e., the number of percentage of samples where the predicted segmentation
overlaps with the ground truth region by at least 50\% is computed.

In this section we will show comparative tables of the quantitative evaluation
of the model of this work and of other current models. For this, a study of the
overall \gls{iou} (see \vref{sec:overall-iou}) and a study of the accuracy or
Prec@0.5 (see \vref{sec:accuracy}) will be carried out. Unfortunately, it was
not possible to present the same number of models in both two sections, due to
the absence of these evaluation metrics in the original publications of the
models.


\subsection{Overall \glsentrylong{iou}}\label{sec:overall-iou}

Regarding the overall \gls{iou}, data have been collected from multiple models,
which are shown in a summarized way in \vref{tab:overall-iou}. The evaluation
has been carried out on the RefCOCO and RefCOCO+ datasets with the splits
\code{val/testA/testB}. As previously mentioned, the RefCOCO+ dataset presents
\glspl{re} of greater complexity, therefore it presents lower overall \gls{iou}
values than in the RefCOCO dataset for all models.

\begin{table}[p]
  \centering
  \caption[Overall \glsentrylong{iou} model comparison]{Overall \acl{iou} model
    comparison. For each of the models the overall \acs{iou} is shown for the
    splits \code{val/testA/testB} in the datasets RefCOCO and RefCOCO+. The
    state of the art in each category is shown in bold. Full names for model
    acronyms can be found in section \textsl{Model Acronyms} on
    page~\pageref{sec:ac-model}. Table created by the author using data from
    second column references.}\label{tab:overall-iou}
  \rowcolors{5}{rowColor}{}
  \begin{tabular}{lc*6c}
    \toprule
    & & \multicolumn{3}{c}{\textbf{RefCOCO}} & \multicolumn{3}{c}{\textbf{RefCOCO+}} \\
    \cmidrule(lr){3-5}\cmidrule(lr){6-8}
    \textbf{Method} & \textbf{Paper}                                               & \code{val}     & \code{testA}   & \code{testB}   & \code{val}     & \code{testA}   & \code{testB}   \\
    \midrule
    \acs{asgn}      & \cite{qiu20:refer_image_segmen_gener_adver_learn}            & 50.46          & 51.20          & 49.27          & 38.41          & 39.79          & 35.97          \\
    \acs{brinet}    & \cite{hu20:bi_direc_relat_infer_networ}                      & 61.35          & 63.37          & 59.57          & 48.57          & 52.87          & 42.13          \\
    \acs{cac}       & \cite{chen19:refer_expres_objec_segmen_caption_aware_consis} & 58.90          & 61.77          & 53.81          & -              & -              & -              \\
    \acs{cmpc}      & \cite{huang20:refer_image_segmen_cross_modal_progr_compr}    & \textbf{61.36} & \textbf{64.53} & \textbf{59.64} & \textbf{49.56} & \textbf{53.44} & \textbf{43.23} \\
    \acs{cmsa}      & \cite{ye21:refer_segmen_images_videos_cross}                 & 58.32          & 60.61          & 55.09          & 43.76          & 47.60          & 37.89          \\
    \acs{dmn}       & \cite{margffoy-tuay18:dynam_multim_instan_segmen}            & 49.78          & 54.83          & 45.13          & 38.88          & 44.22          & 32.29          \\
    \acs{mattnet}   & \cite{yu18:mattn}                                            & 56.51          & 62.37          & 51.70          & 46.67          & 52.39          & 40.08          \\
    \acs{refvos}    & \cite{bellver20:refvos}                                      & 59.45          & 63.19          & 54.17          & 44.71          & 49.73          & 36.17          \\
    \acs{rmi}       & \cite{liu17:recur_multim_inter_refer_image_segmen}           & 45.18          & 45.69          & 45.57          & 29.86          & 30.48          & 29.50          \\
    \acs{rrn}       & \cite{li18:refer_image_segmen_recur_refin_networ}            & 55.33          & 57.26          & 53.95          & 39.75          & 42.15          & 36.11          \\
    \acs{step}      & \cite{chen19:see_throug_text_group_refer_image_segmen}       & 60.04          & 63.46          & 58.97          & 48.18          & 52.33          & 40.41          \\
    \bottomrule
  \end{tabular}\\[1.25ex]
  {\small\textbf{Note}. Models arranged in alphabetical order.}
\end{table}

Several of the evaluated models have been previously described in
\vref{sec:sota}. It should be noted that the differences between the overall
metric \gls{iou} are not too large comparatively by model. You can see an
approximate range of 50--60 for the RefCOCO dataset and 40--50 for the RefCOCO+
dataset (which has more complex expressions). The model presented in this work
outperforms some of the models in the RefCOCO dataset and remains close to the
state of the art. However, in the RefCOCO+ dataset it presents less promising
results.

The absolute winner regarding this metric is the model created by
\myCite{huang20:refer_image_segmen_cross_modal_progr_compr} and called
\gls{cmpc}. It presents the highest values of overall \gls{iou} in all the
categories. This complex reasoning method is based on a multi-step
structure. In the words of its authors,

\begin{quoteBox}
  The \gls{cmpc} module first employs entity and attribute words to perceive
  all the related entities that might be considered by the expression. Then,
  the relational words are adopted to highlight the correct entity as well as
  suppress other irrelevant ones by multimodal graph reasoning. In addition, we
  further leverage a module to integrate the reasoned multimodal features from
  different levels with the guidance of textual information. In this way,
  features from multilevels could communicate with each other and be refined
  based on the textual context. \tcblower
  \quotecite{berners-lee21:introd_web_acces}
\end{quoteBox}

The model \gls{cmpc}, despite being superior in terms of this metric, also
presents a considerably higher complexity than the model presented in this
work. Similarly, the model \gls{brinet}, created by
\myCite{hu20:bi_direc_relat_infer_networ}, is superior in terms of performance,
but in exchange for presenting greater complexity. Precisely, the model
presented in this thesis consists of a simpler and fully end-to-end network,
which presents results that are competitive with the current state of the art.

Also, it should be noted that the proposed model outperforms the \gls{mmi}
model created by \myCite{mao16:gener}. This specific comparison is interesting
because this model is also based on a joint embedding of language and image.


\subsection{Accuracy or Precision at 0.5}\label{sec:accuracy}

Regarding the accuracy or Prec@0.5, a comparative study has also been made,
which is shown in \vref{tab:accuracy}. It should be remembered that Prec@0.5
consists of computing the number of percentage of samples where the predicted
segmentation overlaps with the ground truth region by at least 50
\%. Unfortunately the comparison of models in this section is not easy due to
the significant lack of data for some models. For example, the state-of-the-art
model in the previous section (\gls{cmpc}) only presents accuracy data for the
split of \code{val} in RefCOCO. Likewise, the state-of-the-art model in
RefCOCO+ (shown in bold) does not present data for the RefCOCO dataset, which
makes comparison considerably difficult.

\begin{table}[p]
  \centering
  \caption[Accuracy or Prec@0.5 model comparison]{Accuracy or Prec@0.5 model
    comparison. For each of the models the accuracy percentage or Prec@0.5 is
    shown for the splits \code{val/testA/testB} in the datasets RefCOCO and
    RefCOCO+. The state of the art in each category is shown in bold. Full
    names for model acronyms can be found in section \textsl{Model Acronyms} on
    page~\pageref{sec:ac-model}. Table created by the author using data from
    second column references.}\label{tab:accuracy}
  \rowcolors{5}{rowColor}{}
  \begin{tabular}{lc*6c}
    \toprule
    & & \multicolumn{3}{c}{\textbf{RefCOCO}} & \multicolumn{3}{c}{\textbf{RefCOCO+}} \\
    \cmidrule(lr){3-5}\cmidrule(lr){6-8}
    \textbf{Method}  & \textbf{Paper}                                               & \code{val}     & \code{testA}   & \code{testB}   & \code{val}     & \code{testA}   & \code{testB}   \\
    \midrule
    \acs{brinet}     & \cite{hu20:bi_direc_relat_infer_networ}                      & 71.83          & 75.09          & 68.38          & -              & -              & -              \\
    \acs{cac}        & \cite{chen19:refer_expres_objec_segmen_caption_aware_consis} & 77.08          & 80.34          & 70.62          & -              & -              & -              \\
    \acs{cmatterase} & \cite{liu19:improv_refer_expres_groun_cross_atten_erasin}    & \textbf{78.35} & \textbf{83.14} & \textbf{71.32} & 68.09          & 73.65          & 58.03          \\
    \acs{cmpc}       & \cite{huang20:refer_image_segmen_cross_modal_progr_compr}    & 71.27          & -              & -              & -              & -              & -              \\
    \acs{cmsa}       & \cite{ye21:refer_segmen_images_videos_cross}                 & 69.24          & 73.87          & 64.55          & 45.48          & 51.41          & 37.57          \\
    \acs{faoa}       & \cite{yang19:fast_accur_one_stage_approac_visual_groun}      & 71.15          & 74.88          & 66.32          & 56.88          & 61.89          & 49.46          \\
    \acs{lgran}      & \cite{wang19:neigh}                                          & -              & 76.6           & 66.4           & -              & 64.00          & 53.40          \\
    \acs{mattnet}    & \cite{yu18:mattn}                                            & 76.65          & 81.14          & 69.99          & 65.33          & 71.62          & 56.02          \\
    \acs{mmi}        & \cite{mao16:gener}                                           & -              & 64.90          & 54.51          & -              & 54.03          & 42.81          \\
    \acs{nmtree}     & \cite{liu19:learn_assem_neural_modul_tree}                   & 74.71          & 79.71          & 68.93          & 65.06          & 70.24          & 56.15          \\
    \acs{refvos}     & \cite{bellver20:refvos}                                      &                &                &                &                &                &                \\
    \acs{rmi}        & \cite{liu17:recur_multim_inter_refer_image_segmen}           & 42.99          & 42.99          & 44.99          & 20.52          & 21.22          & 20.78          \\
    \acs{rrn}        & \cite{li18:refer_image_segmen_recur_refin_networ}            & 61.66          & 64.13          & 59.35          & 37.32          & 40.80          & 32.42          \\
    \acs{step}       & \cite{chen19:see_throug_text_group_refer_image_segmen}       & 70.15          & -              & -              & -              & -              & -              \\
    \acs{vilbert}    & \cite{lu19:vilber}                                           & -              & -              & -              & \textbf{72.34} & \textbf{78.52} & \textbf{62.61} \\
    \bottomrule
  \end{tabular}\\[1.25ex]
  {\small\textbf{Note}. Models arranged in alphabetical order.}
\end{table}

Despite this, as can be seen, in the RefCOCO dataset the model that presents a
superior performance is that of \gls{cmatterase}, created by
\myCite{liu19:improv_refer_expres_groun_cross_atten_erasin}. This model is
mainly based on a training strategy based on the idea of eliminating the parts
most used by the model from the linguistic or visual part, so that it is forced
to learn more complex structures. It must be taken into account that despite
this model being the one that obtains the highest accuracy values, it could
possibly be outperformed by the model \acs{vilbert} (of which, unfortunately,
no evaluation data is available for this dataset).

In the RefCOCO+ dataset, the model \gls{vilbert}, created by
\myCite{lu19:vilber}, is proclaimed as the winner and, therefore, state of the
art. Broadly speaking, it consists of reusing the well-known and popular
architecture of \gls{bert} to a multimodal model with visual and textual inputs
that interact with each other using co-attentional transformer layers. It is a
very interesting approach, not only for this specific task, but also for the
field of multimodal learning in general and this is what its authors express,

\begin{quoteBox}
  Our work represents a shift away from learning groundings between vision and
  language only as part of task training and towards treating visual grounding
  as a pretrainable and transferable capability. \tcblower
  \quotecite{lu19:vilber}
\end{quoteBox}

TODO. Comment here how RefVOS relates to the rest.

The standardized accuracy metric is Precission at 0.5 (Prec@0.5), now this
could be done with different thresholds (0.6, 0.7, 0.8, etc.) and the accuracy
should decrease as this value increases, since each instead we look for a more
perfect segmentation to consider it as a positive sample. A study for a
different threshold will not be presented here for the simple reason of lack of
data for the models studied: few or none of the publications do a study and
present their precision results for different threshold values.


\section{Qualitative Evaluation}\label{sec:qualitative-eval-res}

This work can also be easily evaluated qualitatively, since the result of the
segmentation can be seen graphically superimposed on the input image. Overall,
the model proposed in this work considerably well at this task with fairly
consistent and accurate results. The reader can go to
\fhref{https://recomprehension.com}{página web} of this project to test for
himself the operation of the model. Here we will show several examples where
different images have been used and \gls{re} very varied. More specifically,
successful results will be shown in \vref{sec:succesful} and also, a study of
examples will be made in which the model fails---or does not achieve a
sufficiently precise segmentation---in \vref{sec:failed}.


\subsection{Study of Successful Samples}\label{sec:succesful}

The model presented in this work behaves successfully before a great variety of
images and \glspl{re}. Various examples have been collected in
\vref{fig:success}, where the result of the segmentation on the image is shown
in blue and \gls{re} used in the upper part of each figure. We can see, for
example, in the first row (\vref{fig:baseball-1,fig:baseball-2,fig:baseball-3})
the same image representing a photograph taken in a baseball game, where
different \glspl{re} have been used successfully to refer uniquely to each of
the three players that appear in the image. The segmentation obtained is
correct and very precise. Furthermore, the players have been referred to in
different ways: \re{player} and \re{man}, and particularized with: reference
between objects (\re{with baseball bat} and \re{with glove}) and relative
positioning (\re{in the left}).

\begin{figure}[p]
  \centering
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Player with baseball bat}\label{fig:baseball-1}
    \includegraphics[width=\textwidth]{Images/Player with baseball bat.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Middle player with glove}\label{fig:baseball-2}
    \includegraphics[width=\textwidth]{Images/Middle player with glove.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Man in the left}\label{fig:baseball-3}
    \includegraphics[width=\textwidth]{Images/Man in the left.jpg}
  \end{subfigure}

  \bigskip
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Donuts with topping}\label{fig:donut-1}
    \includegraphics[width=\textwidth]{Images/Donuts with topping.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{White background donuts}\label{fig:donut-2}
    \includegraphics[width=\textwidth]{Images/White background donut.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{White donut left behind}\label{fig:donut-3}
    \includegraphics[width=\textwidth]{Images/White donut left behind.jpg}
  \end{subfigure}

  \bigskip
  \begin{subfigure}[t]{.3\textwidth}
    \centering
    \caption{Person in blue}\label{fig:bar-1}
    \includegraphics[width=.95\textwidth]{Images/Person in blue.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.3\textwidth}
    \centering
    \caption{Person with watch}\label{fig:bar-2}
    \includegraphics[width=.95\textwidth]{Images/Person with watch.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.3\textwidth}
    \centering
    \caption{Woman}\label{fig:bar-3}
    \includegraphics[width=.95\textwidth]{Images/Woman.jpg}
  \end{subfigure}

  \bigskip
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Man in white shirt}\label{fig:man-shirt}
    \includegraphics[width=\textwidth]{Images/Man in white shirt.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Bike}\label{fig:bike}
    \includegraphics[width=\textwidth]{Images/Bike.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Train}\label{fig:train}
    \includegraphics[width=\textwidth]{Images/Train.jpg}
  \end{subfigure}
  \caption[Model evaluation succesful examples]{Model evaluation succesful
    examples. Tested with different images and with varied \glspl{re}. Figures created by the author (all). View images in color to better appreciate segmentation.}\label{fig:success}
\end{figure}

In the second row of this same figure
(\vref{fig:donut-1,fig:donut-2,fig:donut-3}) you can see in this case a tray
with donuts and in which \glspl{re} considered more complex have been used,
such as \re{with topping}. Also, in \vref{fig:donut-2} you can see an example
of multiple selection of objects, solved successfully.

\begin{remarkBox}
  This case of multiple object segmentation is not really part of the scope of
  this work, despite being successfully solved by the model. It must be
  remembered that one of the hypotheses of \gls{rec} is that \gls{re} must be
  descriptive enough to refer to one---and only one---object. That is, it is
  assumed that the referenced object is unique.
\end{remarkBox}

The third and last row of this same figure collect more examples with different
correctly segmented images
(\vref{fig:bar-1,fig:bar-2,fig:bar-3,fig:man-shirt,fig:bike,fig:train}). Here
they have been used as a sample \gls{re} towards different objects (e.g.,
\re{bike}, \re{train}). An example of highly complex segmentation has also been
shown (\re{person with watch}), in which the model works correctly despite not
being too precise. In the case of figure \vref{fig:bike} it is worth
highlighting as a positive point a very precise segmentation given the
geometric complexity of the object. The segmentation of the object \re{train}
in the \vref{fig:train} as a whole is also satisfactory (the segmentation in
this case is not trivial due to the length of the object and the small size
within the image of the final part of it).

Finally, within this qualitative evaluation section, we have wanted to add
examples of different \glspl{re} applied to the same image in
\vref{fig:cats}. Here we have started from the original image
(\vref{fig:cats-orig}) and the model has been executed with different
\glspl{re}, starting from the simplest to other more complex ones and in which
extra reference elements have been added. As a novel contribution in this
figure is the use of a \gls{re} in which differentiates between instances using
a comparison: \re{blackest cat}, in \vref{fig:cat-blackest} and of a \gls{re}
taking into account the position of the referred object (\re{with one leg
  extended}).

\begin{figure}[p]
  \centering
  \begin{subfigure}[t]{.3\textwidth}
    \centering
    \caption{Original image}\label{fig:cats-orig}
    \includegraphics[width=.96\textwidth]{Images/Cats.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.3\textwidth}
    \centering
    \caption{Brown cat}
    \includegraphics[width=.96\textwidth]{Images/Brown cat.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.3\textwidth}
    \centering
    \caption{White cat right}
    \includegraphics[width=.96\textwidth]{Images/White cat right.jpg}
  \end{subfigure}

  \bigskip
  \begin{subfigure}[t]{.3\textwidth}
    \centering
    \caption{Blackest cat}\label{fig:cat-blackest}
    \includegraphics[width=.96\textwidth]{Images/Blackest cat.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.3\textwidth}
    \centering
    \caption{Cat closer to the camera}
    \includegraphics[width=.96\textwidth]{Images/Cat closer to the camera.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.3\textwidth}
    \centering
    \caption{Show me the blackest cat}
    \includegraphics[width=.96\textwidth]{Images/Show me the blackest cat.jpg}
  \end{subfigure}

  \bigskip
  \begin{subfigure}[t]{.3\textwidth}
    \centering
    \caption{Show me the blackest cat on the bed}
    \includegraphics[width=.96\textwidth]{Images/Show me the blackest cat on the bed.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.3\textwidth}
    \centering
    \caption{Show me the blackest cat on bed with one leg extended}
    \includegraphics[width=.96\textwidth]{Images/Show me the blackest cat on bed with one leg extended.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.3\textwidth}
    \centering
    \caption{Select prettiest cat among them}\label{fig:cat-prettiest}
    \includegraphics[width=.96\textwidth]{Images/Prettiest cat.jpg}
  \end{subfigure}
  \caption[Comprehension results in an image with cats]{Comprehension results
    in an image with cats laying on a bed. The same image is tested with different \glspl{re}. Figures created by the author (all). View images in color to better appreciate segmentation.}\label{fig:cats}
\end{figure}

A subjective \gls{re} (in which the referred object is not properly selected)
has also been added, as a curiosity (see \vref{fig:cat-prettiest}). In this
case, it is the same model that is using the information extracted from the
dataset to determine something as complex and subjective as aesthetics or
beauty. Of course, this example is also outside the scope of this paper.


\subsection{Study of Failed Samples}\label{sec:failed}

After looking at all these samples of successful comprehension in the previous
section, we might think that the model is perfect. Now, unfortunately, this is
not the case. Different problems appear depending on the image and the \gls{re}
used, either by completely failing, or by performing an insufficiently precise
segmentation or because \gls{re} is wrongly specified. Different examples of
failures with this model are presented in \vref{fig:failure}.

\begin{figure}[p]
  \centering
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Left tennis racket}\label{fig:racket}
    \includegraphics[width=\textwidth]{Images/Left tennis racket.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Blond boy looking back}\label{fig:blond}
    \includegraphics[width=\textwidth]{Images/Blond boy looking back.jpg}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Banana}\label{fig:banana}
    \includegraphics[width=\textwidth]{Images/Banana.jpg}
  \end{subfigure}

  \bigskip
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Woman holding hair dryer}\label{fig:dryer-1}
    \includegraphics[width=.9\textwidth]{Images/Woman holding hair dryer.jpg}
  \end{subfigure}\hspace{.1\textwidth}
  \begin{subfigure}[t]{.32\textwidth}
    \centering
    \caption{Hair dryer}\label{fig:dryer-2}
    \includegraphics[width=.9\textwidth]{Images/Hair dryer.jpg}
  \end{subfigure}

  \bigskip
  \begin{subfigure}[t]{.4\textwidth}
    \centering
    \caption{Statue}\label{fig:statue-1}
    \includegraphics[width=\textwidth]{Images/Statue.jpg}
  \end{subfigure}\hspace{.06\textwidth}
  \begin{subfigure}[t]{.4\textwidth}
    \centering
    \caption{Statue of a bird}\label{fig:statue-2}
    \includegraphics[width=\textwidth]{Images/Statue of a bird.jpg}
  \end{subfigure}

  \bigskip
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \caption{Tennis match referee}\label{fig:referee-1}
    \includegraphics[width=.9\textwidth]{Images/Tennis match referee.jpg}
  \end{subfigure}\hspace{.05\textwidth}
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \caption{Tennis match referee sitting behind}\label{fig:referee-2}
    \includegraphics[width=.9\textwidth]{Images/Tennis match referee sitting behind.jpg}
  \end{subfigure}
  \caption[Failed comprehension examples]{Failed comprehension
    examples. \gls{rec} task fails due to model errors, \gls{re} specification errors and lack of vocabulary. Figures created by the author (all). View images in color to better appreciate segmentation.}\label{fig:failure}
\end{figure}

Among them we show examples of imprecise segmentation
(\vref{fig:racket,fig:blond}). In these cases the model works approximately
correctly locating the referred object, but it is not capable of generating a
sufficiently precise segmentation to be considered a successful sample. In
other cases, furthermore, the location of the object is not even carried out
correctly (\vref{fig:statue-1,fig:statue-2}), where it is quite possibly due to
the ``ignorance'' of the specialized vocabulary model (as is the case of
\re{statue}). This is also the case with object \re{hair dryer} in
\vref{fig:dryer-1}.\footnote{The claim that the model ``ignores'' this
  vocabulary is conjecture by the author. Another feasible possibility in this
  case is that, due to the reduced size of the referred object, the
  segmentation is not correct.} Another example of incorrect segmentation is
the one shown in \vref{fig:banana}, but here the error is due to a bad \gls{re}
wrong specified (there are multiple instances of the object being referred to).

Sometimes also, correct segmentations happen but ``by chance''. This is the
case of \vref{fig:dryer-1}: at first we can believe that the segmentation is
correct and that the model is working correctly, but it really is not. Why? The
model is not able to understand the vocabulary of \re{hair dryer} (see
\vref{fig:dryer-2}), so it is not really reasoning. She is simply using the
word \re{woman} that she understands well and is making a conjecture as to
which of the two women the \gls{re} is referring to.

Finally, in \vref{fig:referee-1,fig:referee-2}, an example is shown in which
\gls{re} at first is not enough (possibly because the model is not capable of
understanding the specialized word of \re{referee}), but when presenting a
\gls{re} more specifically yes that the segmentation is carried out
correctly. This leads one to think that---on a practical application level---it
might be useful not to always segment. In other words, it would be useful to
implement an extra ``trust'' functionality in the segmentation performed. In
this way, the model could ``warn'' if the confidence level is not high
enough. In colloquial words and using the example described: if we wanted to
segment the match referee and start with \gls{re} \re{tennis match referee},
the model could warn us that the confidence it has of performing a correct
segmentation is not high enough, so that we can extend this \gls{re} to provide
more details to the model (\re{tennis match referee sitting behind}) and that
it can segment the referred object more easily.
