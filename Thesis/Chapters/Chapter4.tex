% -*- TeX-master: "../Thesis.tex" -*-


\chapter{Models}\label{cha:models}

\epigraphhead[75]{
  \epigraph{\itshape{}All models are wrong, but some are useful.}
  {---George Edward \textsc{Pelham Box}}
}


\lettrine{M}{odeling consists of creating} a mathematical model that represents
a complex situation as closely as possible. In this work, two different models
will be used: one of them to carry out the work of \gls{rec} starting from a
\gls{re} in the form of text (see\ \vref{sec:rec-model}) and another model for
speech recognition (see\ \vref{sec:speech}), from so that you can also work with
spoken natural language.



\section{\glsentrylong{rec}}\label{sec:rec-model}

For the task of \gls{rec} it will be necessary to find/create a neural model
that solves it. To do this, we will start from a base architecture (a model to
start with) (see\ \vref{sec:base-arch}) and from there variations will be
proposed (both in the model and in the way of training it) in\
\vref{sec:model-iterations}. That is, starting from the base model, an
iterative process of improvement will be carried out.


\subsection{Base Architecture}\label{sec:base-arch}

In\ \vref{fig:model} a graphical representation of the model used as base
architecture is shown. It has two differentiated parts with which the features
are extracted from the visual part and from the language. These features are
then combined to achieve a \emph{multimodal} embedding and thus be able to
generate the segmentation.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{Images/RefVOS.png}
  \caption[\glsentryshort{refvos} model architecture]{\gls{refvos} model
    architecture. It is possible to observe the differentiated models of vision
    and language that are then combined to obtain the multimodal
    characteristics. From \figcite{bellver20:refvos}.}%
  \label{fig:model}
\end{figure}

This model, created by \myCite{bellver20:refvos}, will constitute our starting
base architecture. Next, the image encoder (which is based on atrous
convolutions), the language encoder (which uses transformers) and the
multimodal embedding will be studied separately.

\subsubsection{Image Encoder}\index{Image encoder}

To extract the features of the images a state-of-the-art model called DeepLab
is used, which is a neural network created by \myCite{chen17:rethin} and based
on atrous convolutions (see\ \vref{fig:atrous}). It is a \gls{cnn} used for
semantic segmentation.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Atrous.png}
  \caption[Atrous convolutions examples]{Atrous convolutions examples with
    filter size \(3 \times 3\). The \code{rate} parameter controls the model's
    field-of-view. Standard convolution operation corresponds to an atrous
    convolution with a rate of 1. From \figcite{chen17:rethin}.}%
  \label{fig:atrous}
\end{figure}

One of the advantages of this model compared to standard convolutional neural
models is that it adapts very well to objects at different scales, without the
need for pooling operations. Thus, the creators of this model define atrous
convolutions (also known as dilated convolutions).

\begin{quoteBox}
  Atrous convolution allows us to extract denser feature maps by removing the
  downsampling operations from the last few layers and upsampling the
  corresponding filter kernels, equivalent to inserting holes (``trous'' in
  French) between filter weights.
  \tcblower\quotecite{chen17:rethin}
\end{quoteBox}

In the model used, the well-known ResNet101 network (created by
\myCite{he16:deep}) and a \code{output\_stride}\footnote{The
  \code{output\_stride} is the ratio of input images partial resolution to
  final output resolution will be used as backbone. Setting this ratio to
  smaller values allow the model to extract denser feature responses (view
  section~3.1 from~\cite{chen17:rethin}).} of 8. Likewise, \((12, 24, 36)\)
will be used as \code{rates} of the convolutions in \gls{aspp}. These pyramids
are part of the DeepLab model and consist of performing atrous convolutions in
parallel (with different rates). In this way, by using different rates, it is
possible to capture information from different scales at the same time.

\subsubsection{Language Encoder}\index{Language encoder}

In the case of the language encoder, different possibilities could be
considered, including using a \gls{rnn} or mainly using a transformer. In this
base architecture presented, \gls{refvos} achieves more promising results by
making use of transformers. Specifically, a transformer created by
\myCite{devlin19:bert} and called \gls{bert} is used.

\gls{bert} is a multi-layer bidirectional Transformer encoder (see
\vref{sec:transformers}) that removes the unidirectional constraint present in
previous models related to language representation. It uses \gls{mlm}, i.e.,
randomly masks some tokens from the input and tries to predict the original
token of masked word (just relying on its context). This allows the model to
learn from both left and right context.\footnote{The model \gls{bert} also uses
  the task of \gls{nsp} as an objective training function (see Task~\# 2 in
  section~3.1 from~\cite{devlin19:bert}).}

To integrate \gls{bert} within the model, it is necessary to convert each of
the \glspl{re} to tokens and add two special tokens: \code{[CLS]} and
\code{[SEP]} at the beginning and end of the sentence respectively. This model
will then produce embeddings of dimension 768 for each of the input tokens. The
final hidden vector corresponding to the first input token (\code{[CLS]}) as
the aggregate representation of the \gls{re} (view section~4.1
from~\cite{devlin19:bert}) will be taken.

\subsubsection{Multimodal Embedding}\index{Multimodal!embedding}

Once we have the encoded \gls{re} and the map of visual features from the
convolutional network, it is necessary to obtain a multimodal embedding, which
combines the information from both encoders. The output from the visual encoder
is a tensor of depth 256 and the output from the language encoder is a
768-dimensional vector (see\ \vref{fig:model}).

To combine these two outputs, the encoded \gls{re} of the vector of dimension
768 is converted to one of dimension 256 (which coincides with the depth of the
visual features). These two tensors are then multiplied element-wise to obtain
the multimodal embedding. Finally, a convolutional layer is used to pass a last
tensor with two classes, which separate the \emph{background} from the
\emph{object} that is being referred.


\subsection{Model Iterations}\label{sec:model-iterations}

Now, in this section, what we will try to do is understand the operation of the
base architecture explained and proceed to carry out an iterative process of
improvement of said model. For this we will attack the fundamental constituent
parts of any neural model: change the architecture or change the way of
training. As we know, regarding the architecture, in this case, we have three
different parts (the image encoder, the language encoder and the multimodal
embedding). And, regarding the training of the model, different parts can also
be distinguished: loss function, criteria to stop training, optimization
technique, use of pre-trained parameters, etc.

\subsubsection{Loss Functions}\index{Loss function}

Originally the function used for training is that of \gls{ce}, however, for the
segmentation task (specifically for binary segmentation as is this case) there
are many more (see\ \vref{sec:loss-functions}). Within this entire list there
are many of them that are variations precisely of \gls{ce}. Training a model
with these variations is of little significance in terms of results. Here,
comment that no significant results are found, since, as it is a pre-trained
model and the loss functions are quite similar, there is no progress in
training, reaching an area where the gradients are practically zero.

Other loss functions that could be more interesting are those based on overlap
measures. Among them we highlight that of \gls{dil}, which has been used to
train the model based on pre-trained parameters. This loss function, which has
already been defined in\ \vref{sec:loss-functions}, can be implemented in
PyTorch as follows.

\codeIn{python}{Code/diceLoss.py}

Using this loss function the evaluation of the training of the model is
collected in\ \vref{fig:dl-train}. The training process is decided to stop at
the moment when the loss function is lowest in the split \code{val}. It is
important to observe the magnitudes on the vertical axis of the graph, since
the variations are insignificant. It might seem at first glance that the
evolution of the loss function is quite satisfactory due to the shape of the
graph, but it must be taken into account that the scale of the vertical axis is
extremely small, so the variations of the model parameters are really
insignificant.

\begin{figure}[p]
  \centering
  \tikzset{external/export next=false}
  \begin{tikzpicture}
    \begin{axis}[trainPlot, ylabel={Dice Loss}]
      \addplot table[col sep=comma]{Data/train.csv};
      \addplot table[col sep=comma]{Data/val.csv};
      \draw[dashed] (21, 0.39) -- node[right]{Stop} (21, 0.401);
      \legend{\code{train}, \code{val}}
    \end{axis}
  \end{tikzpicture}
  \caption[Training graph with \glsentrylong{dil}]{Training graph with
    \glsentrylong{dil}. The evolution of the loss function for the different
    epochs for the \code{train/val} splits is shown. We are left with the epoch
    that presents a lower value of the loss function in the split
    \code{val}. Figure created by the author.}%
  \label{fig:dl-train}
\end{figure}

In order to better understand this evolution of the model, the overall
\gls{iou} on this same training process has also been plotted (see \
\vref{fig:dl-val}). In it we can see a very important presence of noise,
variations really without any direction and without presenting a clear
trend. It is also important to highlight in this case that the scale of the
vertical axis is quite small: this same graph on a vertical axis in the range
\((0, 1)\) would be practically flat.

\begin{figure}[p]
  \centering
  \tikzset{external/export next=false}
  \begin{tikzpicture}
    \begin{axis}[trainPlot, ylabel={Overall \acs{iou}}]
      \addplot table[col sep=comma]{Data/other.csv};
      \legend{\code{val}}
    \end{axis}
  \end{tikzpicture}
  \caption[Gráfica de overall \glsentryshort{iou} con
  \glsentrylong{dil}]{Gráfica de overall \glsentryshort{iou} con
    \glsentrylong{dil}. In this case, under the same evolution of the model
    optimizing the \gls{dil} function, it is shown how the overall \gls{iou}
    evolves. Figure created by the author.}%
  \label{fig:dl-val}
\end{figure}

Taking into account that the best epoch corresponds to the number 21 (see \
\vref{fig:dl-train}), we would obtain a value of overall \gls{iou} in this
lower than that obtained by the initial model (see\ \vref{tab:ablation})
. Yes, it is true that we could decide to take the values of the parameters at
another time taking into account the peak of overall \gls{iou} at time number
16. Now, this graph actually presents very small fluctuations that are due to
simple noise produced by the slight variation of the parameters when training
and does not represent a significant improvement in the model.

\subsubsection{Multimodal Embedding}\index{Multimodal!embedding}

Regarding multimodal embedding, there are different possibilities that can be
carried out to obtain joint information both in terms of vision and
language. Among them, those studied by the model \gls{refvos} are those of
addiction, multiplication and concatenation. That is, we can join the visual
features and the language features with an element-wise operation. These
different strategies are shown in\ \vref{tab:ablation} evaluated using the
overall metric \gls{iou} in the RefCOCO dataset in the splits of
\code{val/testA/testB}. As we can see, the fusion strategy that obtains a
superior performance is that of \emph{multiplication}, so it will be the one
used in the future.

\begin{table}[ht]
  \centering
  \caption[Performance of fusion strategies in RefCOCO dataset]{Performance of
    fusion strategies in RefCOCO dataset. The overall \gls{iou} for each fusion
    strategy for visual and language features is shown for the
    \code{val/testA/testB} splits in the RefCOCO dataset. Table created by the
    author.}%
  \label{tab:ablation}
  \rowcolors{3}{}{rowColor}
  \begin{tabular}{l*3c}
    \toprule
    & \multicolumn{3}{c}{\textbf{RefCOCO}} \\
    \cmidrule(lr){2-4}
    \textbf{Strategy} & \code{val}     & \code{testA}   & \code{testB}   \\
    \midrule
    Addition          & 56.60          & 60.87          & 51.29          \\
    Multiplication    & \textbf{59.45} & \textbf{63.19} & \textbf{54.17} \\
    Concatenation     & 55.12          & 58.88          & 49.59          \\
    Projection        & Infeasible     & Infeasible     & Infeasible     \\
    Projection v2     & 21.08          & -              & -              \\
    \bottomrule
  \end{tabular}
\end{table}

These three multimodal feature fusion strategies have in common that they are
presented in an ``arbitrary'' way, so it was studied as an improvement that
this multimodal fusion was learned by the model and not imposed
externally. That is, it was tried that the fusion of features was learned by
the model using data. To do this, using the notation \(V\) for the visual
features tensor and \(L\) for the language features tensor, we have that their
dimensions are \(w \times h \times d\) and \(d\) respectively (\(w\) and \(h\)
represent the width and the height of the visual features respectively). Then,
the idea of following an approach similar to the one proposed by
\myCite{faghri18:vse} arises, where linear porjections are defined from the
features to an embedding space. To do this, it is necessary to reshape the
visual features tensor and think of it as a vector \(V \in \R^{w \times h
\times d}\), (we will define to simplify the notation \(D := w \times h \times
d\), so we will write \(V \in \R^D\)). And, we will also use the vector of
language features \(L \in \R^{d}\). In this way, it is now possible to define
applications to map features to a vector space of common dimension \(J\). That
is, the application \(\phi\) is defined,
\begin{equation}
  \begin{aligned}
    \phi \colon \R^{D} \times \R^{D \times J} &\longrightarrow \R^J \\ (V, W_v)
    &\longmapsto \phi(V, W_v) := W_v V,
  \end{aligned}
\end{equation}
which maps the visual features \(V\) to the joint space \(\R^J\) via the linear
projection defined by the matrix of visual parameters \(W_v\). In the same way,
the application \(\psi\) is defined,
\begin{equation}
  \begin{aligned}
    \psi \colon \R^{d} \times \R^{J \times d} &\longrightarrow \R^J \\ (L, W_l)
    &\longmapsto \psi(L, W_l) := W_l L,
  \end{aligned}
\end{equation}
which maps the language features \(L\) to the joint space \(\R^J\) via the
linear projection defined by the language parameter matrix \(W_l\).

Some decisions had to be made, including the decision of the size of the joint
space \(D\). Taking into account that the vector of language features had
dimension \(d\) and that it would not be useful to propose a reduction in
dimensionality or increase it (since the model already has enough complexity
and free trainable parameters), it was decided to fix that it would not be used
of parameters and \(\psi = \text{Id}\). Therefore, it only remained to add the
function \(\phi\), which was completely defined by the matrix of visual weights
\(W_v\). Now, this initial idea of projection that seemed very useful, was
found to be infeasible due to the enormous size of this matrix and the
impossibility of training this huge number of parameters due to limited
computational resources. We must take into account that \(W_v \in \R^{D \times
J}\), where \(D = w \times h \times d\), and we have chosen \(J = d = 256\) for
\(\psi = \text{Id}\). In other words, the number of parameters in \(W_v\) is on
the order of billions, which makes it infeasible.

Once this problem has been detected, another similar approach is proposed, but
drastically reducing the number of parameters. To do this, it is proposed to
reuse parameters in the depth of the image characteristics (and continue using
\(\psi = \text{Id}\)). That is, use an array for each of the slices in the
depth of the visual features. This is for each slice \(V^i\) in the depth of
the visual features tensor, a weight matrix \(W_v\) is used (the same for all
slices), so that the corresponding embedding for slice \(i\) is defined as
follows \(\tilde{V}^i = W_v V^i\). That is, in this case each of the visual
features matrices is not extended as a vector, but rather is multiplied. To
keep the original dimensions, use \(W_v \in \R^{w \times h}\). In this way, it
was possible to send the visual features tensor \(V\) to another modified
visual features tensor \(\hat{V}\) through a linear projection with trainable
parameters. Later, the multiplication of features was used again to now achieve
the fusion with the language (multiplication is used because it is what had
been shown as more efficient before). Now, what was obtained from these
variations to the model? Pretty bad results. Specifically, in the split of
\code{val} in the RefCOCO dataset an overall \gls{iou} value of 21.08 was
obtained, which is significantly lower than the values obtained by other
techniques, so it was not decided to use this technique. After a reasoning of
the method of the method used, the causes that cause problems in this case are:
\begin{itemize}
  \item \textbf{Loss of spatial information.} The first and most important
  thing is to highlight that this technique described here causes the loss of
  spatial information that comes from the beginning from the image (and this is
  preserved by the convolution operations). That is, when performing the
  multiplication operation between matrices we are at the end combining
  ``pixels'' from different parts of the image without too much success.
  \item \textbf{Meaningless transformation to visual information.} Another
  problem that arises in the application of this technique is that it does not
  contribute significantly to the multimodal fusion between
  characteristics. Rather, it is an addition to the convolutional network used,
  which is not useful, since the model used is a well-known state-of-the-art
  model. In other words, we are adding one more layer without much sense to an
  existing model whose topology has already been precisely selected by its
  creators.
  \item \textbf{Adding unnecessary non-pretrained parameters.} Besides that, we
  have the problem that a significant number of parameters are being added
  (trainable, yes), but they are not pre-trained. That is, we are giving the
  model the ability to more easily overfit with these new parameters that have
  been added.
\end{itemize}

\subsubsection{Training Proccess}

Another possibility to modify is the model training process. This consists of
modifying the optimization algorithm so that the development of the model can
be improved. In\ \vref{sec:trainning} different possibilities have already been
discussed. In the original case of \gls{refvos}, the optimizer \gls{sgd} is
used with Nesterov momentum\index{Nesterov momentum} \num{0.9} and
\code{weight\_decay} of \num{1e-6}, this corresponds to the regularization
\(L_2\) (see\ \vref{sec:regularization}). Another possibility to consider would
be Adam's optimizer\index{Adam optimizer}, for example with the
hyperparameters\index{Hyperparameter} typical of \(\beta_1 = 0.9\), \(\beta_2 =
0.999\). However, it has not been recommended to change this optimization
process, since it is not usual for it to report significant improvements.



\section{Speech Recognition}%
\label{sec:speech}\index{Speech}\index{Speech!recognition}

Speech recognition, also known as \gls{asr} and \gls{stt} is a field of
\gls{cs} that deals with recognition of spoken language into text. For us it
will be useful because it will allow us to segment objects in images using the
voice, that is, we will be able to solve the problem of \gls{rec} using spoken
language.

For this task, we will use a pre-trained neural model to convert from
\gls{stt}. The model, created by
\myCite{veysov20:towar_imagen_momen_speec_text}, is called Silero (see\
\vref{fig:silero}), and it allows converting from mono audio to text in
different languages: English, German, Spanish and Ukrainian.

\begin{figure}[p]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Silero.jpg}
  \caption[Silero \glsentrylong{stt} model architecture]{Silero \acf{stt} model
    architecture. View from top to bottom: input is a mono audio file with
    speech and the output is the text representing the input. From
    \figcite{veysov20:towar_imagen_momen_speec_text}.}%
  \label{fig:silero}
\end{figure}
