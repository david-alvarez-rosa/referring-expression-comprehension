% -*- TeX-master: "../Thesis.tex" -*-


\chapter{Models} \label{cha:model}

\epigraphhead[75]{
  \epigraph{\itshape All models are wrong, but some are useful.}
  {---\scshape George E. P. Box}
}


\lettrine{M}{odelizar} consiste en crear un modelo matemático que represente de
la manera más fiel posible una situación compleja. En este trabajo se usarán
dos modelos diferentes: uno de ellos para realizar el trabajo de \gls{rec}
partiendo de una \gls{re} en forma de texto (ver \vref{sec:rec-model}) y otro
modelo para speech recognition (ver \vref{sec:speech}), de manera que también
se pueda trabajar con lenguaje natural hablado.


\section{\glsentrylong{rec}} \label{sec:rec-model}

Para la obtención de un modelo

El primer modelo, se basará en una arquitectura base (discutida en la
\vref{sec:base-arch}). Para esta arquitectura se mostrarán diferentes
posibilidades de trainning (en la \vref{sec:model-trainning}), con las
cuales se iterará en diferentes posibilidades del modelo (en la
\vref{sec:model-iterations}).

\subsection{Base Architecture} \label{sec:base-arch}

En la \vref{fig:model} se muestra una representación gráfica del modelo
usado. Tiene dos partes diferenciadas con las cuales se produce la extración de
features de la parte visual y del lenguaje. Estas features son después
combiadas para conseguir un embedding \emph{multimodal} y de esta manera ser
capaz de generar la segmentación.

\begin{figure}[htb]
  \centering
  \includegraphics[width=\textwidth]{Images/RefVOS.png}
  \caption[RefVOS model architecture]{RefVOS model architecture. Se pueden
    observar los modelos diferenciados de visión y de lenguaje que después son
    combinados para obtener las características multimodales.}
  \label{fig:model}
  \source{From~\cite{bellver20:refvos}}
\end{figure}

Este modelo, creado por \myCite{bellver20:refvos}, constituirá nuestra
arquitectura base de partida. A continuación se estudiará por separado el image
encoder (que se base en atrous convolutions), el language encoder (que usa
transformers) y el multimodal embedding.

\subsubsection{Image Encoder}
Para extraer las features de las imágenes se usa un modelo \gls{sota} llamado
DeepLab, que es una red neuronal creada por \myCite{chen17:rethin} y basada en
atrous convolutions (see \vref{fig:atrous}). Es un \gls{cnn} usada para
segmentación semántica.

\begin{figure}[htb]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Atrous.png}
  \caption[Atrous convolutions examples]{Atrous convolutions examples with
    filter size \(3 \times 3\). The \code{rate} parameter controls the model's
    field-of-view. Standard convolution operation corresponds to an atrous
    convolution with a rate of 1.}
  \label{fig:atrous}
  \source{From~\cite{chen17:rethin}}
\end{figure}

Una de las ventajas de este modelo frente a modelos neuronales convoluciones
estándar es que se adapta muy bien a objetos en diferentes escalas, sin
necesidad de operaciones de pooling. Así, definen los creadores de este modelo
las atrous convolutions (también conocidas como dilated convolutions).

\begin{quoteBox}
  Atrous convolution allows us to extract denser feature maps by removing the
  downsampling operations from the last few layers and upsampling the
  corresponding filter kernels, equivalent to inserting holes (``trous'' in
  French) between filter weights.
  \tcblower\quoteAuthor{chen17:rethin}
\end{quoteBox}

En el modelo usado, se usará como backbone la conocida red ResNet101 (creada
por \myCite{he16:deep}) y un \code{output\_stride} de 8.\footnote{The
  \code{output\_stride} is the ratio of input imagespatial resolution to final
  output resolution. Setting this ratio to smaller values allow the model to
  extract denser feature responses (view section~3.1
  from~\cite{chen17:rethin}).} Así mismo se usará \((12, 24, 36)\) como
\code{rates} de las convoluciones en la \gls{aspp}. Esta pirámides son parte
del modelo DeepLab y consisten en realizar atrous convolutiones en paralelo
(con diferentes rates). De esta manera, al usar diferentes rates, se consigue
capturar información de diferentes escalas al mismo tiempo.

\subsubsection{Language Encoder}
En el caso del language encoder se podrían considerar diferentes posibilidades,
entre ellas usar una \gls{rnn} o usar un transformer principalmente. En esta
arquitectura base que se presenta, RefVOS consigue resultados más prometedores
haciendo uso de transformadores. Concretamente se usa un transformer creado por
\myCite{devlin19:bert} y llamado \gls{bert}.

\gls{bert} is a multi-layer bidirectional Transformer encoder (see
\vref{sec:transformers}) that removes the unidirectionality constraint present
in previous models related to language representation. It uses \gls{mlm}, i.e.,
randomly masks some tokens from the input and tries to predict the original
token of masked word (just relying on its context). This allows the model to
learn from both left and right context.\footnote{El modelo \gls{bert} también
  usa la tarea de \gls{nsp} como función objetivo de entrenamiento (see
  Task~\#2 in section~3.1 from~\cite{devlin19:bert}).}

Para integrar \gls{bert} dentro del modelo, es necesario convertir a tokens
cada una de las \glspl{re} y añadir dos tokens especiales: \code{[CLS]} and
\code{[SEP]} al principio y al final de la frase respectivamente. Este modelo
producirá, entonces, embeddings de dimensión 768 para cada uno de los tokens de
entrada. Se tomará el final hidden vector corresponding to the first input
token (\code{[CLS]}) as the aggregate representation of the \gls{re} (view
section~4.1 from~\cite{devlin19:bert}).

\subsubsection{Multimodal Embedding}\index{Multimodal!embedding}
Una vez que tenemos the encoded \gls{re} and the map of visuald features from
the convolutional network, es necesario obtener un multimodal embedding, el
cual combine la información de ambos encoders. The output from the visual
encoder is a tensor of depth 256 and the output from the language encoder is a
768-dimensional vector (see \vref{fig:model}).

Para combinar estos dos outputs se convierte la \gls{re} codificada del vector
de dimensión 768 a uno de de dimensión 256 (que coincide con el depth of the
visual features). Estos dos tensores son entonces multiplicados element-wise
para obtener el multimodal embedding. Por último, se usa un convolutional layer
para pasar a un último tensor con dos clases, las cuales separan el
\emph{background} from the \emph{object} that is being referred.


\subsection{Model Iterations}\label{sec:model-iterations}

Model iterations.

% 1. He estado trabajando con el modelo de RefVOS, pero no he conseguido obtener
% ninguna mejora significativa. Hasta ahora lo que he probado ha sido: -- Cambiar
% la manera en la que se unen las neuronas que provienen de la imagen y las que
% provienen del texto (en el modelo se usa multiplicación). He probado a entrenar
% con suma, resta, concatenación y proyectando usando una aplicación lineal
% (multiplicándolas por una matriz y luego combinándolas).  -- Probar diferentes
% funciones de error: "weighted cross entropy", "balanced cross entropy", "focal
% loss". Quiero probar también con otras como: "dice loss", "tversky index", "IoU
% loss"...  Las variaciones que he probado partiendo de parámetros pre-entrenados
% apenas varían el modelo (<1\% mejora), posiblemente porque ya esté el modelo en
% cierto mínimo local de la función de error (al final son todas similares) y los
% gradientes son prácticamente nulos.  Y probando variaciones con parámetros
% no-entrenados no consigo llegar a la precisión de RefVOS (entreno con
% train/validation/test y guardo los parámetros de la época con mejores
% resultados en validación).  Por otro lado, he tratado de crear algún modelo
% desde 0, sin basarme en RefVOS, pero he obtenido resultados nefastos (cercanos
% a la aleatoriedad).


\section{Speech Recognition}%
\label{sec:speech}\index{Speech}\index{Speech!recognition}

Speech recognition, también conocido como \gls{asr} y \gls{stt} es un campo de
\gls{cs} que se ocupa of recognition of spoken language into text. Para
nosotros nos será útil porque nos permitirá segmentar objetos en imágenes
usando la voz, es decir podremos resolver el problema de \gls{rec} usando
spoken language.

Para esta tarea, se usará un pre-trained neuronal model para convertir from
\gls{stt}. El modelo, creado por
\myCite{veysov20:towar_imagen_momen_speec_text}, se llama Silero (see
\vref{fig:silero}), y permite convertir de audio mono a texto en diferentes
idiomas: english, german, spanish and ukraniain.

\begin{figure}[p]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Silero.jpg}
  \caption[Silero \gls*{stt} model architecture]{Silero \gls{stt} model
    architecture. View from top to bottom: input is a mono audio file with
    speech and the output is the text representing the
    input.}\label{fig:silero}
  \source{From~\cite{veysov20:towar_imagen_momen_speec_text}}
\end{figure}
