% -*- TeX-master: "../Thesis.tex" -*-


\chapter{Models} \label{cha:model}

\epigraphhead[75]{
  \epigraph{\itshape All models are wrong, but some are useful.}
  {---\scshape George E. P. Box}
}


\lettrine{M}{odelizar} consiste en crear un modelo matemático que represente de
la manera más fiel posible una situación compleja. En este trabajo se usarán
dos modelos diferentes: uno de ellos para realizar el trabajo de \gls{rec}
partiendo de una \gls{re} en forma de texto (ver \vref{sec:rec-model}) y otro
modelo para speech recognition (ver \vref{sec:speech}), de manera que también
se pueda trabajar con lenguaje natural hablado.


\section{\glsentrylong{rec}} \label{sec:rec-model}

Para la obtención de un modelo

El primer modelo, se basará en una arquitectura base (discutida en la
\vref{sec:base-arch}). Para esta arquitectura se mostrarán diferentes
posibilidades de trainning (en la \vref{sec:model-trainning}), con las
cuales se iterará en diferentes posibilidades del modelo (en la
\vref{sec:model-iterations}).

\subsection{Base Architecture} \label{sec:base-arch}

En la \vref{fig:model} se muestra una representación gráfica del modelo
usado. Tiene dos partes diferenciadas con las cuales se produce la extración de
features de la parte visual y del lenguaje. Estas features son después
combiadas para conseguir un embedding \emph{multimodal} y de esta manera ser
capaz de generar la segmentación.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{Images/RefVOS.png}
  \caption[RefVOS model architecture]{RefVOS model architecture. Se pueden
    observar los modelos diferenciados de visión y de lenguaje que después son
    combinados para obtener las características multimodales.}
  \label{fig:model}
  \source{From \cite{bellver20:refvos}}
\end{figure}

Este modelo, creado por \myCite{bellver20:refvos}, constituirá nuestra
arquitectura base de partida. A continuación se estudiará por separado el image
encoder (que se base en atrous convolutions), el language encoder (que usa
transformers) y el multimodal embedding.

\subsubsection{Image encoder}
Para extraer las features de las imágenes se usa un modelo \gls{sota} llamado
DeepLab, que es una red neuronal creada por \myCite{chen17:rethin} y basada en
atrous convolutions (see \vref{fig:atrous}). Es un \gls{cnn} usada para
segmentación semántica.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Atrous.png}
  \caption[Atrous convolutions examples]{Atrous convolutions examples with
    filter size \(3 \times 3\). The \code{rate} parameter controls the model's
    field-of-view. Standard convolution operation corresponds to an atrous
    convolution with a rate of 1.}
  \label{fig:atrous}
  \source{From \cite{chen17:rethin}}
\end{figure}

Una de las ventajas de este modelo frente a modelos neuronales convoluciones
estándar es que se adapta muy bien a objetos en diferentes escalas, sin
necesidad de operaciones de pooling. Así, definen los creadores de este modelo
las atrous convolutions (también conocidas como dilated convolutions).

\begin{quoteBox}
  Atrous convolution allows us to extract denser feature maps by removing the
  downsampling operations from the last few layers and upsampling the
  corresponding filter kernels, equivalent to inserting holes (``trous'' in
  French) between filter weights.
  \tcblower
  ---\quoteAuthor{chen17:rethin}
\end{quoteBox}

En el modelo usado, se usará como backbone la conocida red ResNet101
\cite{he16:deep} y un \code{output\_stride} de 8.\footnote{The
  \code{output\_stride} is the ratio of input imagespatial resolution to final
  output resolution. Setting this ratio to smaller values allow the model to
  extract denser feature responses (view section~3.1
  from~\cite{chen17:rethin}).} Así mismo se usará \((12, 24, 36)\) como
\code{rates} de las convoluciones en la \gls{aspp}. Esta pirámides son parte
del modelo DeepLab y consisten en realizar atrous convolutiones en paralelo
(con diferentes rates). De esta manera, al usar diferentes rates, se consigue
capturar información de diferentes escalas al mismo tiempo.

\subsubsection{Language encoder}
TODO. Hablar aquí sobre BERT.

\myCite{devlin19:bert}

\subsubsection{Multimodal embedding}
TODO.




\subsection{Model Iterations} \label{sec:model-iterations}

Model iterations.

% 1. He estado trabajando con el modelo de RefVOS, pero no he conseguido obtener
% ninguna mejora significativa. Hasta ahora lo que he probado ha sido: -- Cambiar
% la manera en la que se unen las neuronas que provienen de la imagen y las que
% provienen del texto (en el modelo se usa multiplicación). He probado a entrenar
% con suma, resta, concatenación y proyectando usando una aplicación lineal
% (multiplicándolas por una matriz y luego combinándolas).  -- Probar diferentes
% funciones de error: "weighted cross entropy", "balanced cross entropy", "focal
% loss". Quiero probar también con otras como: "dice loss", "tversky index", "IoU
% loss"...  Las variaciones que he probado partiendo de parámetros pre-entrenados
% apenas varían el modelo (<1\% mejora), posiblemente porque ya esté el modelo en
% cierto mínimo local de la función de error (al final son todas similares) y los
% gradientes son prácticamente nulos.  Y probando variaciones con parámetros
% no-entrenados no consigo llegar a la precisión de RefVOS (entreno con
% train/validation/test y guardo los parámetros de la época con mejores
% resultados en validación).  Por otro lado, he tratado de crear algún modelo
% desde 0, sin basarme en RefVOS, pero he obtenido resultados nefastos (cercanos
% a la aleatoriedad).


\section{Speech Recognition} \label{sec:speech}

También usaremos un modelo neuronal preentrenado para convertir de speech a
texto.
