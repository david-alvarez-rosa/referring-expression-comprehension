% -*- TeX-master: "../Thesis.tex" -*-


\chapter{Models}\label{cha:models}

\epigraphhead[75]{
  \epigraph{\itshape All models are wrong, but some are useful.}
  {---George Edward \textsc{Pelham Box}}
}


\lettrine{M}{odeling consists of creating} a mathematical model that represents
a complex situation as closely as possible. In this work, two different models
will be used: one of them to carry out the work of \gls{rec} starting from a
\gls{re} in the form of text (see \vref{sec:rec-model}) and another model for
speech recognition (see \vref{sec:speech}), from so that you can also work with
spoken natural language.



\section{\glsentrylong{rec}}\label{sec:rec-model}

For the task of \gls{rec} it will be necessary to find/create a neural model
that solves it. To do this, we will start from a base architecture (a model to
start with) (see \vref{sec:base-arch}) and from there variations will be
proposed (both in the model and in the way of training it) in
\vref{sec:model-iterations}. That is, starting from the base model, an
iterative process of improvement will be carried out.


\subsection{Base Architecture}\label{sec:base-arch}

In \vref{fig:model} a graphical representation of the model used as base
architecture is shown. It has two differentiated parts with which the features
are extracted from the visual part and from the language. These features are
then combined to achieve a \emph{multimodal} embedding and thus be able to
generate the segmentation.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{Images/RefVOS.png}
  \caption[RefVOS model architecture]{RefVOS model architecture. It is possible
    to observe the differentiated models of vision and language that are then
    combined to obtain the multimodal characteristics. From
    \figcite{bellver20:refvos}.}\label{fig:model}
\end{figure}

This model, created by \myCite{bellver20:refvos}, will constitute our starting
base architecture. Next, the image encoder (which is based on atrous
convolutions), the language encoder (which uses transformers) and the
multimodal embedding will be studied separately.

\subsubsection{Image Encoder}\index{Image encoder}

To extract the features of the images a state-of-the-art model called DeepLab
is used, which is a neural network created by \myCite{chen17:rethin} and based
on atrous convolutions (see \vref{fig:atrous}). It is a \gls{cnn} used for
semantic segmentation.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Atrous.png}
  \caption[Atrous convolutions examples]{Atrous convolutions examples with
    filter size \(3 \times 3\). The \code{rate} parameter controls the model's
    field-of-view. Standard convolution operation corresponds to an atrous
    convolution with a rate of 1. From
    \figcite{chen17:rethin}.}\label{fig:atrous}
\end{figure}

One of the advantages of this model compared to standard convolutional neural
models is that it adapts very well to objects at different scales, without the
need for pooling operations. Thus, the creators of this model define atrous
convolutions (also known as dilated convolutions).

\begin{quoteBox}
  Atrous convolution allows us to extract denser feature maps by removing the
  downsampling operations from the last few layers and upsampling the
  corresponding filter kernels, equivalent to inserting holes (``trous'' in
  French) between filter weights.
  \tcblower\quotecite{chen17:rethin}
\end{quoteBox}

In the model used, the well-known ResNet101 network (created by
\myCite{he16:deep}) and a \code{output\_stride}\footnote{The
  \code{output\_stride} is the ratio of input images patial resolution to final
  output resolution will be used as backbone. Setting this ratio to smaller
  values allow the model to extract denser feature responses (view section~3.1
  from~\cite{chen17:rethin}).} of 8. Likewise, \((12, 24, 36)\) will be used as
\code{rates} of the convolutions in \gls{aspp}. These pyramids are part of the
DeepLab model and consist of performing atrous convolutions in parallel (with
different rates). In this way, by using different rates, it is possible to
capture information from different scales at the same time.

\subsubsection{Language Encoder}\index{Language encoder}

In the case of the language encoder, different possibilities could be
considered, including using a \gls{rnn} or mainly using a transformer. In this
base architecture presented, RefVOS achieves more promising results by making
use of transformers. Specifically, a transformer created by
\myCite{devlin19:bert} and called \gls{bert} is used.

\gls{bert} is a multi-layer bidirectional Transformer encoder (see
\vref{sec:transformers}) that removes the unidirectionality constraint present
in previous models related to language representation. It uses \gls{mlm}, i.e.,
randomly masks some tokens from the input and tries to predict the original
token of masked word (just relying on its context). This allows the model to
learn from both left and right context.\footnote{The model \gls{bert} also uses
  the task of \gls{nsp} as an objective training function (see Task~\# 2 in
  section~3.1 from~\cite{devlin19:bert}).}

To integrate \gls{bert} within the model, it is necessary to convert each of
the \glspl{re} to tokens and add two special tokens: \code{[CLS]} and
\code{[SEP]} at the beginning and end of the sentence respectively. This model
will then produce embeddings of dimension 768 for each of the input tokens. The
final hidden vector corresponding to the first input token (\code{[CLS]}) as
the aggregate representation of the \gls{re} (view section~4.1
from~\cite{devlin19:bert}) will be taken.

\subsubsection{Multimodal Embedding}\index{Multimodal!embedding}

Once we have the encoded \gls{re} and the map of visuald features from the
convolutional network, it is necessary to obtain a multimodal embedding, which
combines the information from both encoders. The output from the visual encoder
is a tensor of depth 256 and the output from the language encoder is a
768-dimensional vector (see \vref{fig:model}).

To combine these two outputs, the encoded \gls{re} of the vector of dimension
768 is converted to one of dimension 256 (which coincides with the depth of the
visual features). These two tensors are then multiplied element-wise to obtain
the multimodal embedding. Finally, a convolutional layer is used to pass a last
tensor with two classes, which separate the \emph{background} from the
\emph{object} that is being referred.


\subsection{Model Iterations}\label{sec:model-iterations}

TODO. Model iterations. Lo tengo en Notas.org, mirar a ver si se puede exportar
a LaTeX.

\url{https://www.reddit.com/r/emacs/comments/lzsx1q/perfect_emacs_org_mode_exports_to_latex_easy_and/}

TODO. Mirar para hacer las gráficas. Mejor generar CSV con Python y plotear en
LaTeX.

\url{https://tex.stackexchange.com/questions/83888/how-to-plot-data-from-a-csv-file-using-tikz-and-csvsimple}.

\url{https://www.latex4technics.com/?note=lj0uix}


\begin{comment}
  1. He estado trabajando con el modelo de RefVOS, pero no he conseguido
  obtener ninguna mejora significativa. Hasta ahora lo que he probado ha sido:

  - Cambiar la manera en la que se unen las neuronas que provienen de la imagen
  y las que provienen del texto (en el modelo se usa multiplicación). He
  probado a entrenar con suma, resta, concatenación y proyectando usando una
  aplicación lineal (multiplicándolas por una matriz y luego combinándolas).

  - Probar diferentes funciones de error: "weighted cross entropy", "balanced
  cross entropy", "focal loss". Quiero probar también con otras como: "dice
  loss", "tversky index", "IoU loss"...  Las variaciones que he probado
  partiendo de parámetros pre-entrenados apenas varían el modelo (<1\mejora),
  posiblemente porque ya esté el modelo en cierto mínimo local de la función de
  error (al final son todas similares) y los gradientes son prácticamente
  nulos.

  Y probando variaciones con parámetros no-entrenados no consigo llegar a la
  precisión de RefVOS (entreno con train/validation/test y guardo los
  parámetros de la época con mejores resultados en validación).

  Por otro lado, he tratado de crear algún modelo desde 0, sin basarme en
  RefVOS, pero he obtenido resultados nefastos (cercanos a la aleatoriedad).
\end{comment}


Ahora, en este apartada lo que trataremos de hacer es entender el
funcionamiento de la arquitectura base explicada y proceder a realizar un
proceso iterativo de mejora de dicho modelo. Para ello atacaremos a las partes
fundamentales constituyentes de cualquier modelo neuronal: cambiar la
arquitectura o cambiar la manera de entrenar. Como sabemos, respecto a la
arquitectura, en este caso, tenemos tres partes diferenciadas (el image
encoder, el language encoder y el multimodal embedding). Y, respecto al
entrenamiento del modelo también se pueden distinguir diferentes partes: loss
function, criterio para parar el entrenamiento, técnica de optimización, uso de
parámetros pre-entrenados, etc.

\subsubsection{Loss Functions}\index{Loss function}

Originalmente la función utilizada para el entrenamiento es la de \gls{ce}, sin
embargo, para la tarea de segmentación (concretamente para segmentación binaria
como es este caso) existen muchas más (see \vref{sec:loss-functions}). Dentro
de todo este listado hay muchas de ellas que son variaciones precisamente de
\gls{ce}. Entrenar un modelo con estas variaciones resulta poco significado de
cara a resultados. Aquí comentar que no se encuentran resultados significados,
ya que, al tratarse de un modelo ya pre-entrenado y ser las funciones de loss
bastante similares, no se avanza en el entrenamiento se llega a una zona donde
los gradientes son prácticamente nulos.

Otras loss functions que podrían ser más interesantes son las basadas en
overlap measures. Entre ellas destacamos la de \gls{dil}.

\codeIn[firstline=119, lastline=132]{python}{../Code/utils.py}

Aquí sí o sí tengo que meter gráficas de cómo evoluciona la función de pérdida
respecto a las iteracciones en train/val. La tengo por ahí en un archivo .csv.


\subsubsection{Multimodal Embedding}\index{Multimodal!embedding}

Aquí lo que comentaremos es que en vez de sumar o restar o multiplicar las
diferentes características hemos probado a proyectar usando una aplicación
lineal. Aquí hay que volver a realizar todo el entrenamiento. Comentar que esto
simplemente está añadiendo complejidad al modelo, pero sin ningún interés
práctico real. El modelo ya es lo suficientemente complejo para funcionar
bien.


\subsubsection{Model}


\subsubsection{Trai proccess}



\section{Speech Recognition}%
\label{sec:speech}\index{Speech}\index{Speech!recognition}

Speech recognition, also known as \gls{asr} and \gls{stt} is a field of
\gls{cs} that deals with recognition of spoken language into text. For us it
will be useful because it will allow us to segment objects in images using the
voice, that is, we will be able to solve the problem of \gls{rec} using spoken
language.

For this task, we will use a pre-trained neural model to convert from
\gls{stt}. The model, created by
\myCite{veysov20:towar_imagen_momen_speec_text}, is called Silero (see
\vref{fig:silero}), and it allows converting from mono audio to text in
different languages: english, german, spanish and ukraniain.

\begin{figure}[p]
  \centering
  \includegraphics[width=.75\textwidth]{Images/Silero.jpg}
  \caption[Silero \glsentrylong{stt} model architecture]{Silero \acf{stt} model
    architecture. View from top to bottom: input is a mono audio file with
    speech and the output is the text representing the input. From
    \figcite{veysov20:towar_imagen_momen_speec_text}.}\label{fig:silero}
\end{figure}
