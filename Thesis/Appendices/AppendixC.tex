% -*- TeX-master: "../Thesis.tex" -*-


\chapter{Supplementary Material}\label{cha:extra}

\epigraphhead[75]{
  \epigraph{\itshape{}Write what should not be forgotten.}
  {---Isabel \textsc{Allende}}
}


\lettrine{L}{orem ipsum dolor} sit amet, consectetur adipiscing elit, sed do
eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim
veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo
consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse
cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non
proident, sunt in culpa qui officia deserunt mollit anim id est laborum.



\section{Activation Functions}%
\label{sec:activation}\index{Activation function}

Las funciones de activación como ya se ha discutido en el\ \vref{cha:theory}
son fundamentales para la creación de \gls{ann}. Esto es debido principalmente
a la necesidad de introducir no-linealidades a los modelos, de manera que se
facilite el ajuste de estos a funciones complejas: la naturaleza es altamente
no-lineal y sería imposible conseguir resultados útiles usando solo funciones
lineales para el ajuste de datos. En esta sección se discutirán y se compararán
tres de las funciones de activación más usadas: la \gls{relu}, la hyperbolic
tangent\index{Hyperbolic tangent} and the sigmoid function\index{Sigmoid
  function}.

La \gls{relu} es una de las funciones de activación más simples, conocidas y
usadas. Es la función que viene definida por la siguiente expresión,
\begin{equation}
  f(x) = \max\{0, x\},
\end{equation}
con derivada \(f^\prime(x) = \mathbf{1}_{\R^+}(x)\). Presenta diversas ventajas
como: sparse activation (si los valores de las neuronas fueran aleatorios solo
el 50\% de las neuronas tendrían activación no nula), propagación del gradiente
eficiente (no presenta problemas de vanishing gradient o ---al menos---
presenta menos problemas que las funciones de activaciones que saturan en las
dos direcciones) y es el cómputo de la activación es muy eficiente a nivel
computacional. En la \vref{fig:relu} se muestra tanto la gráfica de la función
como de su derivada.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{.375\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
        activationFunction,
        xmin = -1, xmax = 3,
        ymin = 0, ymax = 3]
        \addplot[myPlot] {max(0, x)};
        \addlegendentry{\(f(x) = \max\{0, x\}\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Activation function}
  \end{subfigure}\hspace{3em}
  \begin{subfigure}[b]{.375\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
        activationFunction,
        xmin = -1, xmax = 3,
        ymin = 0, ymax = 3]
        \addplot[myPlot, red, sharp plot, samples at={-5, -1e-6, 1e-6, 5}] {x>=0};
        \addlegendentry{\(f^\prime(x) = \mathbf{1}_{\R^+}(x)\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Activation function derivative}
  \end{subfigure}
  \caption[\Glsentrylong{relu} activation function]{\Acf{relu} activation
    function and derivative. Figures create by the author (both).}%
  \label{fig:relu}
\end{figure}

Cabe destacar que en algunas aplicaciones se pueden presentar complicaciones
con el uso de la función de activación \gls{relu}. Este es debido
principalmente a tres factores: no es diferenciable en el \(0\), no es
zero-centered (que sería una característica deseable en algunos casos) y no
es una función acotada, lo que podría dar problemas de overflow a nivel
computacional.

Otra de las funciones de activación típicamente usadas en este ámbito es el de
la \emph{hyperbolic tangent}\index{Hyperbolic tangent}. Esta función, cuya
gráfica y derivada viene representada en la\ \vref{fig:tangent} presenta
simetría impar y satura de manera simétrica. Pueden aparecer problemas de
vanishing gradient al usar este función.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{.375\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
        activationFunction,
        xmin = -3, xmax = 3,
        ymin = -1, ymax = 1,
        ytick distance = .5]
        \addplot[myPlot] {tanh(x)};
        \addlegendentry{\(f(x) = \tanh x\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Activation function}
  \end{subfigure}\hspace{3em}
  \begin{subfigure}[b]{.375\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
        activationFunction,
        xmin = -3, xmax = 3,
        ymin = -1, ymax = 1,
        ytick distance = .5]
        \addplot[myPlot, red] {1 - tanh(x)^2};
        \addlegendentry{\(f^\prime(x) = 1 - \tanh^2 x\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Activation function derivative}
  \end{subfigure}
  \caption[Hyperbolic tangent activation function]{Hyperbolic tangent
    activation function and derivative. Figures created by the author (both).}%
  \label{fig:tangent}
\end{figure}

Por último, otra de las funciones de activación conocidas es la de la
logistic\index{Logistic function} o sigmoid function\index{Sigmoid
  function}. Esta función es muy conocida dentro del ámbito de \gls{ml} por su
uso en la logistic regression\index{Logistic regression}, definida por la
siguiente expresión,
\begin{equation}
  \sigma(x) = {(1 + e^{-x})}^{-1},
\end{equation}
y cuya derivada se puede expresar en términos de la función original como
\(\sigma^\prime(x) = \sigma(x)(1 - \sigma(x))\).

Es una función (see \vref{fig:sigmoid}) que presenta buenas propiedas
matemáticas como la continuidad y diferenciabilidad en todo su dominio y que
limita la activación de la neurona al rango \([0, 1]\). Esta función es
especialmente útil en el caso de clasificación binaria, pero no es demasiado
usada a día de hoy debido principalmente a que: es posible que cause el
problema de vanishing gradient, no está centrada en el \(0\) y su cálculo es
computationally expensive.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{.375\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
        activationFunction,
        xmin = -4, xmax = 4,
        ymin = 0, ymax = 1,
        xtick distance = 2,
        ytick distance = .5]
        \addplot[myPlot, domain=-8:8] {(1 + e^(-x))^(-1)};
        \addlegendentry{\(\sigma(x) = {(1 + e^{-x})}^{-1}\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Activation function}
  \end{subfigure}\hspace{3em}
  \begin{subfigure}[b]{.375\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
        activationFunction,
        xmin = -4, xmax = 4,
        ymin = 0, ymax = 1,
        xtick distance = 2,
        ytick distance = .5]
        \addplot[myPlot, domain=-8:8, red] {(1 + e^(-x))^(-1)*(1 - (1 + e^(-x))^(-1))};
        \addlegendentry{\(\sigma^\prime(x) = \sigma(x)(1 - \sigma(x))\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Activation function derivative}
  \end{subfigure}
  \caption[Sigmoid activation function]{Sigmoid activation function and
    derivative (also called logistic function and soft step). Figures created
    by the author (both).}%
  \label{fig:sigmoid}
\end{figure}

Las funciones de activación descritas: \gls{relu}, hyperbolic tangent and the
sigmoid function se encuentran ploteadas en conjunto en la
\vref{fig:activation-functions}, donde pueden ser comparadas.

\begin{figure}[p]
  \centering
  \begin{subfigure}[b]{.475\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
        activationFunction,
        xmin = -3, xmax = 3,
        ymin = -1, ymax = 1.5]
        \addplot[myPlot] {max(0, x)};
        \addlegendentry{\(f(x) = \max\{0, x\}\)}
        \addplot[myPlot, red] {tanh(x)};
        \addlegendentry{\(g(x) = \tanh x\)}
        \addplot[myPlot, green] {(1 + e^(-x))^(-1)};
        \addlegendentry{\(\sigma(x) = {(1 + e^{-x})}^{-1}\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Activation functions}
  \end{subfigure}\hfill
  \begin{subfigure}[b]{.475\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
        activationFunction,
        xmin = -3, xmax = 3,
        ymin = -1, ymax = 1.5]
        \addplot[myPlot, sharp plot, samples at={-5, -1e-6, 1e-6, 5}] {x>=0};
        \addlegendentry{\(f^\prime(x) = \mathbf{1}_{\R^+}(x)\)}
        \addplot[myPlot, red] {1 - tanh(x)^2};
        \addlegendentry{\(g^\prime(x) = 1 - \tanh^2 x\)}
        \addplot[myPlot, green] {(1 + e^(-x))^(-1)*(1 - (1 + e^(-x))^(-1))};
        \addlegendentry{\(\sigma^\prime(x) = \sigma(x)(1 - \sigma(x))\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Activation function derivatives}
  \end{subfigure}
  \caption[Activation function comparison]{Activation function
    comparison. \Acf{relu}, hyperbolic tangent and sigmoid activation functions
    are plotted overlapping for a better comparison. Figures created by the
    author (both).}%
  \label{fig:activation-functions}
\end{figure}



















\section{Bibliography Reference}


Here I present a complete list of all the papers I have read for developing
this thesis. It's similar to a commented bibliography reference.
