% -*- TeX-master: "../Thesis.tex" -*-


\chapter{Supplementary Material}\label{cha:extra}

\epigraphhead[75]{
  \epigraph{\itshape{}Write what should not be forgotten.}
  {---Isabel \textsc{Allende}}
}


\lettrine{L}{orem ipsum dolor} sit amet, consectetur adipiscing elit, sed do
eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim
veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo
consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse
cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non
proident, sunt in culpa qui officia deserunt mollit anim id est laborum.



\section{Activation Functions}%
\label{sec:activation}\index{Activation function}

The activation functions as already discussed in\ \vref{cha:theory} are
fundamental for the creation of \gls{ann}. This is mainly due to the need to
introduce non-linearities to the models, so as to facilitate the adjustment of
these to complex functions: the nature is highly non-linear and it would be
impossible to achieve useful results using only linear functions for the
adjustment of data. In this section, three of the most used activation
functions will be discussed and compared: \gls{relu}, the hyperbolic
tangent\index{Hyperbolic tangent} and the sigmoid function\index{Sigmoid
  function}.

\gls{relu} is one of the simplest, known and most widely used activation
functions. It is the function that is defined by the following expression,
\begin{equation}
  f(x) = \max\{0, x\},
\end{equation}
with derivative \(f^\prime(x) = \mathbf{1}_{\R^+}(x)\). It presents several
advantages such as: sparse activation (if the neuron values were random, only
50\% of the neurons would have non-zero activation), efficient gradient
propagation (it does not present problems of vanishing gradient or---at
least---it presents fewer problems than the activation functions that saturate
in both directions) and the computation of the activation is very efficient at
the computational level. In\ \vref{fig:relu} both the graph of the function and
its derivative are shown.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{.375\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
        activationFunction,
        xmin = -1, xmax = 3,
        ymin = 0, ymax = 3]
        \addplot[myPlot] {max(0, x)};
        \addlegendentry{\(f(x) = \max\{0, x\}\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Activation function}
  \end{subfigure}\hspace{3em}
  \begin{subfigure}[b]{.375\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
        activationFunction,
        xmin = -1, xmax = 3,
        ymin = 0, ymax = 3]
        \addplot[myPlot, red, sharp plot, samples at={-5, -1e-6, 1e-6, 5}] {x>=0};
        \addlegendentry{\(f^\prime(x) = \mathbf{1}_{\R^+}(x)\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Activation function derivative}
  \end{subfigure}
  \caption[\Glsentrylong{relu} activation function]{\Acf{relu} activation
    function and derivative. Figures create by the author (both).}%
  \label{fig:relu}
\end{figure}

It should be noted that in some applications complications may occur with the
use of the activation function \gls{relu}. This is mainly due to three factors:
it is not differentiable in \(0\), it is not zero-centered (which would be a
desirable feature in some cases) and it is not a bounded function, which could
lead to overflow problems at the computational level.

Another of the activation functions typically used in this area is that of
\emph{hyperbolic tangent}\index{Hyperbolic tangent}. This function, whose graph
and derivative are represented in\ \vref{fig:tangent}, presents odd symmetry
and saturates symmetrically. Vanishing gradient issues may appear when using
this feature.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{.375\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
        activationFunction,
        xmin = -3, xmax = 3,
        ymin = -1, ymax = 1,
        ytick distance = .5]
        \addplot[myPlot] {tanh(x)};
        \addlegendentry{\(f(x) = \tanh x\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Activation function}
  \end{subfigure}\hspace{3em}
  \begin{subfigure}[b]{.375\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
        activationFunction,
        xmin = -3, xmax = 3,
        ymin = -1, ymax = 1,
        ytick distance = .5]
        \addplot[myPlot, red] {1 - tanh(x)^2};
        \addlegendentry{\(f^\prime(x) = 1 - \tanh^2 x\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Activation function derivative}
  \end{subfigure}
  \caption[Hyperbolic tangent activation function]{Hyperbolic tangent
    activation function and derivative. Figures created by the author (both).}%
  \label{fig:tangent}
\end{figure}

Finally, another of the known activation functions is that of the
logistic\index{Logistic function} or sigmoid function\index{Sigmoid
  function}. This function is well known within the scope of \gls{ml} for its
use in the logistic regression\index{Logistic regression}, defined by the
following expression,
\begin{equation}
  \sigma(x) = {(1 + e^{-x})}^{-1},
\end{equation}
and whose derivative can be expressed in terms of the original function as
\(\sigma^\prime(x) = \sigma(x)(1 - \sigma(x))\).

It is a function (see\ \vref{fig:sigmoid}) that has good mathematical
properties such as continuity and differentiability throughout its domain and
that limits the activation of the neuron to the range \([0, 1]\). This function
is especially useful in the case of binary classification, but it is not widely
used today mainly because: it may cause the vanishing gradient problem, it is
not centered on \(0\) and its calculation is computationally expensive.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{.375\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
        activationFunction,
        xmin = -4, xmax = 4,
        ymin = 0, ymax = 1,
        xtick distance = 2,
        ytick distance = .5]
        \addplot[myPlot, domain=-8:8] {(1 + e^(-x))^(-1)};
        \addlegendentry{\(\sigma(x) = {(1 + e^{-x})}^{-1}\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Activation function}
  \end{subfigure}\hspace{3em}
  \begin{subfigure}[b]{.375\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
        activationFunction,
        xmin = -4, xmax = 4,
        ymin = 0, ymax = 1,
        xtick distance = 2,
        ytick distance = .5]
        \addplot[myPlot, domain=-8:8, red] {(1 + e^(-x))^(-1)*(1 - (1 + e^(-x))^(-1))};
        \addlegendentry{\(\sigma^\prime(x) = \sigma(x)(1 - \sigma(x))\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Activation function derivative}
  \end{subfigure}
  \caption[Sigmoid activation function]{Sigmoid activation function and
    derivative (also called logistic function and soft step). Figures created
    by the author (both).}%
  \label{fig:sigmoid}
\end{figure}

The activation functions described: \gls{relu}, hyperbolic tangent and the
sigmoid function are plotted together in\ \vref{fig:activation-functions},
where they can be compared.

\begin{figure}[p]
  \centering
  \begin{subfigure}[b]{.475\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
        activationFunction,
        xmin = -3, xmax = 3,
        ymin = -1, ymax = 1.5]
        \addplot[myPlot] {max(0, x)};
        \addlegendentry{\(f(x) = \max\{0, x\}\)}
        \addplot[myPlot, red] {tanh(x)};
        \addlegendentry{\(g(x) = \tanh x\)}
        \addplot[myPlot, green] {(1 + e^(-x))^(-1)};
        \addlegendentry{\(\sigma(x) = {(1 + e^{-x})}^{-1}\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Activation functions}
  \end{subfigure}\hfill
  \begin{subfigure}[b]{.475\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
        activationFunction,
        xmin = -3, xmax = 3,
        ymin = -1, ymax = 1.5]
        \addplot[myPlot, sharp plot, samples at={-5, -1e-6, 1e-6, 5}] {x>=0};
        \addlegendentry{\(f^\prime(x) = \mathbf{1}_{\R^+}(x)\)}
        \addplot[myPlot, red] {1 - tanh(x)^2};
        \addlegendentry{\(g^\prime(x) = 1 - \tanh^2 x\)}
        \addplot[myPlot, green] {(1 + e^(-x))^(-1)*(1 - (1 + e^(-x))^(-1))};
        \addlegendentry{\(\sigma^\prime(x) = \sigma(x)(1 - \sigma(x))\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Activation function derivatives}
  \end{subfigure}
  \caption[Activation function comparison]{Activation function
    comparison. \Acf{relu}, hyperbolic tangent and sigmoid activation functions
    are plotted overlapping for a better comparison. Figures created by the
    author (both).}%
  \label{fig:activation-functions}
\end{figure}



\section{Bibliography Reference}


Here I present a complete list of all the papers I have read for developing
this thesis. It's similar to a commented bibliography reference.
